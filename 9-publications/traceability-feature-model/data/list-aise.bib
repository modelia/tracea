% Encoding: UTF-8

@InProceedings{Cambronero_2019,
  author    = {Jose Cambronero and Hongyu Li and Seohyun Kim and Koushik Sen and Satish Chandra},
  booktitle = {Proceedings of the 2019 27th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2019},
  title     = {When deep learning met code search},
  year      = {2019},
  publisher = {{ACM} Press},
  abstract  = {There have been multiple recent proposals on using deep neuralnetworks for code search using natural language. Common acrossthese proposals is the idea ofembeddingcode and natural languagequeries into real vectors and then using vector distance to approx-imate semantic correlation between code and the query. Multipleapproaches exist for learning these embeddings, includingunsuper-visedtechniques, which rely only on a corpus of code examples, andsupervisedtechniques, which use analignedcorpus of paired codeand natural language descriptions. The goal of this supervision isto produce embeddings that are more similar for a query and thecorresponding desired code snippet.Clearly, there are choices in whether to use supervised techniquesat all, and if one does, what sort of network and training to use forsupervision. This paper is the first to evaluate these choices systemat-ically. To this end, we assembled implementations of state-of-the-arttechniques to run on a common platform, training and evaluationcorpora. To explore the design space in network complexity, wealso introduced a new design point that is aminimalsupervisionextension to an existing unsupervised technique.Our evaluation shows that: 1. adding supervision to an existingunsupervised technique can improve performance, though not nec-essarily by much; 2. simple networks for supervision can be moreeffective that more sophisticated sequence-based networks for codesearch; 3. while it is common to use docstrings to carry out super-vision, there is a sizable gap between the effectiveness of docstringsand a more query-appropriate supervision corpus.},
  doi       = {10.1145/3338906.3340458},
  url       = {https://doi.org/10.1145%2F3338906.3340458},
}

@InProceedings{Mesbah_2019,
  author    = {Ali Mesbah and Andrew Rice and Emily Johnston and Nick Glorioso and Edward Aftandilian},
  booktitle = {Proceedings of the 2019 27th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2019},
  title     = {{DeepDelta}: learning to repair compilation errors},
  year      = {2019},
  publisher = {{ACM} Press},
  abstract  = {Programmers spend a substantial amount of time manually repairing code that does not compile. We observe that the repairs for any particular error class typically follow a pattern and are highly mechanical. We propose a novel approach that automatically learns these patterns with a deep neural network and suggests program repairs for the most costly classes of build-time compilation failures. We describe how we collect all build errors and the human-authored, in-progress code changes that cause those failing builds to transition to successful builds at Google. We generate an AST diff from the textual code changes and transform it into a domain-specific language called Delta that encodes the change that must be made to make the code compile. We then feed the compiler diagnostic information (as source) and the Delta changes that resolved the diagnostic (as target) into a Neural Machine Translation network for training. For the two most prevalent and costly classes of Java compilation errors, namely missing symbols and mismatched method signatures, our system called DeepDelta, generates the correct repair changes for 19,314 out of 38,788 (50%) of unseen compilation errors. The correct changes are in the top three suggested fixes 86% of the time on average.},
  doi       = {10.1145/3338906.3340455},
  url       = {https://doi.org/10.1145%2F3338906.3340455},
}

@InProceedings{Ha_2019,
  author    = {Huong Ha and Hongyu Zhang},
  booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
  title     = {{DeepPerf}: Performance Prediction for Configurable Software with Deep Sparse Neural Network},
  year      = {2019},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
  doi       = {10.1109/icse.2019.00113},
  url       = {https://doi.org/10.1109%2Ficse.2019.00113},
}

@InProceedings{Jiang_2019,
  author    = {Lin Jiang and Hui Liu and He Jiang},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  title     = {Machine Learning Based Recommendation of Method Names: How Far are We},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {High quality method names are critical for the readability and maintainability of programs. However, constructing concise and consistent method names is often challenging, especially for inexperienced developers. To this end, advanced machine learning techniques have been recently leveraged to recommend method names automatically for given method bodies/implementation. Recent large-scale evaluations also suggest that such approaches are accurate. However, little is known about where and why such approaches work or don't work. To figure out the state of the art as well as the rationale for the success/failure, in this paper we conduct an empirical study on the state-of-the-art approach code2vec. We assess code2vec on a new dataset with more realistic settings. Our evaluation results suggest that although switching to new dataset does not significantly influence the performance, more realistic settings do significantly reduce the performance of code2vec. Further analysis on the successfully recommended method names also reveals the following findings: 1) around half (48.3%) of the accepted recommendations are made on getter/setter methods; 2) a large portion (19.2%) of the successfully recommended method names could be copied from the given bodies. To further validate its usefulness, we ask developers to manually score the difficulty in naming methods they developed. Code2vec is then applied to such manually scored methods to evaluate how often it works in need. Our evaluation results suggest that code2vec rarely works when it is really needed. Finally, to intuitively reveal the state of the art and to investigate the possibility of designing simple and straightforward alternative approaches, we propose a heuristics based approach to recommending method names. Evaluation results on large-scale dataset suggest that this simple heuristics-based approach significantly outperforms the state-of-the-art machine learning based approach, improving precision and recall by 65.2...},
  doi       = {10.1109/ase.2019.00062},
  url       = {https://doi.org/10.1109%2Fase.2019.00062},
}

@InProceedings{Gopinath_2019,
  author    = {Divya Gopinath and Hayes Converse and Corina Pasareanu and Ankur Taly},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  title     = {Property Inference for Deep Neural Networks},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {We present techniques for automatically inferring formal properties of feed-forward neural networks. We observe that a significant part (if not all) of the logic of feed forward networks is captured in the activation status ('on' or 'off') of its neurons. We propose to extract patterns based on neuron decisions as preconditions that imply certain desirable output property e.g., the prediction being a certain class. We present techniques to extract input properties, encoding convex predicates on the input space that imply given output properties and layer properties, representing network properties captured in the hidden layers that imply the desired output behavior. We apply our techniques on networks for the MNIST and ACASXU applications. Our experiments highlight the use of the inferred properties in a variety of tasks, such as explaining predictions, providing robustness guarantees, simplifying proofs, and network distillation.},
  doi       = {10.1109/ase.2019.00079},
  groups    = {future},
  url       = {https://doi.org/10.1109%2Fase.2019.00079},
}

@InProceedings{Misra_2019,
  author    = {Janardan Misra and Sanjay Podder},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering Workshop ({ASEW})},
  title     = {Mining Text in Incident Repositories: Experiences and Perspectives on Adopting Machine Learning Solutions in Practice},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {Machine learning based data-driven solutions have potential to significantly improve quality of incident management process and make it cost effective. We present our experiences while addressing a spectrum of interrelated problems encountered in practice including identifying semantically related incidents, assignee recommendation, and mapping of incidents to business processes. We argue that despite long-standing research, it is not always straightforward to adopt recommendations from research into practice due to variability and complexity of business constraints and nature of data. We discuss need for meta-level analysis and suggest our own recommendations towards designing pragmatic solutions with low barriers to adoption and addressing right level of challenges.},
  doi       = {10.1109/asew.2019.00039},
  groups    = {future},
  url       = {https://doi.org/10.1109%2Fasew.2019.00039},
}

@InProceedings{Kim_2019,
  author    = {Jinhan Kim and Robert Feldt and Shin Yoo},
  booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
  title     = {Guiding Deep Learning System Testing Using Surprise Adequacy},
  year      = {2019},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system's behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining.},
  doi       = {10.1109/icse.2019.00108},
  url       = {https://doi.org/10.1109%2Ficse.2019.00108},
}

@InProceedings{Islam_2019,
  author    = {Md Johirul Islam and Giang Nguyen and Rangeet Pan and Hridesh Rajan},
  booktitle = {Proceedings of the 2019 27th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2019},
  title     = {A comprehensive study on deep learning bug characteristics},
  year      = {2019},
  publisher = {{ACM} Press},
  abstract  = {Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43% of the times.We have also found that the bugs in the usage of deep learning libraries have some common antipatterns.},
  doi       = {10.1145/3338906.3338955},
  url       = {https://doi.org/10.1145%2F3338906.3338955},
}

@InProceedings{Chen_2019,
  author    = {Yuting Chen and Ting Su and Zhendong Su},
  booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
  title     = {Deep Differential Testing of {JVM} Implementations},
  year      = {2019},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {The Java Virtual Machine (JVM) is the cornerstone of the widely-used Java platform. Thus, it is critical to ensure the reliability and robustness of popular JVM implementations. However, little research exists on validating production JVMs. One notable effort is classfuzz, which mutates Java bytecode syntactically to stress-test different JVMs. It is shown that classfuzz mainly produces illegal bytecode files and uncovers defects in JVMs' startup processes. It remains a challenge to effectively test JVMs' bytecode verifiers and execution engines to expose deeper bugs. This paper tackles this challenge by introducing classming, a novel, effective approach to performing deep, differential JVM testing. The key of classming is a technique, live bytecode mutation, to generate, from a seed bytecode file f, likely valid, executable (live) bytecode files: (1) capture the seed f's live bytecode, the sequence of its executed bytecode instructions; (2) repeatedly manipulate the control- and data-flow in f's live bytecode to generate semantically different mutants; and (3) selectively accept the generated mutants to steer the mutation process toward live, diverse mutants. The generated mutants are then employed to differentially test JVMs. We have evaluated classming on mainstream JVM implementations, including OpenJDK's HotSpot and IBM's J9, by mutating the DaCapo benchmarks. Our results show that classming is very effective in uncovering deep JVM differences. More than 1,800 of the generated classes exposed JVM differences, and more than 30 triggered JVM crashes. We analyzed and reported the JVM runtime differences and crashes, of which 14 have already been confirmed/fixed, including a highly critical security vulnerability in J9 that allowed untrusted code to disable the security manager and elevate its privileges (CVE-2017-1376).},
  doi       = {10.1109/icse.2019.00127},
  url       = {https://doi.org/10.1109%2Ficse.2019.00127},
}

@InProceedings{Pham_2019,
  author    = {Hung Viet Pham and Thibaud Lutellier and Weizhen Qi and Lin Tan},
  booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
  title     = {{CRADLE}: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries},
  year      = {2019},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {Deep learning (DL) systems are widely used in domains including aircraft collision avoidance systems, Alzheimer's disease diagnosis, and autonomous driving cars. Despite the requirement for high reliability, DL systems are difficult to test. Existing DL testing work focuses on testing the DL models, not the implementations (e.g., DL software libraries) of the models. One key challenge of testing DL libraries is the difficulty of knowing the expected output of DL libraries given an input instance. Fortunately, there are multiple implementations of the same DL algorithms in different DL libraries. Thus, we propose CRADLE, a new approach that focuses on finding and localizing bugs in DL software libraries. CRADLE (1) performs cross-implementation inconsistency checking to detect bugs in DL libraries, and (2) leverages anomaly propagation tracking and analysis to localize faulty functions in DL libraries that cause the bugs. We evaluate CRADLE on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and 30 pre-trained models. CRADLE detects 12 bugs and 104 unique inconsistencies, and highlights functions relevant to the causes of inconsistencies for all 104 unique inconsistencies.},
  doi       = {10.1109/icse.2019.00107},
  url       = {https://doi.org/10.1109%2Ficse.2019.00107},
}

@InProceedings{Kongmanee_2019,
  author    = {Jaturong Kongmanee and Phongphun Kijsanayothin and Rattikorn Hewett},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering Workshop ({ASEW})},
  title     = {Securing Smart Contracts in Blockchain},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {Blockchain is an emerging technology that underlies creation and exchange of the digital assets, including cryptocurrency such as Bitcoin and Ether, without the need for a central authority. It provides a public ledger for recording sequence of transactions in blocks that are linked as a chain. Smart contracts are computer programs governing participant agreements that are automatically enforced by consensus protocols in the blockchain. Together, blockchain and smart contracts revolutionize efficient transaction stores, services and workflows that work even among distrusting participants and without a trusted authority. Unfortunately, like most software, smart contracts are vulnerable as evidenced by a recent Decentralized Autonomous Organization (DAO) attack that lost cryptocurrency then-valued about $60 million. Correctness of executions alone is not sufficient to guarantee security of smart contracts. This paper addresses how we can apply model checking, a well-established formal verification technique, to help alleviate security issues in smart contract development. Most existing studies have focused on verification of smart contracts on a specific language and specific platform. Smart contracts may have hidden operational side effects that impact software behaviors. Thus, applying model checking to smart contracts is not necessarily straightforward. This paper presents a general technique for building the core functional models applicable for model checking to identify all possible executions that lead to security breaches. It also shows how resulting executions can be systematically analyzed to help identify security issues. The models are language and system independent in that they can represent any smart contract in any language or any platform. We illustrate and evaluate the technique with a widely used example of a smart contract in a financial system along with experimental results using a well-known model checker, NuSMV in various scenarios.},
  doi       = {10.1109/asew.2019.00032},
  url       = {https://doi.org/10.1109%2Fasew.2019.00032},
}

@InProceedings{Khatchadourian_2019,
  author    = {Raffi Khatchadourian and Yiming Tang and Mehdi Bagherzadeh and Syed Ahmed},
  booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
  title     = {Safe Automated Refactoring for Intelligent Parallelization of Java 8 Streams},
  year      = {2019},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {Streaming APIs are becoming more pervasive in mainstream Object-Oriented programming languages. For example, the Stream API introduced in Java 8 allows for functional-like, MapReduce-style operations in processing both finite and infinite data structures. However, using this API efficiently involves subtle considerations like determining when it is best for stream operations to run in parallel, when running operations in parallel can be less efficient, and when it is safe to run in parallel due to possible lambda expression side-effects. In this paper, we present an automated refactoring approach that assists developers in writing efficient stream code in a semantics-preserving fashion. The approach, based on a novel data ordering and typestate analysis, consists of preconditions for automatically determining when it is safe and possibly advantageous to convert sequential streams to parallel and unorder or de-parallelize already parallel streams. The approach was implemented as a plug-in to the Eclipse IDE, uses the WALA and SAFE analysis frameworks, and was evaluated on 11 Java projects consisting of ~642K lines of code. We found that 57 of 157 candidate streams (36.31%) were refactorable, and an average speedup of 3.49 on performance tests was observed. The results indicate that the approach is useful in optimizing stream code to their full potential.},
  doi       = {10.1109/icse.2019.00072},
  url       = {https://doi.org/10.1109%2Ficse.2019.00072},
}

@InProceedings{Wang_2019,
  author    = {Jingyi Wang and Guoliang Dong and Jun Sun and Xinyu Wang and Peixin Zhang},
  booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
  title     = {Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing},
  year      = {2019},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {Deep neural networks (DNN) have been shown to be useful in a wide range of applications. However, they are also known to be vulnerable to adversarial samples. By transforming a normal sample with some carefully crafted human imperceptible perturbations, even highly accurate DNN make wrong decisions. Multiple defense mechanisms have been proposed which aim to hinder the generation of such adversarial samples. However, a recent work show that most of them are ineffective. In this work, we propose an alternative approach to detect adversarial samples at runtime. Our main observation is that adversarial samples are much more sensitive than normal samples if we impose random mutations on the DNN. We thus first propose a measure of 'sensitivity' and show empirically that normal samples and adversarial samples have distinguishable sensitivity. We then integrate statistical hypothesis testing and model mutation testing to check whether an input sample is likely to be normal or adversarial at runtime by measuring its sensitivity. We evaluated our approach on the MNIST and CIFAR10 datasets. The results show that our approach detects adversarial samples generated by state-of-the-art attacking methods efficiently and accurately.},
  doi       = {10.1109/icse.2019.00126},
  url       = {https://doi.org/10.1109%2Ficse.2019.00126},
}

@InProceedings{Fucci_2019,
  author    = {Davide Fucci and Alireza Mollaalizadehbahnemiri and Walid Maalej},
  booktitle = {Proceedings of the 2019 27th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2019},
  title     = {On using machine learning to identify knowledge in {API} reference documentation},
  year      = {2019},
  publisher = {{ACM} Press},
  abstract  = {Using API reference documentation like JavaDoc is an integral part of software development. Previous research introduced a grounded taxonomy that organizes API documentation knowledge in 12 types, including knowledge about the Functionality, Structure, and Quality of an API. We study how well modern text classification approaches can automatically identify documentation containing specific knowledge types. We compared conventional machine learning (k-NN and SVM) with deep learning approaches trained on manually-annotated Java and .NET API documentation (n = 5,574). When classifying the knowledge types individually (i.e., multiple binary classifiers) the best AUPRC was up to 87},
  doi       = {10.1145/3338906.3338943},
  groups    = {future},
  url       = {https://doi.org/10.1145%2F3338906.3338943},
}

@InProceedings{Guo_2019,
  author    = {Qianyu Guo and Sen Chen and Xiaofei Xie and Lei Ma and Qiang Hu and Hongtao Liu and Yang Liu and Jianjun Zhao and Xiaohong Li},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  title     = {An Empirical Study Towards Characterizing Deep Learning Development and Deployment Across Different Frameworks and Platforms},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions.},
  comment   = {AI-enabled archi},
  doi       = {10.1109/ase.2019.00080},
  groups    = {future},
  url       = {https://doi.org/10.1109%2Fase.2019.00080},
}

@InProceedings{Nejadgholi_2019,
  author    = {Mahdi Nejadgholi and Jinqiu Yang},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  title     = {A Study of Oracle Approximations in Testing Deep Learning Libraries},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {Due to the increasing popularity of deep learning (DL) applications, testing DL libraries is becoming more and more important. Different from testing general software, for which output is often asserted definitely (e.g., an output is compared with an oracle for equality), testing deep learning libraries often requires to perform oracle approximations, i.e., the output is allowed to be within a restricted range of the oracle. However, oracle approximation practices have not been studied in prior empirical work that focuses on traditional testing practices. The prevalence, common practices, maintenance and evolution challenges of oracle approximations remain unknown in literature.

In this work, we study oracle approximation assertions implemented to test four popular DL libraries. Our study shows that there exists a non-negligible portion of assertions that leverage oracle approximation in testing DL libraries. Also, we identify the common sources of oracles on which oracle approximations are being performed through a comprehensive manual study. Moreover, we find that developers frequently modify code related to oracle approximations, i.e., using a different approximation API, modifying the oracle or the output from the code under test, and using a different approximation threshold. Last, we performed an in-depth study to understand the reasons behind the evolution of oracle approximation assertions. Our findings reveal important maintenance challenges that developers may face when maintaining oracle approximation practices as code evolves in DL libraries.},
  doi       = {10.1109/ase.2019.00078},
  url       = {https://doi.org/10.1109%2Fase.2019.00078},
}

@InProceedings{Kusmenko_2019,
  author    = {Evgeny Kusmenko and Svetlana Pavlitskaya and Bernhard Rumpe and Sebastian Stuber},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering Workshop ({ASEW})},
  title     = {On the Engineering of {AI}-Powered Systems},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {More and more tasks become solvable using deep learning technology nowadays. Consequently, the amount of neural network code in software rises continuously. To make the new paradigm more accessible, frameworks, languages, and tools keep emerging. Although, the maturity of these tools is steadily increasing, we still lack appropriate domain specific languages and a high degree of automation when it comes to deep learning for productive systems. In this paper we present a multi-paradigm language family allowing the AI engineer to model and train deep neural networks as well as to integrate them into software architectures containing classical code. Using input and output layers as strictly typed interfaces enables a seamless embedding of neural networks into component-based models. The lifecycle of deep learning components can then be governed by a compiler accordingly, e.g. detecting when (re-)training is necessary or when network weights can be shared between different network instances. We provide a compelling case study, where we train an autonomous vehicle for the TORCS simulator. Furthermore, we discuss how the methodology automates the AI development process if neural networks are changed or added to the system.},
  comment   = {ai-enabled archi},
  doi       = {10.1109/asew.2019.00042},
  groups    = {future},
  url       = {https://doi.org/10.1109%2Fasew.2019.00042},
}

@InProceedings{Zheng_2019,
  author    = {Yan Zheng and Xiaofei Xie and Ting Su and Lei Ma and Jianye Hao and Zhaopeng Meng and Yang Liu and Ruimin Shen and Yingfeng Chen and Changjie Fan},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  title     = {Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  comment   = {Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs, which have been confirmed by the developers, in the commercial games.},
  doi       = {10.1109/ase.2019.00077},
  url       = {https://doi.org/10.1109%2Fase.2019.00077},
}

@InProceedings{Aggarwal_2019,
  author    = {Aniya Aggarwal and Pranay Lohia and Seema Nagar and Kuntal Dey and Diptikalyan Saha},
  booktitle = {Proceedings of the 2019 27th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2019},
  title     = {Black box fairness testing of machine learning models},
  year      = {2019},
  publisher = {{ACM} Press},
  abstract  = {Any given AI system cannot be accepted unless its trustworthiness is proven. An important characteristic of a trustworthy AI system is the absence of algorithmic bias. 'Individual discrimination' exists when a given individual different from another only in 'protected attributes' (e.g., age, gender, race, etc.) receives a different decision outcome from a given machine learning (ML) model as compared to the other individual. The current work addresses the problem of detecting the presence of individual discrimination in given ML models. Detection of individual discrimination is test-intensive in a black-box setting, which is not feasible for non-trivial systems. We propose a methodology for auto-generation of test inputs, for the task of detecting individual discrimination. Our approach combines two well-established techniques - symbolic execution and local explainability for effective test case generation. We empirically show that our approach to generate test cases is very effective as compared to the best-known benchmark systems that we examine.},
  comment   = {Check},
  doi       = {10.1145/3338906.3338937},
  groups    = {future},
  url       = {https://doi.org/10.1145%2F3338906.3338937},
}

@InProceedings{Zhang_2019,
  author    = {Hao Zhang and W.K. Chan},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  title     = {Apricot: A Weight-Adaptation Approach to Fixing Deep Learning Models},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {A deep learning (DL) model is inherently imprecise. To address this problem, existing techniques retrain a DL model over  a  larger  training  dataset  or  with  the  help  of  fault  injected  models or using the insight of failing test cases in a DL model. In this   paper,   we   present   Apricot,   a   novel   weight-adaptation   approach to fixing DL models iteratively. Our key observation is that if the deep learning architecture of a DL model is trained over many different subsets of the original training dataset, the weights in  the  resultant  reduced  DL  model  (rDLM)  can  provide  insights  on the adjustment direction and magnitude of the weights in the original  DL  model  to  handle  the  test  cases  that  the  original  DL  model  misclassifies.  Apricot  generates  a  set  of  such  reduced  DL  models  from  the  original  DL  model.  In  each  iteration,  for  each  failing  test  case  experienced  by  the  input  DL  model  (iDLM),  Apricot  adjusts  each  weight  of  this  iDLM  toward  the  average  weight  of  these  rDLMs  correctly  classifying  the  test  case  and/or  away from that of these rDLMs misclassifying the same test case, followed  by  training  the  weight-adjusted  iDLM  over  the  original  training dataset to generate a new iDLM for the next iteration. The experiment  using  five  state-of-the-art  DL  models  shows  that  Apricot can increase the test accuracy of these models by 0.87%-1.55% with an average of 1.08%. The experiment also reveals the complementary nature of these rDLMs in Apricot.},
  comment   = {archi inside loop},
  doi       = {10.1109/ase.2019.00043},
  groups    = {future},
  url       = {https://doi.org/10.1109%2Fase.2019.00043},
}

@InProceedings{Saccente_2019,
  author    = {Nicholas Saccente and Josh Dehlinger and Lin Deng and Suranjan Chakraborty and Yin Xiong},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering Workshop ({ASEW})},
  title     = {Project Achilles: A Prototype Tool for Static Method-Level Vulnerability Detection of Java Source Code Using a Recurrent Neural Network},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {Software has become an essential component of modern life, but when software vulnerabilities threaten the security of users, new ways of analyzing for software security must be explored. Using the National Institute of Standards and Technology's Juliet Java Suite, containing thousands of examples of defective Java methods for a variety of vulnerabilities, a prototype tool was developed implementing an array of Long-Short Term Memory Recurrent Neural Networks to detect vulnerabilities within source code. The tool employs various data preparation methods to be independent of coding style and to automate the process of extracting methods, labeling data, and partitioning the dataset. The result is a prototype command-line utility that generates an n-dimensional vulnerability prediction vector. The experimental evaluation using 44,495 test cases indicates that the tool can achieve an accuracy higher than 90% for 24 out of 29 different types of CWE vulnerabilities.},
  doi       = {10.1109/asew.2019.00040},
  url       = {https://doi.org/10.1109%2Fasew.2019.00040},
}

@InProceedings{Alizadeh_2019,
  author    = {Vahid Alizadeh and Mohamed Amine Ouali and Marouane Kessentini and Meriem Chater},
  booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  title     = {{RefBot}: Intelligent Software Refactoring Bot},
  year      = {2019},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost. In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any "open" or "merge" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.},
  doi       = {10.1109/ase.2019.00081},
  url       = {https://doi.org/10.1109%2Fase.2019.00081},
}

@InProceedings{Du_2019,
  author    = {Xiaoning Du and Xiaofei Xie and Yi Li and Lei Ma and Yang Liu and Jianjun Zhao},
  booktitle = {Proceedings of the 2019 27th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2019},
  title     = {{DeepStellar}: model-based quantitative analysis of stateful deep learning systems},
  year      = {2019},
  publisher = {{ACM} Press},
  abstract  = {Deep Learning (DL) has achieved tremendous success in many cutting-edge applications. However, the state-of-the-art DL systems still suffer from quality issues. While some recent progress has been made on the analysis of feed-forward DL systems, little study has been done on the Recurrent Neural Network (RNN)-based stateful DL systems, which are widely used in audio, natural languages and video processing, etc. In this paper, we initiate the very first step towards the quantitative analysis of RNN-based DL systems. We model RNN as an abstract state transition system to characterize its internal behaviors. Based on the abstract model, we design two trace similarity metrics and five coverage criteria which enable the quantitative analysis of RNNs. We further propose two algorithms powered by the quantitative measures for adversarial sample detection and coverage-guided test generation. We evaluate DeepStellar on four RNN-based systems covering image classification and automated speech recognition. The results demonstrate that the abstract model is useful in capturing the internal behaviors of RNNs, and confirm that (1) the similarity metrics could effectively capture the differences between samples even with very small perturbations (achieving 97% accuracy for detecting adversarial samples) and (2) the coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).},
  doi       = {10.1145/3338906.3338954},
  groups    = {future},
  url       = {https://doi.org/10.1145%2F3338906.3338954},
}

@InProceedings{Grech_2019,
  author    = {Neville Grech and Lexi Brent and Bernhard Scholz and Yannis Smaragdakis},
  booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
  title     = {Gigahorse: Thorough, Declarative Decompilation of Smart Contracts},
  year      = {2019},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {The rise of smart contracts - autonomous applications running on blockchains - has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract. We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a highlevel 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts. Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts - e.g., Gigahorse can decompile over 99.98% of deployed contracts, compared to 88% for the recently-published Vandal decompiler and under 50% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a “batteries included” approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.},
  comment   = {check},
  doi       = {10.1109/icse.2019.00120},
  url       = {https://doi.org/10.1109%2Ficse.2019.00120},
}

@InProceedings{Zhao_2018,
  author    = {Gang Zhao and Jeff Huang},
  booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2018},
  title     = {{DeepSim}: deep learning code functional similarity},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Measuring code similarity is fundamental for many software engineering tasks, e.g., code search, refactoring and reuse. However, most existing techniques focus on code syntactical similarity only, while measuring code functional similarity remains a challenging problem. In this paper, we propose a novel approach that encodes code control flow and data flow into a semantic matrix in which each element is a high dimensional sparse binary feature vector, and we design a new deep learning model that measures code functional similarity based on this representation. By concatenating hidden representations learned from a code pair, this new model transforms the problem of detecting functionally similar code to binary classification, which can effectively learn patterns between functionally similar code with very different syntactics.

We have implemented our approach, DeepSim, for Java programs and evaluated its recall, precision and time performance on two large datasets of functionally similar code. The experimental results show that DeepSim significantly outperforms existing state-of-the-art techniques, such as DECKARD, RtvNN, CDLH, and two baseline deep neural networks models.},
  doi       = {10.1145/3236024.3236068},
  url       = {https://doi.org/10.1145%2F3236024.3236068},
}

@InProceedings{Ma_2018,
  author    = {Lei Ma and Yang Liu and Jianjun Zhao and Yadong Wang and Felix Juefei-Xu and Fuyuan Zhang and Jiyuan Sun and Minhui Xue and Bo Li and Chunyang Chen and Ting Su and Li Li},
  booktitle = {Proceedings of the 33rd {ACM}/{IEEE} International Conference on Automated Software Engineering - {ASE} 2018},
  title     = {{DeepGauge}: multi-granularity testing criteria for deep learning systems},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.},
  comment   = {archi testing},
  doi       = {10.1145/3238147.3238202},
  groups    = {future},
  url       = {https://doi.org/10.1145%2F3238147.3238202},
}

@InProceedings{Hu_2018,
  author    = {Gang Hu and Linjie Zhu and Junfeng Yang},
  booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2018},
  title     = {{AppFlow}: using machine learning to synthesize robust, reusable {UI} tests},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {UI testing is known to be difficult, especially as today’s development cycles become faster. Manual UI testing is tedious, costly and error- prone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an “add to cart” test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides “smoke testing” requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing.},
  doi       = {10.1145/3236024.3236055},
  url       = {https://doi.org/10.1145%2F3236024.3236055},
}

@InProceedings{Hellendoorn_2018,
  author    = {Vincent J. Hellendoorn and Christian Bird and Earl T. Barr and Miltiadis Allamanis},
  booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2018},
  title     = {Deep learning type inference},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Dynamically typed languages such as JavaScript and Python are increasingly popular, yet static typing has not been totally eclipsed: Python now supports type annotations and languages like TypeScript offer a middle-ground for JavaScript: a strict superset of JavaScript, to which it transpiles, coupled with a type system that permits partially typed programs. However, static typing has a cost: adding annotations, reading the added syntax, and wrestling with the type system to fix type errors. Type inference can ease the transition to more statically typed code and unlock the benefits of richer compile-time information, but is limited in languages like JavaScript as it cannot soundly handle duck-typing or runtime evaluation via eval. We propose DeepTyper, a deep learning model that understands which types naturally occur in certain contexts and relations and can provide type suggestions, which can often be verified by the type checker, even if it could not infer the type initially. DeepTyper, leverages an automatically aligned corpus of tokens and types to accurately predict thousands of variable and function type annotations. Furthermore, we demonstrate that context is key in accurately assigning these types and introduce a technique to reduce overfitting on local cues while highlighting the need for further improvements. Finally, we show that our model can interact with a compiler to provide more than 4,000 additional type annotations with over 95% precision that could not be inferred without the aid of DeepTyper.},
  doi       = {10.1145/3236024.3236051},
  url       = {https://doi.org/10.1145%2F3236024.3236051},
}

@InProceedings{Jiang_2018,
  author    = {Bo Jiang and Ye Liu and W. K. Chan},
  booktitle = {Proceedings of the 33rd {ACM}/{IEEE} International Conference on Automated Software Engineering - {ASE} 2018},
  title     = {{ContractFuzzer}: fuzzing smart contracts for vulnerability detection},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Decentralized cryptocurrencies feature the use of blockchain to transfer values among peers on networks without central agency. Smart contracts are programs running on top of the blockchain consensus protocol to enable people make agreements while minimizing trusts. Millions of smart contracts have been deployed in various decentralized applications. The security vulnerabilities within those smart contracts pose significant threats to their applications. Indeed, many critical security vulnerabilities within smart contracts on Ethereum platform have caused huge financial losses to their users. In this work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs based on the ABI specifications of smart contracts, defines test oracles to detect security vulnerabilities, instruments the EVM to log smart contracts runtime behaviors, and analyzes these logs to report security vulnerabilities. Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities with high precision. In particular, our fuzzing tool successfully detects the vulnerability of the DAO contract that leads to USD 60 million loss and the vulnerabilities of Parity Wallet that have led to the loss of USD 30 million and the freezing of USD 150 million worth of Ether.},
  doi       = {10.1145/3238147.3238177},
  url       = {https://doi.org/10.1145%2F3238147.3238177},
}

@InProceedings{Wang_2018,
  author    = {Jianghao Wang and Hamid Bagheri and Myra B. Cohen},
  booktitle = {Proceedings of the 33rd {ACM}/{IEEE} International Conference on Automated Software Engineering - {ASE} 2018},
  title     = {An evolutionary approach for analyzing Alloy specifications},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Formal methods use mathematical notations and logical reasoning to precisely define a program's specifications, from which we can instantiate valid instances of a system. With these techniques we can perform a multitude of tasks to check system dependability. Despite the existence of many automated tools including ones considered lightweight, they still lack a strong adoption in practice. At the crux of this problem, is scalability and applicability to large real world applications. In this paper we show how to relax the completeness guarantee without much loss, since soundness is maintained. We have extended a popular lightweight analysis, Alloy, with a genetic algorithm. Our new tool, EvoAlloy, works at the level of finite relations generated by Kodkod and evolves the chromosomes based on the failed constraints. In a feasibility study we demonstrate that we can find solutions to a set of specifications beyond the scope where traditional Alloy fails. While small specifications take longer with EvoAlloy, the scalability means we can handle larger specifications. Our future vision is that when specifications are small we can maintain both soundness and completeness, but when this fails, EvoAlloy can switch to its genetic algorithm.},
  doi       = {10.1145/3238147.3240468},
  url       = {https://doi.org/10.1145%2F3238147.3240468},
}

@InProceedings{Liu_2018,
  author    = {Hui Liu and Zhifeng Xu and Yanzhen Zou},
  booktitle = {Proceedings of the 33rd {ACM}/{IEEE} International Conference on Automated Software Engineering - {ASE} 2018},
  title     = {Deep learning based feature envy detection},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Software refactoring is widely employed to improve software quality. A key step in software refactoring is to identify which part of the software should be refactored. To facilitate the identification, a number of approaches have been proposed to identify certain structures in the code (called code smells) that suggest the possibility of refactoring. Most of such approaches rely on manually designed heuristics to map manually selected source code metrics to predictions. However, it is challenging to manually select the best features, especially textual features. It is also difficult to manually construct the optimal heuristics. To this end, in this paper we propose a deep learning based novel approach to detecting feature envy, one of the most common code smells. The key insight is that deep neural networks and advanced deep learning techniques could automatically select features (especially textual features) of source code for feature envy detection, and could automatically build the complex mapping between such features and predictions. We also propose an automatic approach to generating labeled training data for the neural network based classifier, which does not require any human intervention. Evaluation results on open-source applications suggest that the proposed approach significantly improves the state-of-the-art in both detecting feature envy smells and recommending destinations for identified smelly methods.},
  doi       = {10.1145/3238147.3238166},
  url       = {https://doi.org/10.1145%2F3238147.3238166},
}

@InProceedings{Ma_2018,
  author    = {Shiqing Ma and Yingqi Liu and Wen-Chuan Lee and Xiangyu Zhang and Ananth Grama},
  booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2018},
  title     = {{MODE}: automated neural network model debugging via state differential analysis and input selection},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Artificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our technique can fix model bugs effectively and efficiently without introducing new bugs. For simple applications (e.g., digit recognition), MODE improves the test accuracy from 75% to 93% on average whereas the state-of-the-art can only improve to 85% with 11 times more training time. For complex applications and models (e.g., object recognition), MODE is able to improve the accuracy from 75% to over 91% in minutes to a few hours, whereas state-of-the-art fails to fix the bug or even degrades the test accuracy.},
  comment   = {archi outside loop},
  doi       = {10.1145/3236024.3236082},
  url       = {https://doi.org/10.1145%2F3236024.3236082},
}

@InProceedings{Zhang_2018,
  author    = {Mengshi Zhang and Yuqun Zhang and Lingming Zhang and Cong Liu and Sarfraz Khurshid},
  booktitle = {Proceedings of the 33rd {ACM}/{IEEE} International Conference on Automated Software Engineering - {ASE} 2018},
  title     = {{DeepRoad}: {GAN}-based metamorphic testing and input validation framework for autonomous driving systems},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness.

In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.},
  doi       = {10.1145/3238147.3238187},
  url       = {https://doi.org/10.1145%2F3238147.3238187},
}

@InProceedings{Abdessalem_2018,
  author    = {Raja Ben Abdessalem and Shiva Nejati and Lionel C. Briand and Thomas Stifter},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  title     = {Testing vision-based control systems using learnable evolutionary algorithms},
  year      = {2018},
  month     = {may},
  publisher = {{ACM}},
  abstract  = {Vision-based control systems are key enablers of many autonomous vehicular systems, including self-driving cars. Testing such systems is complicated by complex and multidimensional input spaces. We propose an automated testing algorithm that builds on learnable evolutionary algorithms. These algorithms rely on machine learning or a combination of machine learning and Darwinian genetic operators to guide the generation of new solutions (test scenarios in our context). Our approach combines multiobjective population-based search algorithms and decision tree classification models to achieve the following goals: First, classification models guide the search-based generation of tests faster towards critical test scenarios (i.e., test scenarios leading to failures). Second, search algorithms refine classification models so that the models can accurately characterize critical regions (i.e., the regions of a test input space that are likely to contain most critical test scenarios). Our evaluation performed on an industrial automotive automotive system shows that: (1) Our algorithm outperforms a baseline evolutionary search algorithm and generates 78% more distinct, critical test scenarios compared to the baseline algorithm. (2) Our algorithm accurately characterizes critical regions of the system under test, thus identifying the conditions that are likely to lead to system failures.},
  comment   = {check},
  doi       = {10.1145/3180155.3180160},
  url       = {https://doi.org/10.1145%2F3180155.3180160},
}

@InProceedings{Wan_2018,
  author    = {Yao Wan and Zhou Zhao and Min Yang and Guandong Xu and Haochao Ying and Jian Wu and Philip S. Yu},
  booktitle = {Proceedings of the 33rd {ACM}/{IEEE} International Conference on Automated Software Engineering - {ASE} 2018},
  title     = {Improving automatic source code summarization via deep reinforcement learning},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.},
  doi       = {10.1145/3238147.3238206},
  groups    = {future},
  url       = {https://doi.org/10.1145%2F3238147.3238206},
}

@InProceedings{Liu_2018,
  author    = {Han Liu and Chao Liu and Wenqi Zhao and Yu Jiang and Jiaguang Sun},
  booktitle = {Proceedings of the 33rd {ACM}/{IEEE} International Conference on Automated Software Engineering - {ASE} 2018},
  title     = {S-gram: towards semantic-aware security auditing for Ethereum smart contracts},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Smart contracts, as a promising and powerful application on the Ethereum blockchain, have been growing rapidly in the past few years. Since they are highly vulnerable to different forms of attacks, their security becomes a top priority. However, existing security auditing techniques are either limited in fnding vulnerabilities (rely on pre-defned bug paterns) or very expensive (rely on program analysis), thus are insufcient for Ethereum.

To mitigate these limitations, we proposed a novel semanticaware security auditing technique called S-gram for Ethereum. The key insight is a combination of N-gram language modeling and lightweight static semantic labeling, which can learn statistical regularities of contract tokens and capture high-level semantics as well (e.g., flow sensitivity of a transaction). S-gram can be used to predict potential vulnerabilities by identifying irregular token sequences and optimize existing in-depth analyzers (e.g., symbolic execution engines, fuzzers etc.). We have implemented S-gram for Solidity smart contracts in Ethereum. The evaluation demonstrated the potential of S-gram in identifying possible security issues.},
  doi       = {10.1145/3238147.3240728},
  url       = {https://doi.org/10.1145%2F3238147.3240728},
}

@InProceedings{Gu_2018,
  author    = {Xiaodong Gu and Hongyu Zhang and Sunghun Kim},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  title     = {Deep code search},
  year      = {2018},
  month     = {may},
  publisher = {{ACM}},
  abstract  = {To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code. In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled. As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.},
  doi       = {10.1145/3180155.3180167},
  url       = {https://doi.org/10.1145%2F3180155.3180167},
}

@InProceedings{Tian_2018,
  author    = {Yuchi Tian and Kexin Pei and Suman Jana and Baishakhi Ray},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  title     = {{DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars}},
  year      = {2018},
  month     = {may},
  publisher = {{ACM}},
  abstract  = {Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.
However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases.
In this paper, we design, implement and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explores different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.},
  doi       = {10.1145/3180155.3180220},
  url       = {https://doi.org/10.1145%2F3180155.3180220},
}

@InProceedings{Roy_2018,
  author    = {Subhajit Roy and Awanish Pandey and Brendan Dolan-Gavitt and Yu Hu},
  booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2018},
  title     = {Bug synthesis: challenging bug-finding tools with deep faults},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {In spite of decades of research in bug detection tools, there is a surprising dearth of ground-truth corpora that can be used to evaluate the efficacy of such tools. Recently, systems such as LAVA and EvilCoder have been proposed to automatically inject bugs into software to quickly generate large bug corpora, but the bugs created so far differ from naturally occurring bugs in a number of ways. In this work, we propose a new automated bug injection system, Apocalypse, that uses formal techniques—symbolic execution, constraint-based program synthesis and model counting—to automatically inject fair (can potentially be discovered by current bug-detection tools), deep (requiring a long sequence of dependencies to be satisfied to fire), uncorrelated (each bug behaving independent of others), reproducible (a trigger input being available) and rare (can be triggered by only a few program inputs) bugs in large software code bases. In our evaluation, we inject bugs into thirty Coreutils programs as well as the TCAS test suite. We find that bugs synthesized by Apocalypse are highly realistic under a variety of metrics, that they do not favor a particular bug-finding strategy (unlike bugs produced by LAVA), and that they are more difficult to find than manually injected bugs, requiring up around 240× more tests to discover with a state-of-the-art symbolic execution tool.},
  doi       = {10.1145/3236024.3236084},
  url       = {https://doi.org/10.1145%2F3236024.3236084},
}

@InProceedings{Sun_2018-concolic-for-RNN,
  author    = {Youcheng Sun and Min Wu and Wenjie Ruan and Xiaowei Huang and Marta Kwiatkowska and Daniel Kroening},
  booktitle = {Proceedings of the 33rd {ACM}/{IEEE} International Conference on Automated Software Engineering - {ASE} 2018},
  title     = {Concolic testing for deep neural networks},
  year      = {2018},
  publisher = {{ACM} Press},
  abstract  = {Concolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. This paper presents the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we formalise coverage criteria for DNNs that have been studied in the literature, and then develop a coherent method for performing concolic testing to increase test coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.},
  comment   = {check},
  doi       = {10.1145/3238147.3238172},
  groups    = {positive tracing},
  url       = {https://doi.org/10.1145%2F3238147.3238172},
}

@InProceedings{Fu_2017,
  author    = {Wei Fu and Tim Menzies},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering - {ESEC}/{FSE} 2017},
  title     = {Easy over hard: a case study on deep learning},
  year      = {2017},
  publisher = {{ACM} Press},
  abstract  = {While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long training time limits the ability of (a)~a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b)~other researchers to repeat, improve, or even refute that original work.

For example, recently, deep learning was used to find which questions in the Stack Overflow programmer discussion forum can be linked together. That deep learning system took 14 hours to execute. We show here that applying a very simple optimizer called DE to fine tune SVM, it can achieve similar (and sometimes better) results. The DE approach terminated in 10 minutes; i.e. 84 times faster hours than deep learning method.

We offer these results as a cautionary tale to the software analytics community and suggest that not every new innovation should be applied without critical analysis. If researchers deploy some new and expensive process, that work should be baselined against some simpler and faster alternatives.},
  doi       = {10.1145/3106237.3106256},
  groups    = {ai critic},
  url       = {https://doi.org/10.1145%2F3106237.3106256},
}

@InProceedings{Hellendoorn_2017,
  author    = {Vincent J. Hellendoorn and Premkumar Devanbu},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering - {ESEC}/{FSE} 2017},
  title     = {Are deep neural networks the best choice for modeling source code?},
  year      = {2017},
  publisher = {{ACM} Press},
  abstract  = {Statistical language modeling techniques have successfully been applied to source code, yielding a variety of new software development tools, such as tools for code suggestion and improving readability. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. But traditional language models limit the vocabulary to a fixed set of common words. For code, this strong assumption has been shown to have a significant negative effect on predictive performance. But the open vocabulary version of the neural network language models for code have not been introduced in the literature. We present a new open-vocabulary neural language model for code that is not limited to a fixed vocabulary of identifier names. We employ a segmentation into subword units, subsequences of tokens chosen based on a compression criterion, following previous work in machine translation. Our network achieves best in class performance, outperforming even the state-of-the-art methods of Hellendoorn and Devanbu that are designed specifically to model code. Furthermore, we present a simple method for dynamically adapting the model to a new test project, resulting in increased performance. We showcase our methodology on code corpora in three different languages of over a billion tokens each, hundreds of times larger than in previous work. To our knowledge, this is the largest neural language model for code that has been reported.},
  doi       = {10.1145/3106237.3106290},
  url       = {https://doi.org/10.1145%2F3106237.3106290},
}

@InProceedings{Godefroid_2017,
  author    = {Patrice Godefroid and Hila Peleg and Rishabh Singh},
  booktitle = {2017 32nd {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  title     = {Learn{\&}Fuzz: Machine learning for input fuzzing},
  year      = {2017},
  month     = {oct},
  publisher = {{IEEE}},
  abstract  = {Fuzzing consists of repeatedly testing an application with modified, or fuzzed, inputs with the goal of finding security vulnerabilities in input-parsing code. In this paper, we show how to automate the generation of an input grammar suitable for input fuzzing using sample inputs and neural-network-based statistical machine-learning techniques. We present a detailed case study with a complex input format, namely PDF, and a large complex security-critical parser for this format, namely, the PDF parser embedded in Microsoft's new Edge browser. We discuss (and measure) the tension between conflicting learning and fuzzing goals: learning wants to capture the structure of well-formed inputs, while fuzzing wants to break that structure in order to cover unexpected code paths and find bugs. We also present a new algorithm for this learn&fuzz challenge which uses a learnt input probability distribution to intelligently guide where to fuzz inputs.},
  doi       = {10.1109/ase.2017.8115618},
  url       = {https://doi.org/10.1109%2Fase.2017.8115618},
}

@InProceedings{Ma_2017,
  author    = {Shiqing Ma and Yousra Aafer and Zhaogui Xu and Wen-Chuan Lee and Juan Zhai and Yingqi Liu and Xiangyu Zhang},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering - {ESEC}/{FSE} 2017},
  title     = {{LAMP}: data provenance for graph based machine learning algorithms through derivative computation},
  year      = {2017},
  publisher = {{ACM} Press},
  abstract  = {Data provenance tracking determines the set of inputs related to a given output. It enables quality control and problem diagnosis in data engineering. Most existing techniques work by tracking program dependencies. They cannot quantitatively assess the importance of related inputs, which is critical to machine learning algorithms, in which an output tends to depend on a huge set of inputs while only some of them are of importance. In this paper, we propose LAMP, a provenance computation system for machine learning algorithms. Inspired by automatic differentiation (AD), LAMP quantifies the importance of an input for an output by computing the partial derivative. LAMP separates the original data processing and the more expensive derivative computation to different processes to achieve cost-effectiveness. In addition, it allows quantifying importance for inputs related to discrete behavior, such as control flow selection. The evaluation on a set of real world programs and data sets illustrates that LAMP produces more precise and succinct provenance than program dependence based techniques, with much less overhead. Our case studies demonstrate the potential of LAMP in problem diagnosis in data engineering.},
  doi       = {10.1145/3106237.3106291},
  groups    = {future},
  url       = {https://doi.org/10.1145%2F3106237.3106291},
}

@InProceedings{Xu_2017,
  author    = {Zhengzi Xu and Bihuan Chen and Mahinthan Chandramohan and Yang Liu and Fu Song},
  booktitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering ({ICSE})},
  title     = {{SPAIN}: Security Patch Analysis for Binaries towards Understanding the Pain and Pills},
  year      = {2017},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {Software vulnerability is one of the major threats to software security. Once discovered, vulnerabilities are often fixed by applying security patches. In that sense, security patches carry valuable information about vulnerabilities, which could be used to discover, understand and fix (similar) vulnerabilities. However, most existing patch analysis approaches work at the source code level, while binary-level patch analysis often heavily relies on a lot of human efforts and expertise. Even worse, some vulnerabilities may be secretly patched without applying CVE numbers, or only the patched binary programs are available while the patches are not publicly released. These practices greatly hinder patch analysis and vulnerability analysis. In this paper, we propose a scalable binary-level patch analysis framework, named SPAIN, which can automatically identify security patches and summarize patch patterns and their corresponding vulnerability patterns. Specifically, given the original and patched versions of a binary program, we locate the patched functions and identify the changed traces (i.e., a sequence of basic blocks) that may contain security or non-security patches. Then we identify security patches through a semantic analysis of these traces and summarize the patterns through a taint analysis on the patched functions. The summarized patterns can be used to search similar patches or vulnerabilities in binary programs. Our experimental results on several real-world projects have shown that: i) SPAIN identified security patches with high accuracy and high scalability, ii) SPAIN summarized 5 patch patterns and their corresponding vulnerability patterns for 5 vulnerability types, and iii) SPAIN discovered security patches that were not documented, and discovered 3 zero-day vulnerabilities.},
  doi       = {10.1109/icse.2017.49},
  url       = {https://doi.org/10.1109%2Ficse.2017.49},
}

@InProceedings{Lee_2017,
  author    = {Sun-Ro Lee and Min-Jae Heo and Chan-Gun Lee and Milhan Kim and Gaeul Jeong},
  booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering - {ESEC}/{FSE} 2017},
  title     = {Applying deep learning based automatic bug triager to industrial projects},
  year      = {2017},
  publisher = {{ACM} Press},
  abstract  = {Finding the appropriate developer for a bug report, so called `Bug Triage', is one of the bottlenecks in the bug resolution process. To address this problem, many approaches have proposed various automatic bug triage techniques in recent studies. We argue that most previous studies focused on open source projects only and did not consider deep learning techniques. In this paper, we propose to use Convolutional Neural Network and word embedding to build an automatic bug triager. The results of the experiments applied to both industrial and open source projects reveal benefits of the automatic approach and suggest co-operation of human and automatic triagers. Our experience in integrating and operating the proposed system in an industrial development environment is also reported.},
  doi       = {10.1145/3106237.3117776},
  url       = {https://doi.org/10.1145%2F3106237.3117776},
}

@InProceedings{Guo_2017,
  author    = {Jin Guo and Jinghui Cheng and Jane Cleland-Huang},
  booktitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering ({ICSE})},
  title     = {Semantically Enhanced Software Traceability Using Deep Learning Techniques},
  year      = {2017},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {In most safety-critical domains the need for trace-ability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts; however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links; however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.},
  doi       = {10.1109/icse.2017.9},
  url       = {https://doi.org/10.1109%2Ficse.2017.9},
}

@InProceedings{Vendome_2017,
  author    = {Christopher Vendome and Mario Linares-Vasquez and Gabriele Bavota and Massimiliano Di Penta and Daniel German and Denys Poshyvanyk},
  booktitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering ({ICSE})},
  title     = {Machine Learning-Based Detection of Open Source License Exceptions},
  year      = {2017},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications.},
  doi       = {10.1109/icse.2017.19},
  groups    = {future},
  url       = {https://doi.org/10.1109%2Ficse.2017.19},
}

@InProceedings{Raza_2016,
  author    = {Mushtaq Raza and Jo{\~{a}}o Pascoal Faria},
  booktitle = {Proceedings of the 31st {IEEE}/{ACM} International Conference on Automated Software Engineering - {ASE} 2016},
  title     = {{ProcessPAIR}: a tool for automated performance analysis and improvement recommendation in software development},
  year      = {2016},
  publisher = {{ACM} Press},
  abstract  = {High-maturity software development processes can generate significant amounts of data that can be periodically analyzed to identify performance problems, determine their root causes and devise improvement actions. However, conducting that analysis manually is challenging because of the potentially large amount of data to analyze and the effort and expertise required. In this paper, we present ProcessPAIR, a novel tool designed to help developers analyze their performance data with less effort, by automatically identifying and ranking performance problems and potential root causes, so that subsequent manual analysis for the identification of deeper causes and improvement actions can be properly focused. The analysis is based on performance models defined manually by process experts and calibrated automatically from the performance data of many developers. We also show how ProcessPAIR was successfully applied for the Personal Software Process (PSP). A video about ProcessPAIR is available in https://youtu.be/dEk3fhhkduo.},
  doi       = {10.1145/2970276.2970284},
  url       = {https://doi.org/10.1145%2F2970276.2970284},
}

@InProceedings{Gu_2016,
  author    = {Xiaodong Gu and Hongyu Zhang and Dongmei Zhang and Sunghun Kim},
  booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering - {FSE} 2016},
  title     = {Deep {API} learning},
  year      = {2016},
  publisher = {{ACM} Press},
  abstract  = {Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bag-of-words (i.e., keyword matching or word-to-word alignment) and lack a deep understanding of the semantics of the query.
We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bags-of-words assumption, it learns the sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches.},
  doi       = {10.1145/2950290.2950334},
  url       = {https://doi.org/10.1145%2F2950290.2950334},
}

@InProceedings{White_2016,
  author    = {Martin White and Michele Tufano and Christopher Vendome and Denys Poshyvanyk},
  booktitle = {Proceedings of the 31st {IEEE}/{ACM} International Conference on Automated Software Engineering - {ASE} 2016},
  title     = {Deep learning code fragments for code clone detection},
  year      = {2016},
  publisher = {{ACM} Press},
  abstract  = {Code clone detection is an important problem for software maintenance and evolution. Many approaches consider either structure or identifiers, but none of the existing detection techniques model both sources of information. These techniques also depend on generic, handcrafted features to represent code fragments. We introduce learning-based detection techniques where everything for representing terms and fragments in source code is mined from the repository. Our code analysis supports a framework, which relies on deep learning, for automatically linking patterns mined at the lexical level with patterns mined at the syntactic level. We evaluated our novel learning-based approach for code clone detection with respect to feasibility from the point of view of software maintainers. We sampled and manually evaluated 398 file- and 480 method-level pairs across eight real-world Java systems; 93% of the file- and method-level samples were evaluated to be true positives. Among the true positives, we found pairs mapping to all four clone types. We compared our approach to a traditional structure-oriented technique and found that our learning-based approach detected clones that were either undetected or suboptimally reported by the prominent tool Deckard. Our results affirm that our learning-based approach is suitable for clone detection and a tenable technique for researchers.},
  doi       = {10.1145/2970276.2970326},
  url       = {https://doi.org/10.1145%2F2970276.2970326},
}

@InProceedings{Ben_Abdessalem_2016,
  author    = {Raja Ben Abdessalem and Shiva Nejati and Lionel C. Briand and Thomas Stifter},
  booktitle = {Proceedings of the 31st {IEEE}/{ACM} International Conference on Automated Software Engineering - {ASE} 2016},
  title     = {Testing advanced driver assistance systems using multi-objective search and neural networks},
  year      = {2016},
  publisher = {{ACM} Press},
  abstract  = {Recent years have seen a proliferation of complex Advanced Driver Assistance Systems (ADAS), in particular, for use in autonomous cars. These systems consist of sensors and cameras as well as image processing and decision support software components. They are meant to help drivers by providing proper warnings or by preventing dangerous situations. In this paper, we focus on the problem of design time testing of ADAS in a simulated environment. We provide a testing approach for ADAS by combining multi-objective search with surrogate models developed based on neural networks. We use multi-objective search to guide testing towards the most critical behaviors of ADAS. Surrogate modeling enables our testing approach to explore a larger part of the input search space within limited computational resources. We characterize the condition under which the multi-objective search algorithm behaves the same with and without surrogate modeling, thus showing the accuracy of our approach. We evaluate our approach by applying it to an industrial ADAS system. Our experiment shows that our approach automatically identifies test cases indicating critical ADAS behaviors. Further, we show that combining our search algorithm with surrogate modeling improves the quality of the generated test cases, especially under tight and realistic computational resources.},
  doi       = {10.1145/2970276.2970311},
  url       = {https://doi.org/10.1145%2F2970276.2970311},
}

@InProceedings{Li_2016-symbolic-execution-feasibility,
  author    = {Xin Li and Yongjuan Liang and Hong Qian and Yi-Qi Hu and Lei Bu and Yang Yu and Xin Chen and Xuandong Li},
  booktitle = {Proceedings of the 31st {IEEE}/{ACM} International Conference on Automated Software Engineering - {ASE} 2016},
  title     = {Symbolic execution of complex program driven by machine learning based constraint solving},
  year      = {2016},
  publisher = {{ACM} Press},
  abstract  = {Symbolic execution is a widely-used program analysis technique. It collects and solves path conditions to guide the program traversing. However, due to the limitation of the current constraint solvers, it is difficult to apply symbolic execution on programs with complex path conditions, like nonlinear constraints and function calls. In this paper, we propose a new symbolic execution tool MLB to handle such problem. Instead of relying on the classical constraint solving, in MLB, the feasibility problems of the path conditions are transformed into optimization problems, by minimizing some dissatisfaction degree. The optimization problems are then handled by the underlying optimization solver through machine learning guided sampling and validation. MLB is implemented on the basis of Symbolic PathFinder and encodes not only the simple linear path conditions, but also nonlinear arithmetic operations, and even black-box function calls of library methods, into symbolic path conditions. Experiment results show that MLB can achieve much better coverage on complex real-world programs.},
  comment   = {check},
  doi       = {10.1145/2970276.2970364},
  groups    = {positive tracing},
  url       = {https://doi.org/10.1145%2F2970276.2970364},
}

@InProceedings{Cervantes_2016,
  author    = {Humberto Cervantes and Serge Haziyev and Olha Hrytsay and Rick Kazman},
  booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion - {ICSE} {\textquotesingle}16},
  title     = {Smart decisions},
  year      = {2016},
  publisher = {{ACM} Press},
  abstract  = {Architecture design is notoriously difficult to teach and to learn. Most competent architects in industry have deep knowledge won from long years of experience. But if we want architecture design to be methodical and repeatable, we need better methods for teaching it. Simply waiting for an aspiring architect to accumulate 10 or 20 years of experience is not acceptable if we believe that software engineering is a true engineering discipline. In this paper we describe our experiences with the development of a game that aids in teaching architecture design, specifically design employing the Attribute-Driven Design method. We discuss our approach to creating the game, and the "design concepts catalog" that provides the knowledge base for the game. Finally, we report on our experiences with deploying the game, and the (enthusiastic) assessments and feedback that we have received from industrial and academic participants.},
  doi       = {10.1145/2889160.2889184},
  url       = {https://doi.org/10.1145%2F2889160.2889184},
}

@InProceedings{Xu_2016,
  author    = {Bowen Xu and Deheng Ye and Zhenchang Xing and Xin Xia and Guibin Chen and Shanping Li},
  booktitle = {Proceedings of the 31st {IEEE}/{ACM} International Conference on Automated Software Engineering - {ASE} 2016},
  title     = {Predicting semantically linkable knowledge in developer online forums via convolutional neural network},
  year      = {2016},
  publisher = {{ACM} Press},
  abstract  = {Consider a question and its answers in Stack Overflow as a knowledge unit. Knowledge units often contain semantically relevant knowledge, and thus linkable for different purposes, such as duplicate questions, directly linkable for problem solving, indirectly linkable for related information. Recognizing different classes of linkable knowledge would support more targeted information needs when users search or explore the knowledge base. Existing methods focus on binary relatedness (i.e., related or not), and are not robust to recognize different classes of semantic relatedness when linkable knowledge units share few words in common (i.e., have lexical gap). In this paper, we formulate the problem of predicting semantically linkable knowledge units as a multiclass classification problem, and solve the problem using deep learning techniques. To overcome the lexical gap issue, we adopt neural language model (word embeddings) and convolutional neural network (CNN) to capture word- and document-level semantics of knowledge units. Instead of using human-engineered classifier features which are hard to design for informal user-generated content, we exploit large amounts of different types of user-created knowledge-unit links to train the CNN to learn the most informative word- and sentence-level features for the multiclass classification task. Our evaluation shows that our deep-learning based approach significantly and consistently outperforms traditional methods using traditional word representations and human-engineered classifier features.},
  doi       = {10.1145/2970276.2970357},
  url       = {https://doi.org/10.1145%2F2970276.2970357},
}

@InProceedings{Winter_2015,
  author    = {Stefan Winter and Oliver Schwahn and Roberto Natella and Neeraj Suri and Domenico Cotroneo},
  booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
  title     = {No {PAIN}, No Gain? The Utility of {PArallel} Fault {INjections}},
  year      = {2015},
  month     = {may},
  publisher = {{IEEE}},
  abstract  = {Software Fault Injection (SFI) is an established technique for assessing the robustness of a software under test by exposing it to faults in its operational environment. Depending on the complexity of this operational environment, the complexity of the software under test, and the number and type of faults, a thorough SFI assessment can entail (a) numerous experiments and (b) long experiment run times, which both contribute to a considerable execution time for the tests. In order to counteract this increase when dealing with complex systems, recent works propose to exploit parallel hardware to execute multiple experiments at the same time. While Parallel fault Injections (PAIN) yield higher experiment throughput, they are based on an implicit assumption of non-interference among the simultaneously executing experiments. In this paper we investigate the validity of this assumption and determine the trade-off between increased throughput and the accuracy of experimental results obtained from PAIN experiments.},
  doi       = {10.1109/icse.2015.67},
  url       = {https://doi.org/10.1109%2Ficse.2015.67},
}
@inproceedings{Di_Nardo_2015,
	doi = {10.1109/ase.2015.13},
	url = {https://doi.org/10.1109%2Fase.2015.13},
	year = 2015,
	month = {nov},
	publisher = {{IEEE}},
	author = {Daniel Di Nardo and Fabrizio Pastore and Andrea Arcuri and Lionel Briand},
	title = {Evolutionary Robustness Testing of Data Processing Systems Using Models and Data Mutation (T)},
	booktitle = {2015 30th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})}
}

@InProceedings{Lam_2015,
  author    = {An Ngoc Lam and Anh Tuan Nguyen and Hoan Anh Nguyen and Tien N. Nguyen},
  booktitle = {2015 30th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
  title     = {Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports (N)},
  year      = {2015},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {Bug localization refers to the automated process of locating the potential buggy files for a given bug report. To help developers focus their attention to those files is crucial. Several existing automated approaches for bug localization from a bug report face a key challenge, called lexical mismatch, in which the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. This paper presents a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files and documentation if they appear frequently enough in the pairs of reports and buggy files. Our empirical evaluation on real-world projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, HyLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. Two out of three cases, a correct buggy file is in the list of three suggested files.},
  comment   = {check},
  doi       = {10.1109/ase.2015.73},
  url       = {https://doi.org/10.1109%2Fase.2015.73},
}

@InProceedings{Guo_2014,
  author    = {Jin Guo and Natawut Monaikul and Cody Plepel and Jane Cleland-Huang},
  booktitle = {Proceedings of the 29th {ACM}/{IEEE} international conference on Automated software engineering - {ASE} {\textquotesingle}14},
  title     = {Towards an intelligent domain-specific traceability solution},
  year      = {2014},
  publisher = {{ACM} Press},
  abstract  = {State-of-the-art software trace retrieval techniques are unable to perform the complex reasoning that a human analyst follows in order to create accurate trace links between artifacts such as regulatory codes and requirements. As a result, current algorithms often generate imprecise links. To address this problem, we present the Domain-Contextualized Intelligent Traceability Solution (DoCIT), designed to mimic some of the higher level reasoning that a human trace analyst performs. We focus our efforts on the complex domain of communication and control in a transportation system. DoCIT includes rules for extracting ``action units'' from software artifacts, a domain-specific knowledge base for relating semantically similar concepts across action units, and a set of link-creation heuristics which utilize the action units to establish meaningful trace links across pairs of artifacts. Our approach significantly improves the quality of the generated trace links. We illustrate and evaluate DoCIT with examples and experiments from the control and communication sector of a transportation domain.},
  doi       = {10.1145/2642937.2642970},
  url       = {https://doi.org/10.1145%2F2642937.2642970},
}

@InProceedings{Mahmood_2014,
  author    = {Riyadh Mahmood and Nariman Mirzaei and Sam Malek},
  booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering - {FSE} 2014},
  title     = {{EvoDroid}: segmented evolutionary testing of Android apps},
  year      = {2014},
  publisher = {{ACM} Press},
  abstract  = {Proliferation of Android devices and apps has created a demand for applicable automated software testing techniques. Prior research has primarily focused on either unit or GUI testing of Android apps, but not their end-to-end system testing in a systematic manner. We present EvoDroid, an evolutionary approach for system testing of Android apps. EvoDroid overcomes a key shortcoming of using evolutionary techniques for system testing, i.e., the inability to pass on genetic makeup of good individuals in the search. To that end, EvoDroid combines two novel techniques: (1) an Android-specific program analysis technique that identifies the segments of the code amenable to be searched independently, and (2) an evolutionary algorithm that given information of such segments performs a step-wise search for test cases reaching deep into the code. Our experiments have corroborated EvoDroid’s ability to achieve significantly higher code coverage than existing Android testing tools.},
  doi       = {10.1145/2635868.2635896},
  url       = {https://doi.org/10.1145%2F2635868.2635896},
}

@InProceedings{Azim_2014,
  author    = {Md. Tanzirul Azim and Iulian Neamtiu and Lisa M. Marvel},
  booktitle = {Proceedings of the 29th {ACM}/{IEEE} international conference on Automated software engineering - {ASE} {\textquotesingle}14},
  title     = {Towards self-healing smartphone software via automated patching},
  year      = {2014},
  publisher = {{ACM} Press},
  abstract  = {Frequent app bugs and low tolerance for loss of functionality create an impetus for self-healing smartphone software. We take a step towards this via on-the-fly error detection and automated patching. Specifically, we add failure detection and recovery to Android by detecting crashes and ``sealing off'' the crashing part of the app to avoid future crashes. In the detection stage, our system dynamically analyzes app execution to detect certain exceptional situations. In the recovery stage, we use bytecode rewriting to alter app behavior as to avoid such situations in the future. When using our implementation, apps can resume operation (albeit with limited functionality) instead of repeatedly crashing. Our approach does not require access to app source code or any system (e.g., kernel-level) modification. Experiments on several real-world, popular Android apps and bugs show that our approach manages to recover the apps from crashes effectively, timely, and without introducing overhead.},
  doi       = {10.1145/2642937.2642955},
  url       = {https://doi.org/10.1145%2F2642937.2642955},
}

@InProceedings{Kashyap_2014,
  author    = {Vineeth Kashyap and Kyle Dewey and Ethan A. Kuefner and John Wagner and Kevin Gibbons and John Sarracino and Ben Wiedermann and Ben Hardekopf},
  booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering - {FSE} 2014},
  title     = {{JSAI}: a static analysis platform for {JavaScript}},
  year      = {2014},
  publisher = {{ACM} Press},
  abstract  = {JavaScript is used everywhere from the browser to the server, including desktops and mobile devices. However, the current state of the art in JavaScript static analysis lags far behind that of other languages such as C and Java. Our goal is to help remedy this lack. We describe JSAI, a formally specified, robust abstract interpreter for JavaScript. JSAI uses novel abstract domains to compute a reduced product of type inference, pointer analysis, control-flow analysis, string analysis, and integer and boolean constant propagation. Part of JSAI's novelty is user-configurable analysis sensitivity, i.e., context-, path-, and heap-sensitivity. JSAI is designed to be provably sound with respect to a specific concrete semantics for JavaScript, which has been extensively tested against a commercial JavaScript implementation. We provide a comprehensive evaluation of JSAI's performance and precision using an extensive benchmark suite, including real-world JavaScript applications, machine generated JavaScript code via Emscripten, and browser addons. We use JSAI's configurability to evaluate a large number of analysis sensitivities (some well-known, some novel) and observe some surprising results that go against common wisdom. These results highlight the usefulness of a configurable analysis platform such as JSAI.},
  doi       = {10.1145/2635868.2635904},
  groups    = {remove},
  url       = {https://doi.org/10.1145%2F2635868.2635904},
}

@InProceedings{Lee_2014,
  author    = {Sangho Lee and Changhee Jung and Santosh Pande},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering - {ICSE} 2014},
  title     = {Detecting memory leaks through introspective dynamic behavior modelling using machine learning},
  year      = {2014},
  publisher = {{ACM} Press},
  abstract  = {This paper expands staleness-based memory leak detection by presenting a machine learning-based framework. The proposed framework is based on an idea that object staleness can be better leveraged in regard to similarity of objects; i.e., an object is more likely to have leaked if it shows significantly high staleness not observed from other similar objects with the same allocation context.

A central part of the proposed framework is the modeling of heap objects. To this end, the framework observes the staleness of objects during a representative run of an application. From the observed data, the framework generates training examples, which also contain instances of hypothetical leaks. Via machine learning, the proposed framework replaces the error-prone user-definable staleness predicates used in previous research with a model-based prediction.

The framework was tested using both synthetic and real-world examples. Evaluation with synthetic leakage workloads of SPEC2006 benchmarks shows that the proposed method achieves the optimal accuracy permitted by staleness-based leak detection. Moreover, by incorporating allocation context into the model, the proposed method achieves higher accuracy than is possible with object staleness alone. Evaluation with real-world memory leaks demonstrates that the proposed method is effective for detecting previously reported bugs with high accuracy.},
  doi       = {10.1145/2568225.2568307},
  url       = {https://doi.org/10.1145%2F2568225.2568307},
}

@InProceedings{Li_2014,
  author    = {Ding Li and Angelica Huyen Tran and William G. J. Halfond},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering - {ICSE} 2014},
  title     = {Making web applications more energy efficient for {OLED} smartphones},
  year      = {2014},
  publisher = {{ACM} Press},
  doi       = {10.1145/2568225.2568321},
  groups    = {remove},
  url       = {https://doi.org/10.1145%2F2568225.2568321},
}

@InProceedings{Zhang_2014,
  author    = {Mingxing Zhang and Yongwei Wu and Shan Lu and Shanxiang Qi and Jinglei Ren and Weimin Zheng},
  booktitle = {Proceedings of the 22nd {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering - {FSE} 2014},
  title     = {{AI}: a lightweight system for tolerating concurrency bugs},
  year      = {2014},
  publisher = {{ACM} Press},
  doi       = {10.1145/2635868.2635885},
  groups    = {remove},
  url       = {https://doi.org/10.1145%2F2635868.2635885},
}
@inproceedings{Liu_2014,
	doi = {10.1145/2568225.2568229},
	url = {https://doi.org/10.1145%2F2568225.2568229},
	year = 2014,
	publisher = {{ACM} Press},
	author = {Yepang Liu and Chang Xu and Shing-Chi Cheung},
	title = {Characterizing and detecting performance bugs for smartphone applications},
	booktitle = {Proceedings of the 36th International Conference on Software Engineering - {ICSE} 2014}
}

@InProceedings{Devos_2012,
  author    = {Nicolas Devos and Christophe Ponsard and Jean-Christophe Deprez and Renaud Bauvin and Benedicte Moriau and Guy Anckaerts},
  booktitle = {2012 34th International Conference on Software Engineering ({ICSE})},
  title     = {Efficient reuse of domain-specific test knowledge: An industrial case in the smart card domain},
  year      = {2012},
  month     = {jun},
  publisher = {{IEEE}},
  abstract  = {While testing is heavily used and largely automated in software development projects, the reuse of test practices across similar projects in a given domain is seldom systematized and supported by adequate methods and tools. This paper presents a practical approach that emerged from a concrete industrial case in the smart card domain at STMicroelectronics Belgium in order to better address this kind of challenge. The central concept is a test knowledge repository organized as a collection of specific patterns named QPatterns. A systematic process was followed, first to gather, structure and abstract the test practices, then to produce and validate an initial repository, and finally to make it evolve later on Testers can then rely on this repository to produce high quality test plans identifying all the functional and nonfunctional aspects that have to be addressed, as well as the concrete tests that have to be developed within the context of a new project. A tool support was also developed and integrated in a traceable way into the existing industrial test environment. The approach was validated and is currently under deployment at STMicroelectronics Belgium},
  doi       = {10.1109/icse.2012.6227107},
  url       = {https://doi.org/10.1109%2Ficse.2012.6227107},
}

@InProceedings{Wong_2011,
  author    = {Sunny Wong and Yuanfang Cai},
  booktitle = {2011 26th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE} 2011)},
  title     = {Generalizing evolutionary coupling with stochastic dependencies},
  year      = {2011},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {Researchers have leveraged evolutionary coupling derived from revision history to conduct various software analyses, such as software change impact analysis (IA). The problem is that the validity of historical data depends on the recency of changes and varies with different evolution paths-thus, influencing the accuracy of analysis results. In this paper, we formalize evolutionary coupling as a stochastic process using a Markov chain model. By varying the parameters of this model, we define a family of stochastic dependencies that accounts for different types of evolution paths. Each member of this family weighs historical data differently according to their recency and frequency. To assess the utility of this model, we conduct IA on 78 releases of five open source systems, using 16 stochastic dependency types, and compare with the results of several existing approaches. The results show that our stochastic-based IA technique can provide more accurate results than these existing techniques.},
  doi       = {10.1109/ase.2011.6100065},
  groups    = {future},
  url       = {https://doi.org/10.1109%2Fase.2011.6100065},
}

@InProceedings{Chen_2011,
  author    = {Ning Chen and Steven C. H. Hoi and Xiaokui Xiao},
  booktitle = {2011 26th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE} 2011)},
  title     = {Software process evaluation: A machine learning approach},
  year      = {2011},
  month     = {nov},
  publisher = {{IEEE}},
  abstract  = {Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.},
  doi       = {10.1109/ase.2011.6100070},
  url       = {https://doi.org/10.1109%2Fase.2011.6100070},
}

@InProceedings{Kazman_2011,
  author    = {Rick Kazman and Len Bass and James Ivers and Gabriel A. Moreno},
  booktitle = {Proceeding of the 33rd international conference on Software engineering - {ICSE} {\textquotesingle}11},
  title     = {Architecture evaluation without an architecture},
  year      = {2011},
  publisher = {{ACM} Press},
  abstract  = {This paper describes an analysis of some of the challenges facing one portion of the Electrical Smart Grid in the United States - residential Demand Response (DR) systems. The purposes of this paper are twofold: 1) to discover risks to residential DR systems and 2) to illustrate an architecture-based analysis approach to uncovering risks that span a collection of technical and social concerns. The results presented here are specific to residential DR but the approach is general and it could be applied to other systems within the Smart Grid and to other critical infrastructure domains. Our architecture-based analysis is different from most other approaches to analyzing complex systems in that it addresses multiple quality attributes simultaneously (e.g., performance, reliability, security, modifiability, usability, etc.) and it considers the architecture of a complex system from a socio-technical perspective where the actions of the people in the system are as important, from an analysis perspective, as the physical and computational elements of the system. This analysis can be done early in a system's lifetime, before substantial resources have been committed to its construction or procurement, and so it provides extremely cost-effective risk analysis.},
  doi       = {10.1145/1985793.1985886},
  url       = {https://doi.org/10.1145%2F1985793.1985886},
}

@InProceedings{Duley_2010,
  author    = {Adam Duley and Chris Spandikow and Miryung Kim},
  booktitle = {Proceedings of the {IEEE}/{ACM} international conference on Automated software engineering - {ASE} {\textquotesingle}10},
  title     = {A program differencing algorithm for verilog {HDL}},
  year      = {2010},
  publisher = {{ACM} Press},
  abstract  = {During code review tasks, comparing two versions of a hardware design description using existing program differencing tools such as diff is inherently limited because existing program differencing tools implicitly assume sequential execution semantics, while hardware description languages are designed to model concurrent computation. We designed a position-independent differencing algorithm to robustly handle language constructs whose relative orderings do not matter. This paper presents Vdiff, an instantiation of this position-independent differencing algorithm for Verilog HDL. To help programmers reason about the differences at a high-level, Vdiff outputs syntactic differences in terms of Verilog-specific change types. We evaluated Vdiff on two open source hardware design projects. The evaluation result shows that Vdiff is very accurate, with overall 96.8% precision and 97.3% recall when using manually classified differences as a basis of comparison.},
  doi       = {10.1145/1858996.1859093},
  url       = {https://doi.org/10.1145%2F1858996.1859093},
}

@InProceedings{Cleland_Huang_2010,
  author    = {Jane Cleland-Huang and Adam Czauderna and Marek Gibiec and John Emenecker},
  booktitle = {Proceedings of the 32nd {ACM}/{IEEE} International Conference on Software Engineering - {ICSE} {\textquotesingle}10},
  title     = {A machine learning approach for tracing regulatory codes to product specific requirements},
  year      = {2010},
  publisher = {{ACM} Press},
  abstract  = {Regulatory standards, designed to protect the safety, security, and privacy of the public, govern numerous areas of software intensive systems. Project personnel must therefore demonstrate that an as-built system meets all relevant regulatory codes. Current methods for demonstrating compliance rely either on after-the-fact audits, which can lead to significant refactoring when regulations are not met, or else require analysts to construct and use traceability matrices to demonstrate compliance. Manual tracing can be prohibitively time-consuming; however automated trace retrieval methods are not very effective due to the vocabulary mismatches that often occur between regulatory codes and product level requirements. This paper introduces and evaluates two machine-learning methods, designed to improve the quality of traces generated between regulatory codes and product level requirements. The first approach uses manually created traceability matrices to train a trace classifier, while the second approach uses web-mining techniques to reconstruct the original trace query. The techniques were evaluated against security regulations from the USA government's Health Insurance Privacy and Portability Act (HIPAA) traced against ten healthcare related requirements specifications. Results demonstrated improvements for the subset of HIPAA regulations that exhibited high fan-out behavior across the requirements datasets.},
  doi       = {10.1145/1806799.1806825},
  groups    = {future},
  url       = {https://doi.org/10.1145%2F1806799.1806825},
}

@InProceedings{Di_Ruscio_2010,
  author    = {Davide Di Ruscio and Ivano Malavolta and Henry Muccini and Patrizio Pelliccione and Alfonso Pierantonio},
  booktitle = {Proceedings of the 32nd {ACM}/{IEEE} International Conference on Software Engineering - {ICSE} {\textquotesingle}10},
  title     = {Developing next generation {ADLs} through {MDE} techniques},
  year      = {2010},
  publisher = {{ACM} Press},
  abstract  = {Despite the flourishing of languages to describe software architectures, existing Architecture Description Languages (ADLs) are still far away from what it is actually needed. In fact, while they support a traditional perception of a Software Architecture (SA) as a set of constituting elements (such as components, connectors and interfaces), they mostly fail to capture multiple stakeholders concerns and their design decisions that represent a broader view of SA being accepted today. Next generation ADLs must cope with various and ever evolving stakeholder concerns by employing semantic extension mechanisms. In this paper we present a framework, called byADL - Build Your ADL, for developing a new generation of ADLs. byADL exploits model-driven techniques that provide the needed technologies to allow a software architect, starting from existing ADLs, to define its own new generation ADL by: i) adding domain specificities, new architectural views, or analysis aspects, ii) integrating ADLs with development processes and methodologies, and iii) customizing ADLs by fine tuning them. The framework is put in practice in different scenarios showing the incremental extension and customization of the Darwin ADL.},
  doi       = {10.1145/1806799.1806816},
  groups    = {future},
  url       = {https://doi.org/10.1145%2F1806799.1806816},
}

@InProceedings{Jaaskelainen_2009,
  author    = {Antti Jaaskelainen and Mika Katara and Antti Kervinen and Mika Maunumaa and Tuula Paakkonen and Tommi Takala and Heikki Virtanen},
  booktitle = {2009 31st International Conference on Software Engineering - Companion Volume},
  title     = {Automatic {GUI} test generation for smartphone applications - an evaluation},
  year      = {2009},
  publisher = {{IEEE}},
  abstract  = {We present the results of an evaluation where we studied the effectiveness of automatic test generation for graphical user interface (GUI) testing of smartphone applications. To describe the context of our evaluation, the tools and the test model library we have developed for the evaluation are also presented. The library contains test models for basic S60 applications, such as camera, contacts, etc. The tools include an on-line test generator that produces sequences of so called keywords to be executed on the test targets. In our evaluation, we managed to find over 20 defects from applications that had been on the market for several months. We also describe the problems we faced during the evaluation.},
  doi       = {10.1109/icse-companion.2009.5070969},
  url       = {https://doi.org/10.1109%2Ficse-companion.2009.5070969},
}

@InProceedings{Kultur_2008,
  author    = {Yigit Kultur and Burak Turhan and Ayse Basar Bener},
  booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} International Symposium on Foundations of software engineering - {SIGSOFT} {\textquotesingle}08/{FSE}-16},
  title     = {{ENNA}},
  year      = {2008},
  publisher = {{ACM} Press},
  doi       = {10.1145/1453101.1453148},
  groups    = {remove},
  url       = {https://doi.org/10.1145%2F1453101.1453148},
}

@InProceedings{Inkumsah_2008,
  author    = {Kobi Inkumsah and Tao Xie},
  booktitle = {2008 23rd {IEEE}/{ACM} International Conference on Automated Software Engineering},
  title     = {Improving Structural Testing of Object-Oriented Programs via Integrating Evolutionary Testing and Symbolic Execution},
  year      = {2008},
  month     = {sep},
  publisher = {{IEEE}},
  abstract  = {Achieving high structural coverage such as branch coverage in object-oriented programs is an important and yet challenging goal due to two main challenges. First, some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics. Second, covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states. Previous work has developed the symbolic execution technique and the evolutionary testing technique to address these two challenges, respectively. However, neither technique was designed to address both challenges at the same time. To address the respective weaknesses of these two previous techniques, we propose a novel framework called Evacon that integrates evolutionary testing (used to search for desirable method sequences) and symbolic execution (used to generate desirable method arguments). We have implemented our framework and applied it to test 13 classes previously used in evaluating white-box test generation tools. The experimental results show that the tests generated using our framework can achieve higher branch coverage than the ones generated by evolutionary testing, symbolic execution, or random testing within the same amount of time.},
  comment   = {check},
  doi       = {10.1109/ase.2008.40},
  url       = {https://doi.org/10.1109%2Fase.2008.40},
}

@InProceedings{Siebra_2008,
  author    = {Clauirton A. Siebra and Paulo H R Costa and Andre L M Santos and Fabio Q B Silva},
  booktitle = {Proceedings of the 13th international conference on Software engineering - {ICSE} {\textquotesingle}08},
  title     = {Improving the handsets network test process via {DMAIC} concepts},
  year      = {2008},
  publisher = {{ACM} Press},
  abstract  = {The wireless network evolution has allowed that the handset technology provides a broad and new set of resources and facilities to their users. However, this evolution is also increasing the number and complexity of testing prior to handsets deployment, so that there is a need to apply methodologies to ensure the quality of the test process while it evolves. This paper relates how the DMAIC framework could be used as an option to ensure and improve the quality of handsets network test processes. DMAIC is a six sigma framework based on measures and statistical analysis, which has commonly been applied during several stages of software development. Our focus, in this paper, is on the definition of the DMAIC phases and how this framework could be useful to stress problems and lead the effort of tests corrections and improvements.},
  doi       = {10.1145/1368088.1368191},
  url       = {https://doi.org/10.1145%2F1368088.1368191},
}

@InProceedings{Brun,
  author    = {Y. Brun and M.D. Ernst},
  booktitle = {Proceedings. 26th International Conference on Software Engineering},
  title     = {Finding latent code errors via machine learning over program executions},
  year      = {2004},
  publisher = {{IEEE} Comput. Soc},
  abstract  = {This paper proposes a technique for identifying program properties that indicate errors. The technique generates machine learning models of program properties known to result from errors, and applies these models to program properties of user-written code to classify and rank properties that may lead the user to errors. Given a set of properties produced by the program analysis, the technique selects a subset of properties that are most likely to reveal an error. An implementation, the fault invariant classifier, demonstrates the efficacy of the technique. The implementation uses dynamic invariant detection to generate program properties. It uses support vector machine and decision tree learning tools to classify those properties. In our experimental evaluation, the technique increases the relevance (the concentration of fault-revealing properties) by a factor of 50 on average for the C programs, and 4.8 for the Java programs. Preliminary experience suggests that most of the fault-revealing properties do lead a programmer to an error.},
  doi       = {10.1109/icse.2004.1317470},
  url       = {https://doi.org/10.1109%2Ficse.2004.1317470},
}

@InProceedings{Ray_2003,
  author    = {A. Ray and R. Cleaveland},
  booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
  title     = {Architectural interaction diagrams: {AIDs} for system modeling},
  year      = {2003},
  publisher = {{IEEE}},
  abstract  = {This paper develops a modeling paradigm called Architectural Interaction Diagrams, or AIDs, for the high-level design of systems containing concurrent, interacting components. The novelty of AIDs is that they introduce interaction mechanisms, or buses, as first-class entities into the modeling vocabulary. Users then have the capability, in their modeling, of using buses whose behavior captures interaction at a higher level of abstraction than that afforded by modeling notations such as Message Sequence Charts or process algebra, which typically provide only one fixed interaction mechanism. This paper defines AIDs formally by giving them an operational semantics that describes how buses combine subsystem transitions into system-level transitions. This semantics enables AIDs to be simulated; to incorporate subsystems given in different modeling notations into a single system model; and to use testing, debugging and model checking early in the system design cycle in order to catch design errors before they are implemented.},
  doi       = {10.1109/icse.2003.1201218},
  groups    = {future},
  url       = {https://doi.org/10.1109%2Ficse.2003.1201218},
}

@InProceedings{Zage_2003,
  author    = {D. Zage and W. Zage},
  booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
  title     = {An analysis of the fault correction process in a large-scale {SDL} production model},
  year      = {2003},
  publisher = {{IEEE}},
  doi       = {10.1109/icse.2003.1201239},
  groups    = {remove},
  url       = {https://doi.org/10.1109%2Ficse.2003.1201239},
}

@InProceedings{Osterweil_2003,
  author    = {Leon J. Osterweil},
  booktitle = {Proceedings of the 9th European software engineering conference held jointly with 10th {ACM} {SIGSOFT} international symposium on Foundations of software engineering - {ESEC}/{FSE} {\textquotesingle}03},
  title     = {Understanding process and the quest for deeper questions in software engineering research},
  year      = {2003},
  publisher = {{ACM} Press},
  abstract  = {This paper provides a brief summary of some overall currents and directions in my research with a particular emphasis on my work in the area of process. Using this perspective as a basis, the paper suggests the importance of using the challenge of grappling with hard technical problems as a basis for searching for deeper questions. The search for deep and enduring problems at the core of our discipline is rewarding for individual researchers and could provide substance, direction, purpose, and credibility for the community.},
  doi       = {10.1145/940071.940073},
  groups    = {future, remove},
  url       = {https://doi.org/10.1145%2F940071.940073},
}

@InProceedings{Zelkowitz,
  author    = {M.V. Zelkowitz and I. Rus},
  booktitle = {Proceedings of the 23rd International Conference on Software Engineering. {ICSE} 2001},
  title     = {Understanding {IV}{\&}{IV} in a safety critical and complex evolutionary environment: the nasa space shuttle program},
  year      = {2001},
  publisher = {{IEEE} Comput. Soc},
  doi       = {10.1109/icse.2001.919108},
  url       = {https://doi.org/10.1109%2Ficse.2001.919108},
}
@inproceedings{Borgida_1999,
	doi = {10.1145/302405.302660},
	url = {https://doi.org/10.1145%2F302405.302660},
	year = 1999,
	publisher = {{ACM} Press},
	author = {Alex Borgida and Premkumar Devanbu},
	title = {Adding more {\textquotedblleft}{DL}{\textquotedblright} to {IDL}},
	booktitle = {Proceedings of the 21st international conference on Software engineering - {ICSE} {\textquotesingle}99}
}

@InProceedings{Di_Nitto_1999,
  author    = {Elisabetta Di Nitto and David Rosenblum},
  booktitle = {Proceedings of the 21st international conference on Software engineering - {ICSE} {\textquotesingle}99},
  title     = {Exploiting {ADLs} to specify architectural styles induced by middleware infrastructures},
  year      = {1999},
  publisher = {{ACM} Press},
  abstract  = {Architecture Dejnition Languages (ADLs) enable the for- malization of the architecture of software systems and the execution of preliminary analyses on them. These analyses aim at supporting the identification and solution of design problems in the early stages of software development. We have used ADLs to describe middleware-induced architec- tural styles. These styles describe the assumptions and con- straints that middleware infrastructures impose on the archi- tecture of systems. Our work originates from the belief that the explicit representation of these styles at the architectural level can guide designers in the definition of an architecture compliant with a pre-selected middleware infrastructure, or, conversely can support designers in the identification of the most suitable middleware infrastructure for a specific archi- tecture. In this paper we provide an evaluation of ADLs as to their suitability for defining middleware-induced architec- tural styles. We identify new requirements for ADLs, and we highlight the importance of existing capabilities. Although our experimentation starts from an attempt to solve a spe- cific problem, the results we have obtained provide general lessons about ADLs, learned from defining the architecture of existing, complex, distributed, running systems},
  doi       = {10.1145/302405.302406},
  url       = {https://doi.org/10.1145%2F302405.302406},
}

@InProceedings{Yida_Mao,
  author    = {Yida Mao and H.A. Sahraoui and H. Lounis},
  booktitle = {Proceedings 13th {IEEE} International Conference on Automated Software Engineering (Cat. No.98EX239)},
  title     = {Reusability hypothesis verification using machine learning techniques: a case study},
  year      = {1998},
  publisher = {{IEEE} Comput. Soc},
  abstract  = {Since the emergence of object technology, organizations have accumulated a tremendous amount of object-oriented (OO) code. Instead of continuing to recreate components that are similar to existing artifacts, and considering the rising costs of development, many organizations would like to decrease software development costs and cycle time by reusing existing OO components. This paper proposes an experiment to verify three hypotheses about the impact of three internal characteristics (inheritance, coupling and complexity) of OO applications on reusability. This verification is done through a machine learning approach (the C4.5 algorithm and a windowing technique). Two kinds of results are produced: (1) for each hypothesis (characteristic), a predictive model is built using a set of metrics derived from this characteristic; and (2) for each predictive model, we measure its completeness, correctness and global accuracy.},
  doi       = {10.1109/ase.1998.732582},
  groups    = {future},
  url       = {https://doi.org/10.1109%2Fase.1998.732582},
}

@InProceedings{Mashayekhi_1994,
  author    = {Vahid Mashayekhi and Chris Feulner and John Riedl},
  booktitle = {Proceedings of the 2nd {ACM} {SIGSOFT} symposium on Foundations of software engineering - {SIGSOFT} {\textquotesingle}94},
  title     = {{CAIS}},
  year      = {1994},
  publisher = {{ACM} Press},
  doi       = {10.1145/193173.195290},
  groups    = {remove},
  url       = {https://doi.org/10.1145%2F193173.195290},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:future\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:ai critic\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:remove\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:positive tracing\;0\;1\;0x8a8a8aff\;\;\;;
}
