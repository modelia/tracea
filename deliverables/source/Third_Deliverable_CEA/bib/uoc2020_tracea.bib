% Encoding: UTF-8

@InCollection{gomez2018-temporalEMF,
  author    = {Abel G{\'{o}}mez and Jordi Cabot and Manuel Wimmer},
  booktitle = {Conceptual Modeling},
  publisher = {Springer International Publishing},
  title     = {{TemporalEMF}: A Temporal Metamodeling Framework},
  year      = {2018},
  pages     = {365--381},
  doi       = {10.1007/978-3-030-00847-5_26},
  url       = {https://doi.org/10.1007/978-3-030-00847-5_26},
}

@book{xtext,
author = {Bettini, Lorenzo},
title = {Implementing Domain Specific Languages with Xtext and Xtend - Second Edition},
year = {2016},
isbn = {1786464969},
publisher = {Packt Publishing},
edition = {2nd},
abstract = {Learn how to implement a DSL with Xtext and Xtend using easy-to-understand examples and best practices About This BookLeverage the latest features of Xtext and Xtend to develop a domain-specific language. Integrate Xtext with popular third party IDEs and get the best out of both worlds. Discover how to test a DSL implementation and how to customize runtime and IDE aspects of the DSL Who This Book Is For This book is targeted at programmers and developers who want to create a domain-specific language with Xtext. They should have a basic familiarity with Eclipse and its functionality. Previous experience with compiler implementation can be helpful but is not necessary since this book will explain all the development stages of a DSL. What You Will LearnWrite Xtext grammar for a DSL; Use Xtend as an alternative to Java to write cleaner, easier-to-read, and more maintainable code; Build your Xtext DSLs easily with Maven/Tycho and Gradle; Write a code generator and an interpreter for a DSL; Explore the Xtext scoping mechanism for symbol resolution; Test most aspects of the DSL implementation with JUnit; Understand best practices in DSL implementations with Xtext and Xtend; Develop your Xtext DSLs using Continuous Integration mechanisms; Use an Xtext editor in a web applicationIn DetailXtext is an open source Eclipse framework for implementing domain-specific languages together with IDE functionalities. It lets you implement languages really quickly; most of all, it covers all aspects of a complete language infrastructure, including the parser, code generator, interpreter, and more. This book will enable you to implement Domain Specific Languages (DSL) efficiently, together with their IDE tooling, with Xtext and Xtend. Opening with brief coverage of Xtext features involved in DSL implementation, including integration in an IDE, the book will then introduce you to Xtend as this language will be used in all the examples throughout the book. You will then explore the typical programming development workflow with Xtext when we modify the grammar of the DSL. Further, the Xtend programming language (a fully-featured Java-like language tightly integrated with Java) will be introduced. We then explain the main concepts of Xtext, such as validation, code generation, and customizations of runtime and UI aspects. You will have learned how to test a DSL implemented in Xtext with JUnit and will progress to advanced concepts such as type checking and scoping. You will then integrate the typical Continuous Integration systems built in to Xtext DSLs and familiarize yourself with Xbase. By the end of the book, you will manually maintain the EMF model for an Xtext DSL and will see how an Xtext DSL can also be used in IntelliJ. Style and approach A step-by step-tutorial with illustrative examples that will let you master using Xtext and implementing DSLs with its custom language, Xtend.}
}

@misc{Tracea_Repo,
  author = {Edouard R. Batot},
  title = {Tracea},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/modelia/tracea}}
}
@misc{Capra_Repo,
  author = {Jan-Philipp Steghofer and Salome Maro},
  title = {Tracea},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://git.eclipse.org/c/capra/org.eclipse.capra.git/}}
}


@misc{web:som,
  author = {SOM-Research Lab},
  title = {{SOM-Research Lab website}},
  note = {\textsc{url}: \url{https://som-research.uoc.edu/}, last accessed Jan. 1st 1970},
}


@book{EMF, 
author = {Steinberg, David and Budinsky, Frank and Paternostro, Marcelo and Merks, Ed},
title = {EMF: Eclipse Modeling Framework 2.0},
year = {2009},
isbn = {0321331885},
publisher = {Addison-Wesley Professional},
edition = {2nd}
}

@book{wordnet,
    title = {WordNet: An Electronic Lexical Database},
    author = {Christiane Fellbaum},
    year = {1998},
    publisher = {Bradford Books},
    url = {https://wordnet.princeton.edu/}
}




@inproceedings{burgueno2019-uncertainty,
author = {Burgue\~{n}o, Loli and Claris\'{o}, Robert and Cabot, Jordi and G\'{e}rard, S\'{e}bastien and Vallecillo, Antonio},
title = {Belief Uncertainty in Software Models},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MiSE.2019.00011},
doi = {10.1109/MiSE.2019.00011},
abstract = {This paper discusses the representation of Belief Uncertainty in software models. This kind of uncertainty refers to the situation in which the modeler, or any other belief agent, is uncertain about the behavior of the system, or the statements that the model expresses about it. In this work, we propose to assign a degree of belief to model statements (let they be constraints, or any other model expression), which is expressed by a probability (called credence, in statistical terms) that represents a quantification of such a subjective degree of belief. We discuss how it can be represented using current modeling notations, and how to operate with it in order to make informed decisions.},
booktitle = {Proceedings of the 11th International Workshop on Modelling in Software Engineerings},
pages = {19–26},
numpages = {8},
keywords = {degree of belief, software models, uncertainty},
location = {Montreal, Quebec, Canada},
series = {MiSE '19}
}
@BOOK{rumbaugh2004-UML2,
  title = {Unified Modeling Language Reference Manual, The (2nd Edition)},
  publisher = {Pearson Higher Education},
  year = {2004},
  author = {Rumbaugh, James and Jacobson, Ivar and Booch, Grady},
  isbn = {0321245628}
}
@InProceedings{patel2015-hierarchical-clustering,
  author    = {S. {Patel} and S. {Sihmar} and A. {Jatain}},
  booktitle = {2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)},
  title     = {A study of hierarchical clustering algorithms},
  year      = {2015},
  month     = {March},
  pages     = {537-541},
  abstract  = {Clustering algorithm plays a vital role in organizing large amount of information into small number of clusters which provides some meaningful information. Clustering is a process of categorizing set of objects into groups called clusters. Hierarchical clustering is a method of cluster analysis which is used to build hierarchy of clusters. This paper focuses on hierarchical agglomerative clustering. In this paper, we also explain some agglomerative algorithms and their comparison.},
  keywords  = {pattern clustering;set theory;object set categorization;cluster analysis;hierarchical agglomerative clustering algorithms;Clustering algorithms;Algorithm design and analysis;Heuristic algorithms;Partitioning algorithms;Couplings;Complexity theory;Rocks;Agglomerative;Dendrogram;Hierarchical clusterin},
}

@InProceedings{clelandhuang2005-supporting-evidence-dynamic-req-tracing,
  author    = {J. {Cleland-Huang} and R. {Settimi} and {Chuan Duan} and {Xuchang Zou}},
  booktitle = {13th IEEE International Conference on Requirements Engineering (RE'05)},
  title     = {Utilizing supporting evidence to improve dynamic requirements traceability},
  year      = {2005},
  month     = {Aug},
  pages     = {135-144},
  abstract  = {Requirements traceability provides critical support throughout all phases of a software development project. However practice has repeatedly shown the difficulties involved in long term maintenance of traditional traceability matrices. Dynamic retrieval methods minimize the need for creating and maintaining explicit links and can significantly reduce the effort required to perform a manual trace. Unfortunately they suffer from recall and precision problems. This paper introduces three strategies for incorporating supporting information into a probabilistic retrieval algorithm in order to improve the performance of dynamic requirements traceability. The strategies include hierarchical modeling, logical clustering of artifacts, and semi-automated pruning of the probabilistic network. Experimental results indicate that enhancement strategies can be used effectively to improve trace retrieval results thereby increasing the practicality of utilizing dynamic trace retrieval methods.},
  doi       = {10.1109/RE.2005.78},
  issn      = {2332-6441},
  keywords  = {systems analysis;dynamic requirements traceability;traceability matrices;dynamic retrieval;supporting information;probabilistic retrieval algorithm;hierarchical modeling;artifacts logical clustering;semiautomated pruning;probabilistic network;dynamic trace retrieval;Information retrieval;Clustering algorithms;Maintenance engineering;Programming;Software engineering;Engineering management;Failure analysis;Counting circuits;Performance analysis;Silver},
}

@Article{zou2010-term-based-enhancement-for-trace-retrieval,
  author   = {Zou, Xuchang and Settimi, Raffaella and Cleland-Huang, Jane},
  journal  = {Empirical Software Engineering},
  title    = {Improving automated requirements trace retrieval: a study of term-based enhancement methods},
  year     = {2010},
  issn     = {1573-7616},
  number   = {2},
  pages    = {119--146},
  volume   = {15},
  abstract = {Automated requirements traceability methods that utilize Information Retrieval (IR) methods to generate and maintain traceability links are often more efficient than traditional manual approaches, however the traces they generate are imprecise and significant human effort is needed to evaluate and filter the results. This paper investigates and compares three term-based enhancement methods that are designed to improve the performance of a probabilistic automated tracing tool. Empirical studies show that the enhancement methods can be effective in increasing the accuracy of the retrieved traces; however the effectiveness of each method varies according to specific project characteristics. The analysis of such characteristics has lead to the development of two new project-level metrics which can be used to predict the effectiveness of each enhancement method for a given data set. A procedure to automatically extract critical keywords and phrases from a set of traceable artifacts is also presented to enhance the automated trace retrieval algorithm. The procedure is tested on two new datasets.},
  doi      = {10.1007/s10664-009-9114-z},
  refid    = {Zou2010},
  url      = {https://doi.org/10.1007/s10664-009-9114-z},
}

@Article{clelandhuang2007-classification-non-functional-req,
  author   = {Cleland-Huang, Jane and Settimi, Raffaella and Zou, Xuchang and Solc, Peter},
  journal  = {Requirements Engineering},
  title    = {Automated classification of non-functional requirements},
  year     = {2007},
  issn     = {1432-010X},
  number   = {2},
  pages    = {103--120},
  volume   = {12},
  abstract = {This paper describes a technique for automating the detection and classification of non-functional requirements related to properties such as security, performance, and usability. Early detection of non-functional requirements enables them to be incorporated into the initial architectural design instead of being refactored in at a later date. The approach is used to detect and classify stakeholders’ quality concerns across requirements specifications containing scattered and non-categorized requirements, and also across freeform documents such as meeting minutes, interview notes, and memos. This paper first describes the classification algorithm and then evaluates its effectiveness through reporting a series of experiments based on 30 requirements specifications developed as term projects by MS students at DePaul University. A new and iterative approach is then introduced for training or retraining a classifier to detect and classify non-functional requirements (NFR) in datasets dissimilar to the initial training sets. This approach is evaluated against a large free-form requirements document obtained from Siemens Logistics and Automotive Organization. Although to the NFR classifier is unable to detect all of the NFRs, it is useful for supporting an analyst in the error-prone task of manually discovering NFRs, and furthermore can be used to quickly analyse large and complex documents in order to search for NFRs.},
  doi      = {10.1007/s00766-007-0045-1},
  refid    = {Cleland-Huang2007},
  url      = {https://doi.org/10.1007/s00766-007-0045-1},
}

@Article{ozkaya2020-what-is-differenet-in-AI-enabled-systems,
  author   = {I. {Ozkaya}},
  journal  = {IEEE Software},
  title    = {What Is Really Different in Engineering AI-Enabled Systems?},
  year     = {2020},
  issn     = {1937-4194},
  month    = {July},
  number   = {4},
  pages    = {3-6},
  volume   = {37},
  abstract = {Advances in machine learning (ML) algorithms and increasing availability of computational power have resulted in huge investments in systems that aspire to exploit artificial intelligence (AI), in particular ML. AIenabled systems, software-reliant systems that include data and components that implement algorithms mimicking learning and problem solving, have inherently different characteristics than software systems alone.1 However, the development and sustainment of such systems also have many parallels with building, deploying, and sustaining software systems. A common observation is that although software systems are deterministic and you can build and test to a specification, AI-enabled systems, in particular those that include ML components, are generally probabilistic. Systems with ML components can have a high margin of error due to the uncertainty that often follows predictive algorithms. The margin of error can be related to the inability to predict the result in advance or the same result cannot be reproduced. This characteristic makes AI-enabled systems hard to test and verify.2 Consequently, it is easy to assume that what we know about designing and reasoning about software systems does not immediately apply in AI engineering. AI-enabled systems are software systems. The sneaky part about engineering AI systems is they are "just like" conventional software systems we can design and reason about until they?re not.},
  doi      = {10.1109/MS.2020.2993662},
}

@Article{shen2015-linking-entities-with-knowledge-base,
  author   = {W. {Shen} and J. {Wang} and J. {Han}},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  title    = {Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions},
  year     = {2015},
  issn     = {1558-2191},
  month    = {Feb},
  number   = {2},
  pages    = {443-460},
  volume   = {27},
  abstract = {The large number of potential applications from bridging web data with knowledge bases have led to an increase in the entity linking research. Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge base. Potential applications include information extraction, information retrieval, and knowledge base population. However, this task is challenging due to name variations and entity ambiguity. In this survey, we present a thorough overview and analysis of the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future directions.},
  doi      = {10.1109/TKDE.2014.2327028},
  keywords = {knowledge based systems;text analysis;information extraction;information retrieval;knowledge base population;entity linking systems;Web data;Joining processes;Knowledge based systems;Encyclopedias;Internet;Electronic publishing;Couplings;Entity linking;entity disambiguation;knowledge base},
}

@book{kleppe2008-DSLs-with-metamodels,
author = {Kleppe, Anneke},
title = {Software Language Engineering: Creating Domain-Specific Languages Using Metamodels},
year = {2008},
isbn = {0321553454},
publisher = {Addison-Wesley Professional},
edition = {1},
abstract = {Software practitioners are rapidly discovering the immense value of Domain-Specific Languages (DSLs) in solving problems within clearly definable problem domains. Developers are applying DSLs to improve productivity and quality in a wide range of areas, such as finance, combat simulation, macro scripting, image generation, and more. But until now, there have been few practical resources that explain how DSLs work and how to construct them for optimal use. Software Language Engineering fills that need. Written by expert DSL consultant Anneke Kleppe, this is the first comprehensive guide to successful DSL design. Kleppe systematically introduces and explains every ingredient of an effective language specification, including its description of concepts, how those concepts are denoted, and what those concepts mean in relation to the problem domain. Kleppe carefully illuminates good design strategy, showing how to maximize the flexibility of the languages you create. She also demonstrates powerful techniques for creating new DSLs that cooperate well with general-purpose languages and leverage their power. Completely tool-independent, this book can serve as the primary resource for readers using Microsoft DSL tools, the Eclipse Modeling Framework, openArchitectureWare, or any other DSL toolset. It contains multiple examples, an illustrative running case study, and insights and background information drawn from Kleppes leading-edge work as a DSL researcher. Specific topics covered include Discovering the types of problems that DSLs can solve, and when to use them Comparing DSLs with general-purpose languages, frameworks, APIs, and other approaches Understanding the roles and tools available to language users and engineers Creating each component of a DSL specification Modeling both concrete and abstract syntax Understanding and describing language semantics Defining textual and visual languages based on object-oriented metamodeling and graph transformations Using metamodels and associated tools to generate grammars Integrating object-oriented modeling with graph theory Building code generators for new languages Supporting multilanguage models and programs This book provides software engineers with all the guidance they need to create DSLs that solve real problems more rapidly, and with higher-quality code.}
}

  


@incollection{Spanoudakis2005,
  doi = {10.1142/9789812775245_0014},
  url = {https://doi.org/10.1142/9789812775245_0014},
  year = {2005},
  month = aug,
  publisher = {{World} {Scientific}},
  pages = {395--428},
  author = {George Spanoudakis and Andrea Zisman},
  title = {{Software} {Traceability}: A {Roadmap}},
  booktitle = {Handbook Of Software Engineering And Knowledge Engineering}
}

@InProceedings{lohar2013,
  author    = {Lohar, Sugandha and Amornborvornwong, Sorawit and Zisman, Andrea and Cleland-Huang, Jane},
  booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
  title     = {Improving Trace Accuracy through Data-Driven Configuration and Composition of Tracing Features},
  year      = {2013},
  address   = {New York, NY, USA},
  pages     = {378--388},
  publisher = {Association for Computing Machinery},
  series    = {ESEC/FSE 2013},
  abstract  = {Software traceability is a sought-after, yet often elusive quality in large software-intensive systems primarily because the cost and effort of tracing can be overwhelming. State-of-the art solutions address this problem through utilizing trace retrieval techniques to automate the process of creating and maintaining trace links. However, there is no simple one- size-fits all solution to trace retrieval. As this paper will show, finding the right combination of tracing techniques can lead to significant improvements in the quality of generated links. We present a novel approach to trace retrieval in which the underlying infrastructure is configured at runtime to optimize trace quality. We utilize a machine-learning approach to search for the best configuration given an initial training set of validated trace links, a set of available tracing techniques specified in a feature model, and an architecture capable of instantiating all valid configurations of features. We evaluate our approach through a series of experiments using project data from the transportation, healthcare, and space exploration domains, and discuss its implementation in an industrial environment. Finally, we show how our approach can create a robust baseline against which new tracing techniques can be evaluated.},
  doi       = {10.1145/2491411.2491432},
  isbn      = {9781450322379},
  keywords  = {trace configuration, Trace retrieval, configuration},
  location  = {Saint Petersburg, Russia},
  numpages  = {11},
}

@Article{hayes2006,
  author   = {J. H. {Hayes} and A. {Dekhtyar} and S. K. {Sundaram}},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Advancing candidate link generation for requirements tracing: the study of methods},
  year     = {2006},
  number   = {1},
  pages    = {4-19},
  volume   = {32},
  abstract = {This paper addresses the issues related to improving the overall quality of the dynamic candidate link generation for the requirements tracing process for verification and validation and independent verification and validation analysts. The contribution of the paper is four-fold: we define goals for a tracing tool based on analyst responsibilities in the tracing process, we introduce several new measures for validating that the goals have been satisfied, we implement analyst feedback in the tracing process, and we present a prototype tool that we built, RETRO (REquirements TRacing On-target), to address these goals. We also present the results of a study used to assess RETRO's support of goals and goal elements that can be measured objectively.},
  url      = {https://doi.org/10.1109/TSE.2006.3},
}
  
  @inproceedings{guo2016coldStart,
author = {Guo, Jin and Rahimi, Mona and Cleland-Huang, Jane and Rasin, Alexander and Hayes, Jane Huffman and Vierhauser, Michael},
title = {Cold-Start Software Analytics},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901740},
doi = {10.1145/2901739.2901740},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {142–153},
numpages = {12},
keywords = {software analytics, cold-start, configuration},
location = {Austin, Texas},
series = {MSR ’16}
}

@InBook{Gotel2012,
  author     = {Gotel, Orlena and Cleland-Huang, Jane and Hayes, Jane Huffman and Zisman, Andrea and Egyed, Alexander and Gr{\"u}nbacher, Paul and Dekhtyar, Alex and Antoniol, Giuliano and Maletic, Jonathan and M{\"a}der, Patrick},
  pages      = {3--22},
  publisher  = {Springer London},
  title      = {Traceability Fundamentals},
  year       = {2012},
  address    = {London},
  isbn       = {978-1-4471-2239-5},
  abstract   = {This chapter seeks to provide a reference resource on traceability fundamentals. It defines the essential traceability terminology of trace, trace artifact, trace link, traceability and tracing, and is supplemented by an extensive glossary that has been developed and endorsed by members of the traceability community. This chapter also offers a model of a generic traceability process and describes the essential activities involved in the life cycle of a trace. This model has been used as a frame of reference for articulating the grand challenge of traceability, as reported in the chapter by Gotel et al. of this book. The chapter also describes the basic types of traceability and explains a number of key associated concepts.},
  booktitle  = {Software and Systems Traceability},
  doi        = {10.1007/978-1-4471-2239-5_1},
  groups     = {meta},
  keywords   = {read},
  readstatus = {read},
  url        = {https://doi.org/10.1007/978-1-4471-2239-5_1},
}

 @inproceedings{mei2007labelingMultinomalTopicModels,
author = {Mei, Qiaozhu and Shen, Xuehua and Zhai, ChengXiang},
title = {Automatic Labeling of Multinomial Topic Models},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281246},
doi = {10.1145/1281192.1281246},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {490–499},
numpages = {10},
keywords = {multinomial distribution, statistical topic models, topic model labeling},
location = {San Jose, California, USA},
series = {KDD ’07}
}
  @INPROCEEDINGS{li2013ontologybasedTraceRetrieval,  author={Y. {Li} and J. {Cleland-Huang}},  booktitle={2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)},   title={Ontology-based trace retrieval},   year={2013},  volume={},  number={},  pages={30-36},}

@Article{dashofy2005xmlbasedArchitectureDSL,
  author     = {Dashofy, Eric M. and Hoek, Andr\'{e} van der and Taylor, Richard N.},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  title      = {A Comprehensive Approach for the Development of Modular Software Architecture Description Languages},
  year       = {2005},
  issn       = {1049-331X},
  month      = apr,
  number     = {2},
  pages      = {199–245},
  volume     = {14},
  address    = {New York, NY, USA},
  doi        = {10.1145/1061254.1061258},
  issue_date = {April 2005},
  keywords   = {ArchStudio 3, xADL 2.0, XML, Architecture description languages, read},
  numpages   = {47},
  publisher  = {Association for Computing Machinery},
  readstatus = {read},
  url        = {https://doi.org/10.1145/1061254.1061258},
}

@Article{clelandhuang2007bestPracticeForAutomatedTraceability,
  author   = {J. {Cleland-Huang} and B. {Berenbach} and S. {Clark} and R. {Settimi} and E. {Romanova}},
  journal  = {Computer},
  title    = {Best Practices for Automated Traceability},
  year     = {2007},
  number   = {6},
  pages    = {27-35},
  volume   = {40},
  abstract = {Automated traceability applies information-retrieval techniques to generate candidate links, sharply reducing the effort of manual approaches to build and maintain a requirements trace matrix as well as providing after-the-fact traceability in legacy documents.The authors describe nine best practices for implementing effective automated traceability.},
  file     = {:clelandhuang2007_Best practice for automated traceability.pdf:PDF;:C\:/Users/Edouard/Dropbox/Work/UOC/Tracea/biblio/clelandhuang2007_Best practice for automated traceability.pdf:PDF},
  groups   = {meta},
}

@Article{dit2013informationRetrievalTraceabilityForFeatureLocation,
  author    = {Bogdan Dit and Meghan Revelle and Denys Poshyvanyk},
  journal   = {Empirical Software Engineering},
  title     = {Integrating information retrieval, execution and link analysis algorithms to improve feature location in software},
  year      = {2013},
  number    = {2},
  pages     = {277--309},
  volume    = {18},
  abstract  = {Data fusion is the process of integrating multiple sources of information such that their combination yields better results than if the data sources are used individually. This paper applies the idea of data fusion to feature location, the process of identifying the source code that implements specific functionality in software. A data fusion model for feature location is presented which defines new feature location techniques based on combining information from textual, dynamic, and web mining or link analyses algorithms applied to software. A novel contribution of the proposed model is the use of advanced web mining algorithms to analyze execution information during feature location. The results of an extensive evaluation on three Java systems indicate that the new feature location techniques based on web mining improve the effectiveness of existing approaches by as much as 87%.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/ese/DitRP13.bib},
  doi       = {10.1007/s10664-011-9194-4},
  groups    = {Analysis, Conceptualisation},
  timestamp = {Sun, 28 May 2017 13:22:43 +0200},
  url       = {https://doi.org/10.1007/s10664-011-9194-4},
}

@article{buckland94precisionRecall,
  author    = {Michael K. Buckland and
               Fredric C. Gey},
  title     = {The Relationship between Recall and Precision},
  journal   = {{JASIS}},
  volume    = {45},
  number    = {1},
  pages     = {12--19},
  year      = {1994},
  url       = {https://doi.org/10.1002/(SICI)1097-4571(199401)45:1\<12::AID-ASI2\>3.0.CO;2-L},
  doi       = {10.1002/(SICI)1097-4571(199401)45:1\<12::AID-ASI2\>3.0.CO;2-L},
  timestamp = {Fri, 30 Nov 2018 13:29:11 +0100},
  biburl    = {https://dblp.org/rec/journals/jasis/BucklandG94.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
  
@article{bengio2012gradientBasedTraining,
  author    = {Yoshua Bengio},
  title     = {Practical recommendations for gradient-based training of deep architectures},
  journal   = {CoRR},
  volume    = {abs/1206.5533},
  year      = {2012},
  url       = {http://arxiv.org/abs/1206.5533},
  archivePrefix = {arXiv},
  eprint    = {1206.5533},
  timestamp = {Mon, 13 Aug 2018 16:47:20 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1206-5533.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{guo2013expertSystemInTraceabilityDSL,
  author     = {J. {Guo} and J. {Cleland-Huang} and B. {Berenbach}},
  booktitle  = {2013 21st IEEE International Requirements Engineering Conference (RE)},
  title      = {Foundations for an expert system in domain-specific traceability},
  year       = {2013},
  pages      = {42-51},
  abstract   = {Attempts to utilize information retrieval techniques to fully automate the creation of traceability links have been hindered by terminology mismatches between source and target artifacts. Therefore, current trace retrieval algorithms tend to produce imprecise and incomplete results. In this paper we address this mismatch by proposing an expert system which integrates a knowledge base of domain concepts and their relationships, a set of logic rules for defining relationships between artifacts based on these rules, and a process for mapping artifacts into a structure against which the rules can be applied. This paper lays down the core foundations needed to integrate an expert system into the automated tracing process. We construct a knowledge base and inference rules for part of a large industrial project in the transportation domain and empirically show that our approach significantly improves precision and recall of the generated trace links.},
  groups     = {Analysis},
  keywords   = {read},
  readstatus = {read},
}

@Book{tekinerdogan2017engineeringIntelligenceSocioTechnical,
  author    = {Bedir Tekinerdogan},
  publisher = {Wageningen University \& Research},
  title     = {Engineering connected intelligence : a socio-technical perspective},
  year      = {2017},
  note      = {Inaugural lecture Wageningen University \& Research, 2 February 2017},
  doi       = {10.18174/401115},
  keywords  = {smart systems, system-of-systems, socio-technical systems, industry 4.0, Internet of Things},
  language  = {English},
}

@InProceedings{tekinerdogan2007-metamodel-for-tracing-concers-accross-life-cycle,
  author     = {Tekinerdo{\u{g}}an, Bedir and Hofmann, Christian and Ak{\c{s}}it, Mehmet and Bakker, Jethro},
  booktitle  = {Early Aspects: Current Challenges and Future Directions},
  title      = {Metamodel for Tracing Concerns Across the Life Cycle},
  year       = {2007},
  address    = {Berlin, Heidelberg},
  editor     = {Moreira, Ana and Grundy, John},
  pages      = {175--194},
  publisher  = {Springer Berlin Heidelberg},
  isbn       = {978-3-540-76811-1},
  keywords   = {read},
  readstatus = {read},
}

@Article{poshyvanyk2007,
  author  = {D. {Poshyvanyk} and Y. {Gueheneuc} and A. {Marcus} and G. {Antoniol} and V. {Rajlich}},
  journal = {IEEE Transactions on Software Engineering},
  title   = {Feature Location Using Probabilistic Ranking of Methods Based on Execution Scenarios and Information Retrieval},
  year    = {2007},
  number  = {6},
  pages   = {420-432},
  volume  = {33},
}

@InProceedings{keenan2012-workbench-for-traceability,
  author    = {E. {Keenan} and A. {Czauderna} and G. {Leach} and J. {Cleland-Huang} and Y. {Shin} and E. {Moritz} and M. {Gethers} and D. {Poshyvanyk} and J. {Maletic} and J. H. {Hayes} and A. {Dekhtyar} and D. {Manukian} and S. {Hossein} and D. {Hearn}},
  booktitle = {2012 34th International Conference on Software Engineering (ICSE)},
  title     = {TraceLab: An experimental workbench for equipping researchers to innovate, synthesize, and comparatively evaluate traceability solutions},
  year      = {2012},
  month     = {June},
  pages     = {1375-1378},
  abstract  = {TraceLab is designed to empower future traceability research, through facilitating innovation and creativity, increasing collaboration between researchers, decreasing the startup costs and effort of new traceability research projects, and fostering technology transfer. To this end, it provides an experimental environment in which researchers can design and execute experiments in TraceLab's visual modeling environment using a library of reusable and user-defined components. TraceLab fosters research competitions by allowing researchers or industrial sponsors to launch research contests intended to focus attention on compelling traceability challenges. Contests are centered around specific traceability tasks, performed on publicly available datasets, and are evaluated using standard metrics incorporated into reusable TraceLab components. TraceLab has been released in beta-test mode to researchers at seven universities, and will be publicly released via CoEST.org in the summer of 2012. Furthermore, by late 2012 TraceLab's source code will be released as open source software, licensed under GPL. TraceLab currently runs on Windows but is designed with cross platforming issues in mind to allow easy ports to Unix and Mac environments.},
  doi       = {10.1109/ICSE.2012.6227244},
  issn      = {1558-1225},
  keywords  = {computerised instrumentation;innovation management;technology transfer;Unix;TraceLab;technology transfer;traceability research projects;visual modeling environment;open source software;GPL;Unix environment;Mac environment;Measurement;Software engineering;Principal component analysis;Software;Benchmark testing;Libraries;Java;Traceability;Instrumentation;TraceLab;Benchmarks;Experiments;eXtreme Software Engineering Lab},
}

@InProceedings{florez2019-finegrained-req2code,
  author    = {J. M. {Florez}},
  booktitle = {2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},
  title     = {Automated Fine-Grained Requirements-to-Code Traceability Link Recovery},
  year      = {2019},
  month     = {May},
  pages     = {222-225},
  abstract  = {Problem: Existing approaches for requirements-to-code traceability link recovery rely on text retrieval to trace requirements to coarse-grained code documents (e.g., methods, files, classes, etc.), while suffering from low accuracy problems. Hypotheses: The salient information in most requirements is expressed as functional constraints, which can be automatically identified and categorized. Moreover, people use recognizable discourse patterns when describing them and developers use well-defined patterns for implementing them. Contributions: Recasting the requirements-to-code traceability link problem as an accurate matching between functional constraints and their implementation.},
  doi       = {10.1109/ICSE-Companion.2019.00087},
  issn      = {2574-1934},
  keywords  = {information retrieval;text analysis;functional constraints;requirements-to-code traceability link problem;text retrieval;coarse-grained code documents;automated fine-grained requirements-to-code traceability link recovery;traceability;static analysis;discourse analysis;qualitative analysis},
}

@InProceedings{rempl2014-conformance-of-traceability-to-guidelines,
  author    = {Rempel, Patrick and M\"{a}der, Patrick and Kuschke, Tobias and Cleland-Huang, Jane},
  booktitle = {Proceedings of the 36th International Conference on Software Engineering},
  title     = {Mind the Gap: Assessing the Conformance of Software Traceability to Relevant Guidelines},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {943–954},
  publisher = {Association for Computing Machinery},
  series    = {ICSE 2014},
  abstract  = {Many guidelines for safety-critical industries such as aeronautics, medical devices, and railway communications, specify that traceability must be used to demonstrate that a rigorous process has been followed and to provide evidence that the system is safe for use. In practice, there is a gap between what is prescribed by guidelines and what is implemented in practice, making it difficult for organizations and certifiers to fully evaluate the safety of the software system. In this paper we present an approach, which parses a guideline to extract a Traceability Model depicting software artifact types and their prescribed traces. It then analyzes the traceability data within a project to identify areas of traceability failure. Missing traceability paths, redundant and/or inconsistent data, and other problems are highlighted. We used our approach to evaluate the traceability of seven safety-critical software systems and found that none of the evaluated projects contained traceability that fully conformed to its relevant guidelines.},
  doi       = {10.1145/2568225.2568290},
  isbn      = {9781450327565},
  keywords  = {software and system safety, compliance, guideline, inspection, Software traceability, conformance, certification, failure patterns, safety critical, safety, assessment},
  location  = {Hyderabad, India},
  numpages  = {12},
  url       = {https://doi.org/10.1145/2568225.2568290},
}

@InProceedings{rath2018-guo-augmenting-incomplete-traces,
  author    = {Rath, Michael and Rendall, Jacob and Guo, Jin L. C. and Cleland-Huang, Jane and M\"{a}der, Patrick},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  title     = {Traceability in the Wild: Automatically Augmenting Incomplete Trace Links},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {834–845},
  publisher = {Association for Computing Machinery},
  series    = {ICSE ’18},
  abstract  = {Software and systems traceability is widely accepted as an essential element for supporting many software development tasks. Today's version control systems provide inbuilt features that allow developers to tag each commit with one or more issue ID, thereby providing the building blocks from which project-wide traceability can be established between feature requests, bug fixes, commits, source code, and specific developers. However, our analysis of six open source projects showed that on average only 60% of the commits were linked to specific issues. Without these fundamental links the entire set of project-wide links will be incomplete, and therefore not trustworthy. In this paper we address the fundamental problem of missing links between commits and issues. Our approach leverages a combination of process and text-related features characterizing issues and code changes to train a classifier to identify missing issue tags in commit messages, thereby generating the missing links. We conducted a series of experiments to evaluate our approach against six open source projects and showed that it was able to effectively recommend links for tagging issues at an average of 96% recall and 33% precision. In a related task for augmenting a set of existing trace links, the classifier returned precision at levels greater than 89% in all projects and recall of 50%.},
  doi       = {10.1145/3180155.3180207},
  isbn      = {9781450356381},
  keywords  = {link recovery, traceability, open source, machine learning},
  location  = {Gothenburg, Sweden},
  numpages  = {12},
  url       = {https://doi.org/10.1145/3180155.3180207},
}

@InBook{bonde2006-different-levels-of-abstraction,
  author    = {Bond{\'e}, Lossan and Boulet, Pierre and Dekeyser, Jean-Luc},
  editor    = {Vachoux, A.},
  pages     = {263--276},
  publisher = {Springer Netherlands},
  title     = {Traceability and Interoperability at Different Levels of Abstraction in Model-Driven Engineering},
  year      = {2006},
  address   = {Dordrecht},
  isbn      = {978-1-4020-4998-9},
  booktitle = {Applications of Specification and Design Languages for SoCs: Selected papers from FDL 2005},
  doi       = {10.1007/978-1-4020-4998-9_15},
  url       = {https://doi.org/10.1007/978-1-4020-4998-9_15},
}

@InProceedings{tinnes2019-improving-art-reuse-with-traceability,
  author    = {C. {Tinnes} and A. {Biesdorf} and U. {Hohenstein} and F. {Matthes}},
  booktitle = {2019 IEEE/ACM 10th International Symposium on Software and Systems Traceability (SST)},
  title     = {Ideas on Improving Software Artifact Reuse via Traceability and Self-Awareness},
  year      = {2019},
  month     = {May},
  pages     = {13-16},
  abstract  = {We describe our vision towards automatic software and system development and argue that reusing knowledge from existing projects as well as traceability between corresponding artifacts are important steps towards this vision. We furthermore list barriers that are currently experienced with software artifact reuse and traceability in industry and suggest some ideas to overcome these barriers.},
  doi       = {10.1109/SST.2019.00013},
  file      = {:tinnes2019-improving-art-reuse-with-traceability.pdf:PDF},
  issn      = {2157-2194},
  keywords  = {software maintenance;software quality;software reusability;traceability;self-awareness;automatic software;system development;reusing knowledge;corresponding artifacts;software artifact reuse;Software;Tools;Task analysis;Computational modeling;Engines;Self-aware;Natural languages;Software Reuse;Traceability;Self Aware Systems;Natural Language Processing;Architecture Knowledge Management},
}

@InProceedings{seiler2019-comparing-trac-through-IR-Commits-Logs,
  author    = {M. {Seiler} and P. {Hübner} and B. {Paech}},
  booktitle = {2019 IEEE/ACM 10th International Symposium on Software and Systems Traceability (SST)},
  title     = {Comparing Traceability through Information Retrieval, Commits, Interaction Logs, and Tags},
  year      = {2019},
  month     = {May},
  pages     = {21-28},
  abstract  = {Context and motivation: Traceability is used to follow and understand the relationships between various software engineering artifacts such as requirements and source code. Comprehensive traceability of software engineering artifacts is important to ensure that a software can be further developed or maintained. Traceability links are often created manually by using commit ids or retrospectively by using information retrieval (IR). Question/Problem: However, creating traceability links manually is costly and it is error-prone in retrospect. As part of our ongoing research on feature management where traceability is also of interest, we use a lightweight tagging approach to relate artifacts. We want to investigate how such a tag-based approach compares to approaches using commit ids, interaction logs (IL), and IR for creating traceability links. Principal ideas/results: We conducted a case study in which students applied the tag-based, the IL-based, and the commit-based approach. We transformed the tags to traceability links and compared them with the commit-based and IL-based approach as well as with IR-based approaches, using a gold standard. We applied different improvements. Contribution: This is the first study comparing four completely different traceability approaches in one project. The combination of IL and commit ids shows the best precision and recall but is intrusive. The other approaches differ less in precision and recall and both need improvement for practical application. We discuss the benefits and drawbacks of the different approaches and state implications for research and practice.},
  doi       = {10.1109/SST.2019.00015},
  issn      = {2157-2194},
  keywords  = {information retrieval;program diagnostics;software maintenance;information retrieval;interaction tags;IR-based approaches;commit-based IL-based approach;tag-based approach;commit ids;traceability links;software engineering artifacts;interaction logs;Large scale integration;Software engineering;Tagging;Software;Distance measurement;Gold;traceability;tagging;interaction logs;information retrieval},
}

@InProceedings{farrar2019-comparing-stemming-technics,
  author    = {D. {Farrar} and J. {Huffman Hayes}},
  booktitle = {2019 IEEE/ACM 10th International Symposium on Software and Systems Traceability (SST)},
  title     = {A Comparison of Stemming Techniques in Tracing},
  year      = {2019},
  month     = {May},
  pages     = {37-44},
  abstract  = {We examine the effects of stemming on the tracing of software engineering artifacts. We compare two common stemming algorithms to each other as well as to a baseline of no stemming. We evaluate the algorithms on eight tracing datasets. We run the experiment using the TraceLab experimental framework to allow for ease of repeatability and knowledge sharing among the tracing community. We compare the algorithms on precision at recall levels of [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], as well as on mean average precision values. The experiment indicated that neither the Porter stemmer nor the Krovetz stemmer outperformed the other on all datasets tested.},
  doi       = {10.1109/SST.2019.00017},
  issn      = {2157-2194},
  keywords  = {document handling;information retrieval;stemming techniques;software engineering artifacts;tracing datasets;knowledge sharing;tracing community;mean average precision values;tracelab experimental framework;Krovetz stemmer;Measurement;Tools;Software;Dictionaries;Software engineering;Standards;traceability, stemming, empirical research},
}

@InProceedings{ghaisas2019-traceability-for-a-knowledge-driven-SW,
  author    = {S. {Ghaisas}},
  booktitle = {2019 IEEE/ACM 10th International Symposium on Software and Systems Traceability (SST)},
  title     = {Traceability for a Knowledge-Driven Software Engineering},
  year      = {2019},
  month     = {May},
  pages     = {1-1},
  abstract  = {Software Engineering (SE) industry views traceability as a means to obtain a comprehensive view of relevant knowledge. For a successful execution and delivery of large projects, the core Software Engineering process and artifacts need to be synchronized with processes and artifacts associated with the business domain, project management, and compliance.},
  doi       = {10.1109/SST.2019.00008},
  issn      = {2157-2194},
  keywords  = {project management;software engineering;software engineering process;knowledge-driven software engineering;business domain;project management;Software engineering;Project management;Software;Industries;Knowledge engineering;Synchronization;traceability, domain knowledge, compliance, project management},
}

@InProceedings{rahimi2019-Evolving-trace-req2source,
  author    = {M. {Rahimi} and J. {Cleland-Huang}},
  booktitle = {2019 IEEE/ACM 10th International Symposium on Software and Systems Traceability (SST)},
  title     = {Evolving Software Trace Links between Requirements and Source Code},
  year      = {2019},
  month     = {May},
  pages     = {12-12},
  abstract  = {Maintaining trace links in response to continuous changes occurring in software systems is arduous. In this paper, we present a Trace Link Evolver (TLE) to automatically evolve source-to-requirement trace links according to underlying changes in the system. TLE depends on a set of heuristics coupled with refactoring detection tools and information retrieval algorithms to detect predefined change scenarios that occur across contiguous versions of a software system. Our evaluations show that considering both structural and semantic changes leads to more accurate trace link evolution.},
  doi       = {10.1109/SST.2019.00012},
  issn      = {2157-2194},
  keywords  = {information retrieval;software maintenance;source code;software system;TLE;source-to-requirement trace links;information retrieval algorithms;structural changes;semantic changes;evolving software trace links;trace link evolver;refactoring detection tools;trace link evolution;Computer science;Software systems;Hazards;Information retrieval;Tools;Heuristic algorithms;Traceability;Evolution;Maintenance},
}

@Article{rahimi2018-Evolving-trace-req2source,
  author   = {Rahimi, Mona and Cleland-Huang, Jane},
  journal  = {Empirical Software Engineering},
  title    = {Evolving software trace links between requirements and source code},
  year     = {2018},
  issn     = {1573-7616},
  number   = {4},
  pages    = {2198--2231},
  volume   = {23},
  abstract = {Traceability provides support for diverse software engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, in practice, there is a tendency for trace links to degrade over time as the system continually evolves. This is especially true for links between source-code and upstream artifacts such as requirements - because developers frequently refactor and change code without updating the links. In this paper we present TLE (Trace Link Evolver), a solution for automating the evolution of bidirectional trace links between source code classes or methods and requirements. TLE depends on a set of heuristics coupled with refactoring detection tools and informational retrieval algorithms to detect predefined change scenarios that occur across contiguous versions of a software system. We first evaluate TLE at the class level in a controlled experiment to evolve trace links for revisions of two Java applications. Second, we comparatively evaluate several variants of TLE across six releases of our in-house Dronology project. We study the results of integrating human analyst feed back in the evolution cycle of this emerging project. Additionally, in this system, we compare the efficacy of class-level versus method-level evolution of trace links. Finally, we evaluate TLE in a larger scale across 27 releases of the Cassandra Database System and show that the evolved trace links are significantly more accurate than those generated using only information retrieval techniques.},
  refid    = {Rahimi2018},
  url      = {https://doi.org/10.1007/s10664-017-9561-x},
}

@InProceedings{lian2019-Traceability-reveals-quality-in-source,
  author    = {X. {Lian} and A. {Fakhry} and L. {Zhang} and J. {Cleland-Huang}},
  booktitle = {2015 IEEE/ACM 8th International Symposium on Software and Systems Traceability},
  title     = {Leveraging Traceability to Reveal the Tapestry of Quality Concerns in Source Code},
  year      = {2015},
  month     = {May},
  pages     = {50-56},
  abstract  = {Software quality concerns, related to attributes such as reliability, security, and performance, are realized through a series of architectural decisions impacting the choice of frameworks, styles, tactics, and even high-level design patterns. These decisions are often undocumented and, as a result, developers maybe unaware of the relationship between various sections of the code and quality concerns. In this paper we utilize an existing classifier to detect architectural tactics in code, and then present three different visualization techniques for visualizing the impact of quality concerns on code. We demonstrate our approach against the Cassandra database system and show that our visualizations offer potentially useful perspectives on the tapestry of quality concerns woven throughout the code.},
  doi       = {10.1109/SST.2015.15},
  issn      = {2157-2194},
  keywords  = {program diagnostics;program visualisation;software architecture;software quality;software reliability;source code (software);traceability;source code;software quality concerns;reliability;security;architectural decisions;classifier;architectural tactics;visualization techniques;Cassandra database system;Peer-to-peer computing;Visualization;Monitoring;Scalability;Heart beat;Biomedical monitoring;Security;Traceability;visualization;architecture;quality concerns},
}

@InProceedings{shin2015-guidelines-benchmark-auto-traceability,
  author    = {Y. {Shin} and J. H. {Hayes} and J. {Cleland-Huang}},
  booktitle = {2015 IEEE/ACM 8th International Symposium on Software and Systems Traceability},
  title     = {Guidelines for Benchmarking Automated Software Traceability Techniques},
  year      = {2015},
  month     = {May},
  pages     = {61-67},
  abstract  = {To comparatively evaluate automated trace ability solutions, we need to develop standardized benchmarks. However there is currently no consensus on how a benchmark should be constructed and used to evaluate competing techniques. In this paper we discuss recurring problems in evaluating trace ability techniques, identify essential properties that evaluation methods should possess, and provide guidelines for benchmarking software trace ability techniques. We illustrate the properties and guidelines using empirical evaluation of three software trace ability techniques on nine data sets.},
  doi       = {10.1109/SST.2015.13},
  issn      = {2157-2194},
  keywords  = {benchmark testing;program diagnostics;software engineering;automated software traceability technique;benchmarking;evaluation method;Measurement;Accuracy;Benchmark testing;Software;Guidelines;Software engineering;Communities;Traceability;measurement;evaluation metrics;benchmarks},
}

@InProceedings{li2013-trace-matrix-analyzer,
  author    = {W. {Li} and J. H. {Hayes} and F. {Yang} and K. {Imai} and J. {Yannelli} and C. {Carnes} and M. {Doyle}},
  booktitle = {2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)},
  title     = {Trace Matrix Analyzer (TMA)},
  year      = {2013},
  month     = {May},
  pages     = {44-50},
  abstract  = {A Trace Matrix (TM) represents the relationship between software engineering artifacts and is foundational for many software assurance techniques such as criticality analysis. In a large project, a TM might represent the relationships between thousands of elements of dozens of artifacts (for example, between design elements and code elements, between requirements and test cases). In mission- and safety-critical systems, a third party agent may be given the job to assess a TM prepared by the developer. Due to the size and complexity of the task, automated techniques are needed. We have developed a technique for analyzing a TM, called Trace Matrix Analyzer (TMA), so that third party agents can perform their work faster and more effectively. To validate, we applied TMA to two TMs with known problems and golden answersets: MoonLander and MODIS. We also asked an experienced software engineer to manually review the TM. We found that TMA properly identified TM issues and was much faster than manual review, but also falsely identified issues for one dataset. This work addresses the Trusted Grand Challenge, research projects 3, 5, and 6.},
  doi       = {10.1109/TEFSE.2013.6620153},
  groups    = {Analysis, Conceptualisation},
  issn      = {2157-2194},
  keywords  = {formal specification;multi-agent systems;program diagnostics;safety-critical software;trace matrix analyzer;TMA;software engineering artifacts;software assurance techniques;code elements;mission-critical systems;safety-critical systems;third party agent;task complexity;automated techniques;MoonLander;MODIS;trusted grand challenge;research projects;Manuals;MODIS;Vectors;Software engineering;Noise measurement;Radio access networks;Educational institutions;Formal Specification;Temporal Requirements;Translation;Requirement Comprehension;Trusted Grand Challenge;Research Projects 3, 5, and 6},
}

@Article{nejat2012-traceability-sysml-safety-certification,
  author   = {Shiva Nejati and Mehrdad Sabetzadeh and Davide Falessi and Lionel Briand and Thierry Coq},
  journal  = {Information and Software Technology},
  title    = {A SysML-based approach to traceability management and design slicing in support of safety certification: Framework, tool support, and case studies},
  year     = {2012},
  issn     = {0950-5849},
  note     = {Special Section: Engineering Complex Software Systems through Multi-Agent Systems and Simulation},
  number   = {6},
  pages    = {569 - 590},
  volume   = {54},
  abstract = {Context
Traceability is one of the basic tenets of all safety standards and a key prerequisite for software safety certification. In the current state of practice, there is often a significant traceability gap between safety requirements and software design. Poor traceability, in addition to being a non-compliance issue on its own, makes it difficult to determine whether the design fulfills the safety requirements, mainly because the design aspects related to safety cannot be clearly identified.
Objective
The goal of this article is to develop a framework for specifying and automatically extracting design aspects relevant to safety requirements. This goal is realized through the combination of two components: (1) A methodology for establishing traceability between safety requirements and design, and (2) an algorithm that can extract for any given safety requirement a minimized fragment (slice) of the design that is sound, and yet easy to understand and inspect.
Method
We ground our framework on System Modeling Language (SysML). The framework includes a traceability information model, a methodology to establish traceability, and mechanisms for model slicing based on the recorded traceability information. The framework is implemented in a tool, named SafeSlice.
Results
We prove that our slicing algorithm is sound for temporal safety properties, and argue about the completeness of slices based on our practical experience. We report on the lessons learned from applying our approach to two case studies, one benchmark and one industrial case. Both studies indicate that our approach substantially reduces the amount of information that needs to be inspected for ensuring that a given (behavioral) safety requirement is met by the design.},
  doi      = {https://doi.org/10.1016/j.infsof.2012.01.005},
  groups   = {MDE},
  keywords = {Safety certification, SysML, Traceability, Model slicing},
  url      = {http://www.sciencedirect.com/science/article/pii/S095058491200016X},
}

@InProceedings{paz2019-Modelling-Avionics-Certification,
  author    = {A. {Paz} and G. {El Boussaidi}},
  booktitle = {2019 IEEE/ACM 6th International Workshop on Requirements Engineering and Testing (RET)},
  title     = {A Requirements Modelling Language to Facilitate Avionics Software Verification and Certification},
  year      = {2019},
  month     = {May},
  pages     = {1-8},
  abstract  = {Engineering avionics software is a complex task. Even more so due to their safety-critical nature. Aviation authorities require avionics software suppliers to provide appropriate evidence of achieving DO-178C objectives for the verification of outputs from the requirements and design processes, and requirements-based testing. This concern is leading suppliers to consider and incorporate more effective engineering methods that can support them in their verification and certification endeavours. This paper presents SpecML, a modelling language providing a requirements specification infrastructure for avionics software. The goal of SpecML is threefold: 1) enforce certification information mandated by DO-178C, 2) capture requirements in natural language to encourage adoption in industry. and 3) capture requirements in a structured, semantically-rich formalism to enable requirements-based analyses and testing. The modelling language has been developed as a UML profile extending SysML Requirements. A reference implementation has been developed and experiences on its application to an openly-available avionics software specification are reported.},
  doi       = {10.1109/RET.2019.00008},
  keywords  = {aerospace computing;avionics;formal specification;program testing;program verification;safety-critical software;SysML;facilitate avionics software verification;engineering avionics software;safety-critical nature;aviation authorities;avionics software suppliers;requirements-based testing;SpecML;requirements specification infrastructure;certification information;natural language;openly-available avionics software specification;SysML requirements;requirements modelling language;design processes;DO-178C objectives;UML profile;Software;Unified modeling language;Aerospace electronics;Natural languages;Testing;Industries;Certification;requirements modelling language, requirements-based testing, avionics software, DO-178C, certification},
}

@InProceedings{gannous2019-Certification-into-Model-based-Testing-for-Safety-Critical-Systems,
  author    = {A. {Gannous} and A. {Andrews}},
  booktitle = {2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE)},
  title     = {Integrating Safety Certification Into Model-Based Testing of Safety-Critical Systems},
  year      = {2019},
  month     = {Oct},
  pages     = {250-260},
  abstract  = {Testing plays an important role in assuring the safety of safety-critical systems (SCS). Testing SCSs should include tasks to test how the system operates in the presence of failures. With the increase of autonomous, sensing-based functionality in safety-critical systems, efficient and cost-effective testing that maximizes safety evidences has become increasingly challenging. A previously proposed framework for testing safety-critical systems called Model-Combinatorial based testing (MCbt) has the potential for addressing these challenges. MCbt is a framework that proposes an integration of model-based testing, fault analysis, and combinatorial testing to produce the maximum number of evidences for an efficient safety certification process but was never actually used to derive a specific testing approach. In this paper, we present a concrete application of MCbt with an application to a case study. The validation showed that MCbt is more efficient and produces more safety evidences compared to state-of-the-art testing approaches.},
  doi       = {10.1109/ISSRE.2019.00033},
  groups    = {MDE},
  issn      = {2332-6549},
  keywords  = {certification;program diagnostics;program testing;safety-critical software;software fault tolerance;fault analysis;model-combinatorial based testing;cost-effective testing;sensing-based functionality;SCS testing;safety certification;safety evidences;MCbt;safety-critical systems;Model based Testing, Combinatorial Testing, Fault Modelling, Safety Analysis, Fault Tree Analysis, Finite State Machines, EFSM, CEFSM, Testing Safety-critical Systems},
}

@InProceedings{russo2019-MDE-and-Certification,
  author    = {S. {Russo} and F. {Scippacercola}},
  booktitle = {2016 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
  title     = {Model-Based Software Engineering and Certification: Some Open Issues},
  year      = {2016},
  month     = {Oct},
  pages     = {237-240},
  abstract  = {Model-based software engineering methodologies, languages, standards, technologies, and tools are in place since many years. While they proved to be effective in several application sectors, e.g. for embedded systems, empirical studies show that their use in industries pursues a variety of goals and that often they are only partially applied, so it is still not clear to what extent they are actually adopted in the engineering practice and whether they achieve the claimed advantages. Notwithstanding this, model-based techniques are being increasingly advocated for use in critical systems engineering. As critical systems have to undergo certification, the question raised to what extent model-based engineering supports certification. While this is not a new issue, the literature is still at the beginning. We examine key aspects in this respect, and identify some open issues.},
  doi       = {10.1109/ISSREW.2016.24},
  file      = {:russo2016-MDE-and-Certification.pdf:PDF},
  groups    = {MDE},
  keywords  = {certification;embedded systems;software engineering;model-based software engineering methodologies;model-based software certification;embedded systems;critical systems engineering;Unified modeling language;Standards;Safety;Software;Analytical models;Software engineering;Model-Based Software Engineering;Model-Driven Engineering;Model-Driven Architecture;Software Certification},
}

@InProceedings{slotosch2018-Modeling-and-Certification-of-MDD-Processes,
  author    = {Slotosch, Oscar AND Abu-Alqumsan, Mohammad},
  booktitle = {Modellierung 2018},
  title     = {Modeling and Safety-Certification of Model-based Development Processes},
  year      = {2018},
  address   = {Bonn},
  editor    = {Schaefer, Ina AND Karagiannis, Dimitris AND Vogelsang, Andreas AND Méndez, Daniel AND Seidl, Christoph},
  pages     = {261-273},
  publisher = {Gesellschaft für Informatik e.V.},
}

@Article{ardagna2015-dependability-Certification-of-services-MDD-approach,
  author   = {Ardagna, Claudio A. and Jhawar, Ravi and Piuri, Vincenzo},
  journal  = {Computing},
  title    = {Dependability certification of services: a model-based approach},
  year     = {2015},
  issn     = {1436-5057},
  number   = {1},
  pages    = {51--78},
  volume   = {97},
  abstract = {The advances and success of the Service-Oriented Architecture (SOA) paradigm have produced a revolution in ICT, particularly, in the way in which software applications are implemented and distributed. Today, applications are increasingly provisioned and consumed as web services over the Internet, and business processes are implemented by dynamically composing loosely coupled applications provided by different suppliers. In this highly dynamic context, clients (e.g., business owners or users selecting a service) are concerned about the dependability of their services and business processes. In this paper, we define a certification scheme that allows to verify the dependability properties of services and business processes. Our certification scheme relies on discrete-time Markov chains and awards machine-readable dependability certificates to services, whose validity is continuously verified using run-time monitoring. Our solution can be integrated within existing SOAs, to extend the discovery and selection process with dependability requirements and certificates, and to support a dependability-aware service composition.},
  refid    = {Ardagna2015},
  url      = {https://doi.org/10.1007/s00607-013-0348-7},
}

@Conference{azevedo2019-traceability-metamodel-and-reference-model,
  author       = {Bruno Azevedo. and Mario Jino.},
  booktitle    = {Proceedings of the 14th International Conference on Evaluation of Novel Approaches to Software Engineering - Volume 1: ENASE,},
  title        = {Modeling Traceability in Software Development: A Metamodel and a Reference Model for Traceability},
  year         = {2019},
  organization = {INSTICC},
  pages        = {322-329},
  publisher    = {SciTePress},
  abstract     = {Many traceability models lack well-defined traceability link types, provide incomplete coverage of situations, do not provide mechanisms to ensure consistency of traceability, and consider only requirements traceability ignoring other activities of development. We propose a set of basic concepts for traceability, a reference model, and a comprehensive metamodel for traceability created using this reference model. The reference model defines: basic elements for traceability, basic actions to be done on artifacts, basic properties that sets of link types and artifact types should have, basic categories that should be realized regarding these sets, and basic set of processes for traceability. The metamodel is composed of a visual model defining how its elements interact, the definition and semantic description of link types and artifact types which realize the categories of the reference model, and a set of detailed processes describing the steps to maintain traceability and system consistency. Our proposal aims to reduce the problems identified; the reference model provides directions to help the creation, or evaluation, of a traceability model; the metamodel provides semantically described traceability link types, coverage of the most common situations, mechanisms to ensure consistency of traceability, and covers the most common activities in software development.},
  doi          = {10.5220/0007715103220329},
  file         = {:azevedo2019-traceability-metamodel-and-reference-model.pdf:PDF},
  isbn         = {978-989-758-375-9},
  keywords     = {read, prio1, rank4},
  priority     = {prio1},
  ranking      = {rank4},
  readstatus   = {read},
}

@InBook{huhn2010-UML-for-Safety-and-Certification,
  author    = {Huhn, Michaela and Hungar, Hardi},
  editor    = {Giese, Holger and Karsai, Gabor and Lee, Edward and Rumpe, Bernhard and Sch{\"a}tz, Bernhard},
  pages     = {201--237},
  publisher = {Springer Berlin Heidelberg},
  title     = {8 UML for Software Safety and Certification},
  year      = {2010},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-16277-0},
  abstract  = {With the proliferation of UML in the development of embedded real-time systems, the interest in methods and techniques integrating safety aspects into a UML-based software and system development process has increased. This chapter provides a survey on relevant UML profiles and dialects as well as on design and verification methods and process issues supporting a safety assessment. These subjects are discussed in the light of norms and standards on software development for safety-critical systems.},
  booktitle = {Model-Based Engineering of Embedded Real-Time Systems: International Dagstuhl Workshop, Dagstuhl Castle, Germany, November 4-9, 2007. Revised Selected Papers},
  doi       = {10.1007/978-3-642-16277-0_8},
  url       = {https://doi.org/10.1007/978-3-642-16277-0_8},
}

@Article{tran2011-vbTrace-view-based-MDD-processdriven-SOAs-traceability,
  author   = {Tran, Huy and Zdun, Uwe and Dustdar, Schahram},
  journal  = {Software \& Systems Modeling},
  title    = {VbTrace: using view-based and model-driven development to support traceability in process-driven SOAs},
  year     = {2011},
  issn     = {1619-1374},
  number   = {1},
  pages    = {5--29},
  volume   = {10},
  abstract = {In process-driven, service-oriented architectures, there are a number of important factors that hinder the traceability between design and implementation artifacts. First of all, there are no explicit links between process design and implementation languages not only due to the differences of syntax and semantics but also the differences of granularity. The second factor is the complexity caused by tangled process concerns that multiplies the difficulty of analyzing and understanding the trace dependencies. Finally, there is a lack of adequate tool support for establishing and maintaining the trace dependencies between process designs and implementations. We present in this article a view-based, model-driven traceability approach that tackles these challenges. Our approach supports (semi-)automatically eliciting and (semi-)formalizing trace dependencies among process development artifacts at different levels of granularity and abstraction. A proof-of-concept tool support has been realized, and its functionality is illustrated via an industrial case study.},
  groups   = {MDE},
  refid    = {Tran2011},
  url      = {https://doi.org/10.1007/s10270-009-0137-0},
}

@Article{goknil2014-change-impact-analysis-for-requirement-metamodel,
  author   = {Arda Goknil and Ivan Kurtev and Klaas [van den Berg] and Wietze Spijkerman},
  journal  = {Information and Software Technology},
  title    = {Change impact analysis for requirements: A metamodeling approach},
  year     = {2014},
  issn     = {0950-5849},
  number   = {8},
  pages    = {950 - 972},
  volume   = {56},
  abstract = {Context
Following the evolution of the business needs, the requirements of software systems change continuously and new requirements emerge frequently. Requirements documents are often textual artifacts with structure not explicitly given. When a change in a requirements document is introduced, the requirements engineer may have to manually analyze all the requirements for a single change. This may result in neglecting the actual impact of a change. Consequently, the cost of implementing a change may become several times higher than expected.
Objective
In this paper, we aim at improving change impact analysis in requirements by using formal semantics of requirements relations and requirements change types.
Method
In our previous work we present a requirements metamodel with commonly used requirements relation types and their semantics formalized in first-order logic. In this paper the classification of requirements changes based on structure of a textual requirement is provided with formal semantics. The formalization of requirements relations and changes is used for propagating proposed changes and consistency checking of proposed changes in requirements models. The tool support for change impact analysis in requirements models is an extension of our Tool for Requirements Inferencing and Consistency Checking (TRIC).
Results
The described approach for change impact analysis helps in the elimination of some false positive impacts in change propagation, and enables consistency checking of changes.
Conclusion
We illustrate our approach in an example which shows that the formal semantics of requirements relations and change classification enables change alternatives to be proposed semi-automatically, the reduction of some false positive impacts and contradicting changes in requirements to be determined.},
  doi      = {https://doi.org/10.1016/j.infsof.2014.03.002},
  groups   = {Analysis, Conceptualisation},
  keywords = {Requirements metamodel, Change impact analysis, Proposing and propagating changes},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914000615},
}

@InProceedings{wolfl-qualified-avionics-experience-report,
  author    = {A. {Wölfl} and N. {Siegmund} and S. {Apel} and H. {Kosch} and J. {Krautlager} and G. {Weber-Urbina}},
  booktitle = {2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {Generating Qualifiable Avionics Software: An Experience Report (E)},
  year      = {2015},
  month     = {Nov},
  pages     = {726-736},
  abstract  = {We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.},
  doi       = {10.1109/ASE.2015.35},
  keywords  = {avionics;helicopters;program compilers;software engineering;qualifiable avionics software;data-management component;NH90 helicopter;Airbus helicopter;legally-binding certification process;model-based technology;product-line technology;software repository analysis;Aerospace electronics;Helicopters;System software;Interviews;Hardware;Encoding},
}

@InProceedings{fittkau2013-explorviz-Trace-Visualization,
  author    = {F. {Fittkau} and J. {Waller} and C. {Wulf} and W. {Hasselbring}},
  booktitle = {2013 First IEEE Working Conference on Software Visualization (VISSOFT)},
  title     = {Live trace visualization for comprehending large software landscapes: The ExplorViz approach},
  year      = {2013},
  month     = {Sep.},
  pages     = {1-4},
  abstract  = {The increasing code complexity in modern enterprise software systems exceeds the capabilities of most software engineers to understand the system's behavior by just looking at its program code. Large software landscapes, e.g., applications in a cloud infrastructure, further increase this complexity. A solution to these problems is visualizing the applications of the software landscape to ease program comprehension and to understand the respective communication. An established visualization concept is the 3D city metaphor. It utilizes the familiarity with navigating a city to improve program comprehension. Dynamic analysis, e.g., monitoring, can provide the required program traces of the communication. In this paper, we present our live visualization approach of monitoring traces for large software landscapes. It combines a landscape and a system level perspective. The landscape level perspective provides an overview of the software landscape utilizing the viewer's familiarity with UML. The system level perspective provides a visualization utilizing the city metaphor for each software system.},
  doi       = {10.1109/VISSOFT.2013.6650536},
  groups    = {Analysis},
  keywords  = {cloud computing;data visualisation;software maintenance;Unified Modeling Language;software system;UML;landscape level perspective;program traces;dynamic analysis;program comprehension;3D city metaphor;cloud infrastructure;ExplorViz approach;software landscapes;live trace visualization;Visualization;Three-dimensional displays;Cities and towns;Unified modeling language;Software systems;Monitoring},
}

@InProceedings{denney2006-source-code-evidence-certification,
  author    = {E. {Denney} and B. {Fischer}},
  booktitle = {Second International Symposium on Leveraging Applications of Formal Methods, Verification and Validation (isola 2006)},
  title     = {Extending Source Code Generators for Evidence-Based Software Certification},
  year      = {2006},
  month     = {Nov},
  pages     = {138-145},
  abstract  = {Automated code generation offers many advantages over manual software development but treating generators as trusted black boxes raise problems for certification. Traditional process-oriented approaches to certification thus require that the generator be verified to the same level of assurance as the generated code, but this is infeasible for realistic generators. However, generators can be extended to support an evidence-based approach to certification. By careful design of the trusted kernel, assurance of the generator itself is not required. In this paper, we describe several related extensions to two in-house code generators to provide two forms of evidence along with the code: safety proofs and safety explanations. We also describe how additionally provided links are used to trace between the code and the safety artifacts.},
  doi       = {10.1109/ISoLA.2006.76},
  keywords  = {certification;program compilers;program diagnostics;program verification;theorem proving;source code generators;evidence-based software certification;trusted black boxes;process-oriented approaches;generator verification;evidence-based approach;trusted kernel;safety proofs;safety explanations;code tracing;safety artifacts;theorem provers;Certification;Software safety;Qualifications;Error correction;Application software;NASA;Kernel;User interfaces;Programming profession;Productivity;automated code generation;safety;certi?cation;quali?cation;evidence-based;user interfaces;theorem provers;traceability},
}

@Article{borg2014-SmS-IR-for-traceability,
  author   = {Borg, Markus and Runeson, Per and Ardö, Anders},
  journal  = {Empirical Software Engineering},
  title    = {Recovering from a decade: a systematic mapping of information retrieval approaches to software traceability},
  year     = {2014},
  issn     = {1573-7616},
  number   = {6},
  pages    = {1565--1616},
  volume   = {19},
  abstract = {Engineers in large-scale software development have to manage large amounts of information, spread across many artifacts. Several researchers have proposed expressing retrieval of trace links among artifacts, i.e. trace recovery, as an Information Retrieval (IR) problem. The objective of this study is to produce a map of work on IR-based trace recovery, with a particular focus on previous evaluations and strength of evidence. We conducted a systematic mapping of IR-based trace recovery. Of the 79 publications classified, a majority applied algebraic IR models. While a set of studies on students indicate that IR-based trace recovery tools support certain work tasks, most previous studies do not go beyond reporting precision and recall of candidate trace links from evaluations using datasets containing less than 500 artifacts. Our review identified a need of industrial case studies. Furthermore, we conclude that the overall quality of reporting should be improved regarding both context and tool details, measures reported, and use of IR terminology. Finally, based on our empirical findings, we present suggestions on how to advance research on IR-based  trace recovery.},
  file     = {:borg2014-SmS-IR-for-traceability.pdf:PDF},
  groups   = {meta},
  refid    = {Borg2014},
  url      = {https://doi.org/10.1007/s10664-013-9255-y},
}

@InProceedings{post2009-link-functional-req-to-verification,
  author    = {H. {Post} and C. {Sinz} and F. {Merz} and T. {Gorges} and T. {Kropf}},
  booktitle = {2009 17th IEEE International Requirements Engineering Conference},
  title     = {Linking Functional Requirements and Software Verification},
  year      = {2009},
  month     = {Aug},
  pages     = {295-302},
  abstract  = {Synchronization between component requirements and implementation centric tests remains a challenge that is usually addressed by requirements reviews with testers and traceability policies. The claim of this work is that linking requirements, their scenario-based formalizations, and software verification provides a promising extension to this approach. Formalized scenarios, for example in the form of low-level assume/assert statements in C, are easier to trace to requirements than traditional test sets. For a verification engineer, they offer an opportunity to better participate in requirements changes. Changes in requirements can be more easily propagated because adapting formalized scenarios is often easier than deriving and updating a large set of test cases. The proposed idea is evaluated in a case study encompassing over 50 functional requirements of an automotive software developed at Robert Bosch GmbH. Results indicate that requirement formalization together with formal verification leads to the discovery of implementation problems missed in a traditional testing process.},
  doi       = {10.1109/RE.2009.43},
  issn      = {2332-6441},
  keywords  = {formal specification;object-oriented programming;program diagnostics;program testing;program verification;software maintenance;systems analysis;functional requirement review;software verification;component requirement change;software testing;software traceability policy;scenario-based formalization;C statement;Joining processes;Software testing;System testing;Automotive engineering;Programming;Computer industry;Safety;Computer science;Control systems;Formal verification;functional requirements;verification;bounded model checking},
}

@InProceedings{sabaliauskaite2010-req2verif-industrial-context,
  author    = {Sabaliauskaite, Giedre and Loconsole, Annabella and Engstr{\"o}m, Emelie and Unterkalmsteiner, Michael and Regnell, Bj{\"o}rn and Runeson, Per and Gorschek, Tony and Feldt, Robert},
  booktitle = {Requirements Engineering: Foundation for Software Quality},
  title     = {Challenges in Aligning Requirements Engineering and Verification in a Large-Scale Industrial Context},
  year      = {2010},
  address   = {Berlin, Heidelberg},
  editor    = {Wieringa, Roel and Persson, Anne},
  pages     = {128--142},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {[Context and motivation] When developing software, coordination between different organizational units is essential in order to develop a good quality product, on time and within budget. Particularly, the synchronization between requirements and verification processes is crucial in order to assure that the developed software product satisfies customer requirements. [Question/problem] Our research question is: what are the current challenges in aligning the requirements and verification processes? [Principal ideas/results] We conducted an interview study at a large software development company. This paper presents preliminary findings of these interviews that identify key challenges in aligning requirements and verification processes. [Contribution] The result of this study includes a range of challenges faced by the studied organization grouped into the categories: organization and processes, people, tools, requirements process, testing process, change management, traceability, and measurement. The findings of this study can be used by practitioners as a basis for investigating alignment in their organizations, and by scientists in developing approaches for more efficient and effective management of the alignment between requirements and verification.},
  isbn      = {978-3-642-14192-8},
}

@Article{tai2015-tree-LSTM-Semantic-representation,
  author        = {Kai Sheng Tai and Richard Socher and Christopher D. Manning},
  journal       = {CoRR},
  title         = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
  year          = {2015},
  volume        = {abs/1503.00075},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/TaiSM15.bib},
  eprint        = {1503.00075},
  timestamp     = {Mon, 13 Aug 2018 16:48:20 +0200},
  url           = {http://arxiv.org/abs/1503.00075},
}

@InProceedings{guo2017-semantically-enhanced-tracebility-deep-learning,
  author    = {Guo, Jin and Cheng, Jinghui and Cleland-Huang, Jane},
  booktitle = {Proceedings of the 39th International Conference on Software Engineering},
  title     = {Semantically Enhanced Software Traceability Using Deep Learning Techniques},
  year      = {2017},
  pages     = {3–14},
  publisher = {IEEE Press},
  series    = {ICSE ’17},
  abstract  = {In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts, however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links, however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.},
  doi       = {10.1109/ICSE.2017.9},
  groups    = {Conceptualisation},
  isbn      = {9781538638682},
  keywords  = {traceability, recurrent neural network, semantic representation, deep learning},
  location  = {Buenos Aires, Argentina},
  numpages  = {12},
  url       = {https://doi.org/10.1109/ICSE.2017.9},
}

@Article{bjarnasson20016-theory-of-distances-in-SE,
  author     = {Bjarnason, Elizabeth and Smolander, Kari and Engstr\"{o}m, Emelie and Runeson, Per},
  journal    = {Inf. Softw. Technol.},
  title      = {A Theory of Distances in Software Engineering},
  year       = {2016},
  issn       = {0950-5849},
  month      = feb,
  number     = {C},
  pages      = {204–219},
  volume     = {70},
  address    = {USA},
  doi        = {10.1016/j.infsof.2015.05.004},
  issue_date = {February 2016},
  keywords   = {Verification, Distances, Requirements engineering, Human factors, Theory, Empirical software engineering},
  numpages   = {16},
  publisher  = {Butterworth-Heinemann},
  url        = {https://doi.org/10.1016/j.infsof.2015.05.004},
}

@Article{rutle2018-MT-coevolution-with-traceability-and-graph-transfo,
  author    = {A Rutle and L Iovino and H König and Z Diskin},
  journal   = {Modelling Foundations and Applications},
  title     = {Automatic Transformation Co-evolution Using Traceability Models and Graph Transformation},
  year      = {2018},
  volume    = {10890},
  address   = {Cham},
  editor    = {Pierantonio A., Trujillo S.},
  groups    = {Conceptualisation},
  publisher = {Springer},
}

@Article{amar2013-model-coevolution-uding-traceability,
  author    = {B Amar and H Leblanc and B Coulette and P Dhaussy},
  journal   = {Communications in Computer and Information Science},
  title     = {Automatic Co-evolution of Models Using Traceability},
  year      = {2013},
  volume    = {170},
  abstract  = {Model Driven Engineering allows models to be considered as data and then used as first class entities in dedicated transformations languages. As a result, recurring problems linked to software production are emerging in this new development context. One such problem is to maintain an inter-model consistency during execution of a process based on models and model transformations. When some related models must co-evolve, what appends when different transformations are applied separately on each of these models? To prevent this, we assume that one of these models is the master model and we propose an automatic co-evolution of the other models based on the traceability of the main transformation. So the contribution of this paper is a conceptual framework where the necessary transformations of repercussion can be easily deployed.},
  address   = {Berlin, Heidelberg},
  editor    = {Cordeiro J., Virvou M., Shishkov B.},
  groups    = {MDE},
  publisher = {Springer},
}

@Article{marca2013-ebusiness-and-social-network-language-action,
  author    = {D Marca},
  journal   = {Communications in Computer and Information Science},
  title     = {E-Business and Social Networks: Tapping Dynamic Niche Markets Using Language-Action and Artificial Intelligence},
  year      = {2013},
  volume    = {170},
  address   = {Berlin, Heidelberg},
  editor    = {Cordeiro J., Virvou M., Shishkov B.},
  publisher = {Springer},
}

@InProceedings{wenzel2007-Tracing-model-elements,
  author    = {S. {Wenzel} and H. {Hutter} and U. {Kelter}},
  booktitle = {2007 IEEE International Conference on Software Maintenance},
  title     = {Tracing Model Elements},
  year      = {2007},
  month     = {Oct},
  pages     = {104-113},
  abstract  = {In model-driven engineering developers work mainly or only with models, which exist in many versions. This paper presents an approach to trace single model elements or groups of elements within a version history of a model. It also offers analysis capabilities such as detection of logical coupling between model elements. The approach uses a differencing algorithm blown as SiDiff to identify similar elements in different versions of a model. SiDiff is highly configurable and thus our tracing approach can be adapted to all diagram types of the UML and to a large set of domain specific languages. The approach has been implemented as an Eclipse plug-in that visualizes all relevant information about the traces and it allows developers to interactively explore details. It has been evaluated by several groups of test persons; they considered most of the functions of the tool to be very useful.},
  doi       = {10.1109/ICSM.2007.4362623},
  issn      = {1063-6773},
  keywords  = {program diagnostics;software engineering;Unified Modeling Language;model-driven engineering developer;model element tracing;SiDiff;UML;domain specific language;differencing algorithm;tracing approach;Eclipse plug-in;information visualization;Object oriented modeling;Mathematical model;Power system modeling;Unified modeling language;Model driven engineering;Software engineering;History;Domain specific languages;Collaborative software;Project management},
}

@InProceedings{santiago2013traceability-in-MDE,
  author    = {Santiago, Iv{\'a}n and Vara, Juan Manuel and de Castro, Mar{\'i}a Valeria and Marcos, Esperanza},
  booktitle = {Conceptual Modeling},
  title     = {Towards the Effective Use of Traceability in Model-Driven Engineering Projects},
  year      = {2013},
  address   = {Berlin, Heidelberg},
  editor    = {Ng, Wilfred and Storey, Veda C. and Trujillo, Juan C.},
  pages     = {429--437},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The key role of models in any Model-Driven Engineering (MDE) process provides a new landscape for dealing with traceability. In the context of MDE traces are merely links between the elements of the different models handled along the software development cycle. Traces can be consequently stored in models that can be processed by means of model-based techniques. To take advantage of this scenario, this paper introduces iTrace, a framework for the management and analysis of traceability information in MDE projects. We present the methodological proposal bundled in the framework as well as the tooling that supports it. Finally, a case study is used to introduce some of the functionalities offered by the framework.},
  groups    = {MDE, Analysis, Conceptualisation},
  isbn      = {978-3-642-41924-9},
}

@InProceedings{aranega2011-trace-for-mutation-analysis-in-model-transformation,
  author    = {Aranega, Vincent and Mottu, Jean-Marie and Etien, Anne and Dekeyser, Jean-Luc},
  booktitle = {Models in Software Engineering},
  title     = {Traceability for Mutation Analysis in Model Transformation},
  year      = {2011},
  address   = {Berlin, Heidelberg},
  editor    = {Dingel, Juergen and Solberg, Arnor},
  pages     = {259--273},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Model transformation can't be directly tested using program techniques. Those have to be adapted to model characteristics. In this paper we focus on one test technique: mutation analysis. This technique aims to qualify a test data set by analyzing the execution results of intentionally faulty program versions. If the degree of qualification is not satisfactory, the test data set has to be improved. In the context of model, this step is currently relatively fastidious and manually performed.},
  groups    = {MDE},
  isbn      = {978-3-642-21210-9},
}

@Article{seibel2010-dynamic-hierarchical-models-comprehensive-traceability,
  author   = {Seibel, Andreas and Neumann, Stefan and Giese, Holger},
  journal  = {Software \& Systems Modeling},
  title    = {Dynamic hierarchical mega models: comprehensive traceability and its efficient maintenance},
  year     = {2010},
  issn     = {1619-1374},
  number   = {4},
  pages    = {493--528},
  volume   = {9},
  abstract = {In the world of model-driven engineering (MDE) support for traceability and maintenance of traceability information is essential. On the one hand, classical traceability approaches for MDE address this need by supporting automated creation of traceability information on the model element level. On the other hand, global model management approaches manually capture traceability information on the model level. However, there is currently no approach that supports comprehensive traceability, comprising traceability information on both levels, and efficient maintenance of traceability information, which requires a high-degree of automation and scalability. In this article, we present a comprehensive traceability approach that combines classical traceability approaches for MDE and global model management in form of dynamic hierarchical mega models. We further integrate efficient maintenance of traceability information based on top of dynamic hierarchical mega models. The proposed approach is further outlined by using an industrial case study and by presenting an implementation of the concepts in form of a prototype.},
  refid    = {Seibel2010},
  url      = {https://doi.org/10.1007/s10270-009-0146-z},
}

@Article{schwarz2010-graph-based-traceability,
  author   = {Schwarz, Hannes and Ebert, Jürgen and Winter, Andreas},
  journal  = {Software \& Systems Modeling},
  title    = {Graph-based traceability: a comprehensive approach},
  year     = {2010},
  issn     = {1619-1374},
  number   = {4},
  pages    = {473--492},
  volume   = {9},
  abstract = {In recent years, traceability has been globally accepted as being a key success factor of software development projects. However, the multitude of different, poorly integrated taxonomies, approaches and technologies impedes the application of traceability techniques in practice. This paper presents a comprehensive view on traceability, pertaining to the whole software development process. Based on the state of the art, the field is structured according to six specific activities related to traceability as follows: definition, recording, identification, maintenance, retrieval, and utilization. Using graph technology, a comprehensive and seamless approach for supporting these activities is derived, combining them in one single conceptual framework. This approach supports the definition of metamodels for traceability information, recording of traceability information in graph-based repositories, identification and maintenance of traceability relationships using transformations, as well as retrieval and utilization of traceability information using a graph query language. The approach presented here is applied in the context of the ReDSeeDS project (Requirements Driven Software Development System) that aims at requirements-based software reuse. ReDSeeDS makes use of traceability information to determine potentially reusable architectures, design, or code artifacts based on a given set of reusable requirements. The project provides case studies from different domains for the validation of the approach.},
  file     = {:schwarz2010-graph-based-traceability.pdf:PDF},
  refid    = {Schwarz2010},
  url      = {https://doi.org/10.1007/s10270-009-0141-4},
}

@InProceedings{oldevik2007-improving-traceability-in-MDE-business-application,
  author = {Jon Oldevik and G{\o}ran K. Olsen and Tor Neple},
  title  = {Improving Traceability in Model-Driven Development of Business Applications},
  year   = {2007},
}

@InBook{rothenberg1989-the-nature-of-modeling,
  author    = {Rothenberg, J.},
  pages     = {75–92},
  publisher = {John Wiley \& Sons, Inc.},
  title     = {The Nature of Modeling},
  year      = {1989},
  address   = {USA},
  isbn      = {0471605999},
  booktitle = {Artificial Intelligence, Simulation & Modeling},
  numpages  = {18},
}

@Article{winkler2010-survey-traceability-and-MDE,
  author   = {Winkler, Stefan and von Pilgrim, Jens},
  journal  = {Software and Systems Modeling},
  title    = {A survey of traceability in requirements engineering and model-driven development},
  year     = {2010},
  issn     = {1619-1374},
  number   = {4},
  pages    = {529--565},
  volume   = {9},
  abstract = {Traceability--the ability to follow the life of software artifacts--is a topic of great interest to software developers in general, and to requirements engineers and model-driven developers in particular. This article aims to bring those stakeholders together by providing an overview of the current state of traceability research and practice in both areas. As part of an extensive literature survey, we identify commonalities and differences in these areas and uncover several unresolved challenges which affect both domains. A good common foundation for further advances regarding these challenges appears to be a combination of the formal basis and the automated recording opportunities of MDD on the one hand, and the more holistic view of traceability in the requirements engineering domain on the other hand.},
  groups   = {meta},
  refid    = {Winkler2010},
  url      = {https://doi.org/10.1007/s10270-009-0145-0},
}

@InProceedings{jarke1992-information-systems-quality,
  author    = {Matthias Jarke and Klaus Pohl},
  booktitle = {The Impact of Computer Supported Technologies in Information Systems Development, Proceedings of the {IFIP} {WG8.2} Working Conference on The Impact of Computer Supported Technologies in Information Systems Development, Minneapolis, Minnesota, USA, 14-17 June, 1992},
  title     = {Information Systems Quality and Quality Informations Systems},
  year      = {1992},
  editor    = {Kenneth E. Kendall and Kalle Lyytinen and Janice I. DeGross},
  pages     = {345--375},
  publisher = {North-Holland},
  series    = {{IFIP} Transactions},
  volume    = {{A-8}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/ifip8-2/JarkeP92.bib},
  timestamp = {Wed, 16 Oct 2002 13:43:31 +0200},
}

@InProceedings{paige2010-MDE-Traceability-classifications,
	
  author   = {Paige, Richard and Olsen, Gøran and Kolovos, Dimitrios and Zschaler, Steffen and Power, Christopher},
  title    = {Building Model-Driven Engineering Traceability Classifications},
  booktitle= {Computer Science},
  year     = {2010},
  month    = {01},
  abstract = {Model-Driven Engineering involves the application of many different model management operations, some automated, some manual. For developers to stay in control of their models and codebase, trace information must be maintained by all model management operations. This leads to a large number of trace links, which themselves need to be managed, queried, and evaluated. Classifications of traceability and trace links are an essential capability required for understanding and managing trace links. We present a process for building traceability classifications for a variety of widely used and accepted operations (both automated and manual) and show the results of applying the process to a rich traceability context.},
  file     = {:paige2010-MDE-Traceability-classifications.pdf:PDF}
}

@InProceedings{helming2009-traceability-change-awareness,
  year = {2009},
  publisher = {Springer Berlin Heidelberg},
  pages = {372--376},
  author = {Jonas Helming and Maximilian Koegel and Helmut Naughton and Joern David and Aleksandar Shterev},
  title = {Traceability-Based Change Awareness},
  booktitle = {Model Driven Engineering Languages and Systems},
  month    = {10},
  volume   = {5795},
  abstract = {Many tools in software engineering projects support the visualization and collaborative modification of custom sets of artifacts. This includes tools for requirements engineering, UML tools for design, project management tools, developer tools and many more. A key factor for success in software engineering projects is the collective understanding of changes applied to these artifacts. To support this, there are several strategies to automatically notify project participants about relevant changes. Known strategies are limited to a fixed set of artifacts and/or make no use of traceability information to supply change notifications. This paper proposes a change notification approach based on traceability in a unified model and building upon operation-based change tracking. The unified model explicitly combines system specification models and project management models into one fully traceable model. To show the benefit of our approach we compare it to related approaches in a case study.},
  doi      = {10.1007/978-3-642-04425-0_28},
  groups   = {MDE},
}

@InProceedings{grammel2010-facet-based-traceability-data-extraction-in-MDE,
  author    = {Grammel, Birgit and Kastenholz, Stefan},
  booktitle = {Proceedings of the 6th ECMFA Traceability Workshop},
  title     = {A Generic Traceability Framework for Facet-Based Traceability Data Extraction in Model-Driven Software Development},
  year      = {2010},
  address   = {New York, NY, USA},
  pages     = {7–14},
  publisher = {Association for Computing Machinery},
  series    = {ECMFA-TW ’10},
  abstract  = {Traceability of artefacts induces the means of understanding the complexity of logical relations existing among artefacts, that are created during software development. In turn, this provides the necessary knowledge for reasoning about the quality of software. With the inception of Model-Driven Software Engineering, the advantage of generating traceability information automatically, eases the problem of creating and maintaining trace links, which is a labor intensive task, when done manually. Yet, there is still a wide range of open challenges in existing traceability solutions and a need to consolidate traceability domain knowledge. This paper proposes a generic traceability framework for augmenting arbitrary model transformation approaches with a traceability mechanism. Essentially, this augmentation is based on a domain-specific language for traceability, accounting for facet-based data extraction.},
  doi       = {10.1145/1814392.1814394},
  groups    = {MDE},
  isbn      = {9781605589930},
  location  = {Paris, France},
  numpages  = {8},
  url       = {https://doi.org/10.1145/1814392.1814394},
}


@Article{lara2007-Triple-graph-patterns,
  author    = {Juan de Lara and Esther Guerra and Paolo Bottoni},
  journal   = {{ECEASST}},
  title     = {Triple Patterns: Compact Specifications for the Generation of Operational Triple Graph Grammar Rules},
  year      = {2007},
  volume    = {6},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/eceasst/LaraGB07.bib},
  doi       = {10.14279/tuj.eceasst.6.54},
  timestamp = {Wed, 22 May 2019 16:42:52 +0200},
  url       = {https://doi.org/10.14279/tuj.eceasst.6.54},
}

@Article{santiago2012-MDE-as-a-new-landscape-for-traceability-SLR,
  author   = {Iván Santiago and Álvaro Jiménez and Juan Manuel Vara and Valeria [De Castro] and Verónica A. Bollati and Esperanza Marcos},
  journal  = {Information and Software Technology},
  title    = {Model-Driven Engineering as a new landscape for traceability management: A systematic literature review},
  year     = {2012},
  issn     = {0950-5849},
  note     = {Special Section on Software Reliability and Security},
  number   = {12},
  pages    = {1340 - 1356},
  volume   = {54},
  abstract = {Context
Model-Driven Engineering provides a new landscape for dealing with traceability in software development.
Objective
Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering.
Method
We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments.
Results
Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals.
Conclusion
The evaluation shows that the most addressed operations are storage, CRUD and visualization, while the most immature operations are exchange and analysis traceability information.},
  doi      = {https://doi.org/10.1016/j.infsof.2012.07.008},
  groups   = {MDE, Featuring, meta},
  keywords = {Traceability, Model-Driven Engineering, Systematic literature review},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001346},
}

@Article{vara2014-traceability-in-MDD-MTransfo,
  author    = {Juan Manuel Vara and Ver{\'{o}}nica Andrea Bollati and {\'{A}}lvaro Jim{\'{e}}nez and Esperanza Marcos},
  journal   = {{IEEE} Trans. Software Eng.},
  title     = {Dealing with Traceability in the MDDof Model Transformations},
  year      = {2014},
  number    = {6},
  pages     = {555--583},
  volume    = {40},
  abstract  = {Traceability has always been acknowledged as a relevant topic in Software Engineering. However, keeping track of the relationships between the different assets involved in a development process is a complex and tedious task. The fact that the main assets handled in any model-driven engineering project are models and model transformations eases the task. In order to take advantage of this scenario, which has not been appropriately capitalized on by the most widely adopted model transformation languages before, this work presents MeTAGeM-Trace, a methodological and technical proposal with which to support the model-driven development of model transformations that include trace generation. The underlying idea is to start from a high-level specification of the transformation which is subsequently refined into lower-level transformation models in terms of a set of DSLs until the source code that implements the transformation can be generated. Running this transformation produces not only the corresponding target models, but also a trace model between the elements of the source and target models. As part of the proposal, an EMF-based toolkit has been developed to support the development of ATL and ETL model transformations. This toolkit has been empirically validated by conducting a set of case studies following a systematic research methodology.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/tse/VaraBJM14.bib},
  doi       = {10.1109/TSE.2014.2316132},
  file      = {:mde/vara2014-traceability-in-MDD-MTransfo.pdf:PDF},
  groups    = {Conceptualisation},
  timestamp = {Thu, 15 Jun 2017 21:30:50 +0200},
  url       = {https://doi.org/10.1109/TSE.2014.2316132},
}

@Article{marcen2020-req2model-with-EA-ranking-train-system,
  author    = {Ana Cristina Marc{\'{e}}n and Ra{\'{u}}l Lape{\~{n}}a and Oscar Pastor and Carlos Cetina},
  journal   = {J. Syst. Softw.},
  title     = {Traceability Link Recovery between Requirements and Models using an Evolutionary Algorithm Guided by a Learning to Rank Algorithm: Train control and management case},
  year      = {2020},
  pages     = {110519},
  volume    = {163},
  abstract  = {Traceability Link Recovery (TLR) has been a topic of interest for many years within the software engineering community. In recent years, TLR has been attracting more attention, becoming the subject of both fundamental and applied research. However, there still exists a large gap between the actual needs of industry on one hand and the solutions published through academic research on the other.

In this work, we propose a novel approach, named Evolutionary Learning to Rank for Traceability Link Recovery (TLR-ELtoR). TLR-ELtoR recovers traceability links between a requirement and a model through the combination of evolutionary computation and machine learning techniques, generating as a result a ranking of model fragments that can realize the requirement.

TLR-ELtoR was evaluated in a real-world case study in the railway domain, comparing its outcomes with five TLR approaches (Information Retrieval, Linguistic Rule-based, Feedforward Neural Network, Recurrent Neural Network, and Learning to Rank). The results show that TLR-ELtoR achieved the best results for most performance indicators, providing a mean precision value of 59.91%, a recall value of 78.95%, a combined F-measure of 62.50%, and a MCC value of 0.64. The statistical analysis of the results assesses the magnitude of the improvement, and the discussion presents why TLR-ELtoR achieves better results than the baselines.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/jss/MarcenLPC20.bib},
  doi       = {10.1016/j.jss.2020.110519},
  groups    = {MDE},
  timestamp = {Thu, 19 Mar 2020 10:23:20 +0100},
  url       = {https://doi.org/10.1016/j.jss.2020.110519},
}

@Book{ruiz18-traceME-conceptual-model-evolution,
  author    = {Marcela Ruiz},
  publisher = {Springer},
  title     = {TraceME: {A} Traceability-Based Method for Conceptual Model Evolution - Model-Driven Techniques, Tools, Guidelines, and Open Challenges in Conceptual Model Evolution},
  year      = {2018},
  isbn      = {978-3-319-89715-8},
  series    = {Lecture Notes in Business Information Processing},
  volume    = {312},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/series/lnbip/Ruiz18.bib},
  doi       = {10.1007/978-3-319-89716-5},
  timestamp = {Wed, 16 May 2018 14:25:04 +0200},
  url       = {https://doi.org/10.1007/978-3-319-89716-5},
}

@InProceedings{aboussoror2012-Seeing-errors-trace-visualisation,
  author    = {El Arbi Aboussoror and Ileana Ober and Iulian Ober},
  booktitle = {Model Driven Engineering Languages and Systems - 15th International Conference, {MODELS} 2012, Innsbruck, Austria, September 30-October 5, 2012. Proceedings},
  title     = {Seeing Errors: Model Driven Simulation Trace Visualization},
  year      = {2012},
  editor    = {Robert B. France and J{\"{u}}rgen Kazmeier and Ruth Breu and Colin Atkinson},
  pages     = {480--496},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {7590},
  abstract  = {Powerful theoretical frameworks exist for model validation and verification, yet their use in concrete projects is limited. This is partially due to the fact that the results of model verification and simulation are difficult to exploit. This paper reports on a model driven approach that supports the user during the error diagnosis phases, by allowing customizable simulation trace visualization. Our thesis is that we can use models to significantly improve the information visualization during the diagnosis phase. This thesis is supported by Metaviz - a model-driven framework for simulation trace visualization. Metaviz uses the IFx-OMEGA model validation platform and a state-of-the-art information visualization reference model together with a well-defined development process guiding the user into building custom visualizations,essentially by defining model transformations. This approach has the potential to improve the practical usage of modeling techniques and to increase the usability and attractiveness of model validation tools.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/models/AboussororOO12.bib},
  doi       = {10.1007/978-3-642-33666-9\_31},
  file      = {:aboussoror2012-Seeing-errors-trace-visualisation.pdf:PDF},
  timestamp = {Tue, 14 May 2019 10:00:45 +0200},
  url       = {https://doi.org/10.1007/978-3-642-33666-9\_31},
}

@InProceedings{grammel2012-model-matching-for-traceability-in-MDE,
  author    = {Birgit Grammel and Stefan Kastenholz and Konrad Voigt},
  booktitle = {Model Driven Engineering Languages and Systems - 15th International Conference, {MODELS} 2012, Innsbruck, Austria, September 30-October 5, 2012. Proceedings},
  title     = {Model Matching for Trace Link Generation in Model-Driven Software Development},
  year      = {2012},
  editor    = {Robert B. France and J{\"{u}}rgen Kazmeier and Ruth Breu and Colin Atkinson},
  pages     = {609--625},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {7590},
  abstract  = {With the advent of Model-driven Software Engineering, the advantage of generating trace links between source and target model elements automatically, eases the problem of creating and maintaining traceability data. Yet, an existing transformation engine as in the above case is not always given in model-based development, (i.e. when transformations are implemented manually) and can not be leveraged for the sake of trace link generation through the transformation mapping. We tackle this problem by using model matching techniques to generate trace links for arbitrary source and target models. Thereby, our approach is based on a novel, language-agnostic concept defining three similarity measures for matching. To achieve this, we exploit metamodel matching techniques for graph-based model matching. Furthermore, we evaluate our approach according to large-scale SAP business transformations and the ATL Zoo.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/models/GrammelKV12.bib},
  doi       = {10.1007/978-3-642-33666-9\_39},
  groups    = {MDE},
  timestamp = {Tue, 14 May 2019 10:00:45 +0200},
  url       = {https://doi.org/10.1007/978-3-642-33666-9\_39},
}

@InProceedings{mader2010-visual-tracability-modeling-language,
  author    = {Patrick M{\"{a}}der and Jane Cleland{-}Huang},
  booktitle = {Model Driven Engineering Languages and Systems - 13th International Conference, {MODELS} 2010, Oslo, Norway, October 3-8, 2010, Proceedings, Part {I}},
  title     = {A Visual Traceability Modeling Language},
  year      = {2010},
  editor    = {Dorina C. Petriu and Nicolas Rouquette and {\O}ystein Haugen},
  pages     = {226--240},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {6394},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/models/MaderC10.bib},
  doi       = {10.1007/978-3-642-16145-2\_16},
  timestamp = {Tue, 14 May 2019 10:00:45 +0200},
  url       = {https://doi.org/10.1007/978-3-642-16145-2\_16},
}

@InProceedings{franch2020-data-driven-RE-vision-paper,
  author    = {Xavier Franch and Norbert Seyff and Marc Oriol and Samuel Fricker and Iris Groher and Michael Vierhauser and Manuel Wimmer},
  booktitle = {Requirements Engineering: Foundation for Software Quality - 26th International Working Conference, {REFSQ} 2020, Pisa, Italy, March 24-27, 2020, Proceedings {[REFSQ} 2020 was postponed]},
  title     = {Towards Integrating Data-Driven Requirements Engineering into the Software Development Process: {A} Vision Paper},
  year      = {2020},
  editor    = {Nazim Madhavji and Liliana Pasquale},
  pages     = {135--142},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {12045},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/refsq/FranchSOFGVW20.bib},
  doi       = {10.1007/978-3-030-44429-7\_10},
  timestamp = {Thu, 19 Mar 2020 10:40:32 +0100},
  url       = {https://doi.org/10.1007/978-3-030-44429-7\_10},
}

@InProceedings{voigt2010-matchbox-mm-matching-for-semi-automating-mapping-generation,
  author    = {Voigt, Konrad and Ivanov, Petko and Rummler, Andreas},
  booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
  title     = {MatchBox: Combined Meta-Model Matching for Semi-Automatic Mapping Generation},
  year      = {2010},
  address   = {New York, NY, USA},
  pages     = {2281–2288},
  publisher = {Association for Computing Machinery},
  series    = {SAC ’10},
  doi       = {10.1145/1774088.1774563},
  isbn      = {9781605586397},
  keywords  = {meta-model matching, model engineering, mapping generation},
  location  = {Sierre, Switzerland},
  numpages  = {8},
  url       = {https://doi.org/10.1145/1774088.1774563},
}

@Article{mader2015-raw-project-to-data-business,
  author   = {Patrick {Mäder} and Jane {Cleland-Huang}},
  journal  = {IEEE Software},
  title    = {From Raw Project Data to Business Intelligence},
  year     = {2015},
  issn     = {1937-4194},
  month    = {July},
  number   = {4},
  pages    = {22-25},
  volume   = {32},
  abstract = {VTML (Visual Trace Modeling Language) empowers project stakeholders to issue useful queries. The Web extra at https://youtu.be/RH4rvFgj8lQ is an audio podcast in which author Jane Cleland-Huang provides an audio recording of the Requirements column, in which she discusses how VTML (Visual Trace Modeling Language) empowers project stakeholders to issue useful queries.},
  doi      = {10.1109/MS.2015.92},
  keywords = {competitive intelligence;project management;visual languages;business intelligence;raw project data;visual trace modeling language;VTML;project stakeholders;Unified modeling language;Visualization;Data models;Software development;Software engineering;Visual analytics;Query processing;TIM;Traceability Information Mode;VTML;Visual Trace Modeling Language;SQL queries;trace links;software engineering;software development},
}

@InProceedings{olive2002-representation-of-generic-relationship-types-in-modeling,
  author    = {Oliv{\'e}, Antoni},
  booktitle = {Advanced Information Systems Engineering},
  title     = {Representation of Generic Relationship Types in Conceptual Modeling},
  year      = {2002},
  address   = {Berlin, Heidelberg},
  editor    = {Pidduck, Anne Banks and Ozsu, M. Tamer and Mylopoulos, John and Woo, Carson C.},
  pages     = {675--691},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {A generic relationship type is a relationship type that may have several realizations in a domain. Typical examples are IsPartOf, IsMemberOf or Materializes, but there are many others. The use of generic relationship types offers several important benefits. However, the achievement of these benefits requires an adequate representation method of the generic relationship types, and their realizations, in the conceptual schemas. In this paper, we propose two new alternative methods for this representation; we describe the contexts in which one or the other is more appropriate, and show their advantages over the current methods. We also explain the adaptation of the methods to the UML.},
  file      = {:olive2002-representation-of-generic-relationship-types-in-modeling.pdf:PDF},
  groups    = {Conceptualisation},
  isbn      = {978-3-540-47961-1},
}

@Article{bruneliere2019-feature-based-survey-of-model-view-approaches,
  author   = {Bruneliere, Hugo and Burger, Erik and Cabot, Jordi and Wimmer, Manuel},
  journal  = {Software \& Systems Modeling},
  title    = {A feature-based survey of model view approaches},
  year     = {2019},
  issn     = {1619-1374},
  number   = {3},
  pages    = {1931--1952},
  volume   = {18},
  abstract = {When dealing with complex systems, information is very often fragmented across many different models expressed within a variety of (modeling) languages. To provide the relevant information in an appropriate way to different kinds of stakeholders, (parts of) such models have to be combined and potentially revamped by focusing on concerns of particular interest for them. Thus, mechanisms to define and compute views over models are highly needed. Several approaches have already been proposed to provide (semi)automated support for dealing with such model views. This paper provides a detailed overview of the current state of the art in this area. To achieve this, we relied on our own experiences of designing and applying such solutions in order to conduct a literature review on this topic. As a result, we discuss the main capabilities of existing approaches and propose a corresponding research agenda. We notably contribute a feature model describing what we believe to be the most important characteristics of the support for views on models. We expect this work to be helpful to both current and potential future users and developers of model view techniques, as well as to any person generally interested in model-based software and systems engineering.},
  refid    = {Bruneliere2019},
  url      = {https://doi.org/10.1007/s10270-017-0622-9},
}

@InProceedings{ziegenhagen2020-tracing-life-cycles,
  author    = {Dennis Ziegenhagen and Elke Pulverm{\"{u}}ller and Andreas Speck},
  booktitle = {Proceedings of the 15th International Conference on Evaluation of Novel Approaches to Software Engineering, {ENASE} 2020, Prague, Czech Republic, May 5-6, 2020},
  title     = {Capturing Tracing Data Life Cycles for Supporting Traceability},
  year      = {2020},
  editor    = {Raian Ali and Hermann Kaindl and Leszek A. Maciaszek},
  pages     = {564--571},
  publisher = {{SCITEPRESS}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/enase/ZiegenhagenPS20.bib},
  doi       = {10.5220/0009581805640571},
  groups    = {Conceptualisation},
  timestamp = {Thu, 04 Jun 2020 17:11:38 +0200},
  url       = {https://doi.org/10.5220/0009581805640571},
}

@Article{dvandoorn2019-platform-capitalism-producing-data-assets,
  author   = {van Doorn, Niels and Badger, Adam},
  journal  = {Antipode},
  title    = {Platform Capitalism’s Hidden Abode: Producing Data Assets in the Gig Economy},
  number   = {n/a},
  volume   = {n/a},
  abstract = {Abstract In this article, we argue that the governance of gig work under conditions of financialised platform capitalism is characterised by a process that we call “dual value production”: the monetary value produced by the service provided is augmented by the use and speculative value of the data produced before, during, and after service provision. App-governed gig workers hence function as pivotal conduits in software systems that produce digital data as a particular asset class. We reflect on the production of data assets and the unequal distribution of opportunities for their valorisation, after which we survey a number of strategies seeking data-centric worker empowerment. These strategies, we argue, are crucial attempts to push back against platform capitalism’s domination, bankrolled by what we term “meta-platforms”. Ultimately, it is the massive wealth and synergetic capacities of meta-platforms that constitute the most formidable obstacle to worker power and social justice in increasingly data-driven societies.},
  doi      = {10.1111/anti.12641},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/anti.12641},
  keywords = {gig economy, platform capitalism, datafication, data assetisation, finance capital, meta-platform},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/anti.12641},
}

@InProceedings{lin2017-tiqi-NL-interface-for-querying-software-project-data,
  author    = {Jinfeng Lin and Yalin Liu and Jin Guo and Jane Cleland-Huang and William Goss and Wenchuang Liu and Sugandha Lohar and Natawut Monaikul and Alexander Rasin},
  booktitle = {2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {TiQi: A natural language interface for querying software project data},
  year      = {2017},
  month     = {Oct},
  pages     = {973-977},
  abstract  = {Software projects produce large quantities of data such as feature requests, requirements, design artifacts, source code, tests, safety cases, release plans, and bug reports. If leveraged effectively, this data can be used to provide project intelligence that supports diverse software engineering activities such as release planning, impact analysis, and software analytics. However, project stakeholders often lack skills to formulate complex queries needed to retrieve, manipulate, and display the data in meaningful ways. To address these challenges we introduce TiQi, a natural language interface, which allows users to express software-related queries verbally or written in natural language. TiQi is a web-based tool. It visualizes available project data as a prompt to the user, accepts Natural Language (NL) queries, transforms those queries into SQL, and then executes the queries against a centralized or distributed database. Raw data is stored either directly in the database or retrieved dynamically at runtime from case tools and repositories such as Github and Jira. The transformed query is visualized back to the user as SQL and augmented UML, and raw data results are returned. Our tool demo can be found on YouTube at the following link:http://tinyurl.com/TIQIDemo.},
  doi       = {10.1109/ASE.2017.8115714},
  groups    = {Conceptualisation},
  keywords  = {distributed databases;natural language interfaces;program debugging;public domain software;query processing;software engineering;SQL;Unified Modeling Language;TiQi;natural language interface;software projects;design artifacts;source code;safety cases;release plans;bug reports;project intelligence;release planning;software analytics;project stakeholders;complex queries;software project data querying;Structured Query Language;Natural languages;Hazards;Software;Tools;Distributed databases;Unified modeling language;Natural Language Interface;Project Data;Query},
}

@Article{perez2020-genetic-query-reformulation-for-feature-location,
  author      = {P{\'e}rez, Francisca and Ziadi, Tewfik and Cetina, Carlos},
  journal     = {{IEEE Transactions on Software Engineering}},
  title       = {{Utilizing Automatic Query Reformulations as Genetic Operations to Improve Feature Location in Software Models}},
  year        = {2020},
  abstract    = {In the combination of Model-Driven Engineering (MDE) and Search-Based Software Engineering (SBSE), genetic operations are one of the key ingredients. Our work proposes a novel adaptation of automatic query reformulations as genetic operations that leverage the latent semantics of software models (the cornerstone artefact of MDE). We analyze the impact of these reformulation operations in a real-world industrial case study of feature location in models. As baselines, we use: 1) the widespread single-point crossover plus random mutation; and 2) mask crossover plus random mutation, which is the best performer for feature location in models. We also perform a statistical analysis to provide quantitative evidence of the impact of the results and to show that this impact is significant. Our reformulation operations improve the results of the best baseline by 37.73% in recall and 14.08% in precision. These results are relevant for the task of feature location in models (one of the main activities performed during software maintenance and evolution). Furthermore, given that the only requirement to apply our approach is term availability in models, our work opens a new research direction to improve more tasks in MDE such as bug location or requirements traceability.},
  doi         = {10.1109/TSE.2020.3000520},
  hal_id      = {hal-02852488},
  hal_version = {v1},
  keywords    = {Automatic Query Reformulations ! ; Model-Driven Engineering ; Search-Based Software Engineering},
  pdf         = {https://hal.sorbonne-universite.fr/hal-02852488/file/TSE19_ModelFragmentReformulation.pdf},
  publisher   = {{Institute of Electrical and Electronics Engineers}},
  url         = {https://hal.sorbonne-universite.fr/hal-02852488},
}

@InProceedings{heisig2019-generic-traceability-metamodel-end-to-end-capra,
  author    = {Heisig, Philipp and Stegh\"{o}fer, Jan-Philipp and Brink, Christopher and Sachweh, Sabine},
  booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
  title     = {A Generic Traceability Metamodel for Enabling Unified End-to-End Traceability in Software Product Lines},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {2344–2353},
  publisher = {Association for Computing Machinery},
  series    = {SAC ’19},
  doi       = {10.1145/3297280.3297510},
  file      = {:heisig2019-generic-traceability-metamodel-end-to-end-capra.pdf:PDF},
  groups    = {Conceptualisation},
  isbn      = {9781450359337},
  keywords  = {requirement, software product line, component model, traceability, feature model, workflow, model-driven engineering},
  location  = {Limassol, Cyprus},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3297280.3297510},
}

@InProceedings{mader2008-rule-based-maintenance-post-requirements-traceability,
  author    = {Patrick {Mäder} and Olive {Gotel} and I. {Philippow}},
  booktitle = {2008 16th IEEE International Requirements Engineering Conference},
  title     = {Rule-Based Maintenance of Post-Requirements Traceability Relations},
  year      = {2008},
  month     = {Sep.},
  pages     = {23-32},
  abstract  = {An accurate set of traceability relations between software development artifacts is desirable to support evolutionary development. However, even where an initial set of traceability relations has been established, their maintenance during subsequent development activities is time consuming and error prone, which results in traceability decay. This paper focuses solely on the problem of maintaining a set of traceability relations in the face of evolutionary change, irrespective of whether generated manually or via automated techniques, and it limits its scope to UML-driven development activities post-requirements specification. The paper proposes an approach for the automated update of existing traceability relations after changes have been made to UML analysis and design models. The update is based upon predefined rules that recognize elementary change events as constituent steps of broader development activities. A prototype traceMaintainer has been developed to demonstrate the approach. Currently, traceMaintainer can be used with two commercial software development tools to maintain their traceability relations. The prototype has been used in two experiments. The results are discussed and our ongoing work is summarized.},
  doi       = {10.1109/RE.2008.24},
  issn      = {2332-6441},
  keywords  = {program diagnostics;software maintenance;Unified Modeling Language;rule-based maintenance;post-requirements traceability relation;traceability relations;software development artifacts;evolutionary development;traceability decay;evolutionary change;UML-driven development activities;post-requirements specification;UML analysis model;UML design model;predefined rules;Software systems;Unified modeling language;Prototypes;Programming;Software prototyping;Information retrieval;Computer science;USA Councils;Computer errors;Testing;Change;Post-requirements traceability;Rule-based traceability;Traceability maintenance},
}

@Article{antoniol2017-traceability-grand-challenges,
  author        = {Giuliano Antoniol and Jane Cleland{-}Huang and Jane Huffman Hayes and Michael Vierhauser},
  journal       = {CoRR},
  title         = {Grand Challenges of Traceability: The Next Ten Years},
  year          = {2017},
  volume        = {abs/1710.03129},
  abstract      = {n 2007, the software and systems traceability community met at the first Natural Bridge symposium on the Grand Challenges of Traceability to establish and address research goals for achieving effective, trustworthy, and ubiquitous traceability. Ten years later, in 2017, the community came together to evaluate a decade of progress towards achieving these goals. These proceedings document some of that progress. They include a series of short position papers, representing current work in the community organized across four process axes of traceability practice. The sessions covered topics from Trace Strategizing, Trace Link Creation and Evolution, Trace Link Usage, real-world applications of Traceability, and Traceability Datasets and benchmarks. Two breakout groups focused on the importance of creating and sharing traceability datasets within the research community, and discussed challenges related to the adoption of tracing techniques in industrial practice. Members of the research community are engaged in many active, ongoing, and impactful research projects. Our hope is that ten years from now we will be able to look back at a productive decade of research and claim that we have achieved the overarching Grand Challenge of Traceability, which seeks for traceability to be always present, built into the engineering process, and for it to have "effectively disappeared without a trace". We hope that others will see the potential that traceability has for empowering software and systems engineers to develop higher-quality products at increasing levels of complexity and scale, and that they will join the active community of Software and Systems traceability researchers as we move forward into the next decade of research.},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1710-03129.bib},
  eprint        = {1710.03129},
  file          = {:antoniol2017-traceability-grand-challenges.pdf:PDF},
  groups        = {meta},
  timestamp     = {Mon, 13 Aug 2018 16:48:11 +0200},
  url           = {http://arxiv.org/abs/1710.03129},
}

@InProceedings{borg2012-tracea-taxonomy-for-IR-tools,
  author    = {M. {Borg} and P. {Runeson} and L. {Brodén}},
  booktitle = {16th International Conference on Evaluation Assessment in Software Engineering (EASE 2012)},
  title     = {Evaluation of traceability recovery in context: A taxonomy for information retrieval tools},
  year      = {2012},
  month     = {May},
  pages     = {111-120},
  abstract  = {Background: Development of complex, software intensive systems generates large amounts of information. Several researchers have developed tools implementing information retrieval (IR) approaches to suggest traceability links among artifacts. Aim: We explore the consequences of the fact that a majority of the evaluations of such tools have been focused on benchmarking of mere tool output. Method: To illustrate this issue, we have adapted a framework of general IR evaluations to a context taxonomy specifically for IR-based traceability recovery. Furthermore, we evaluate a previously proposed experimental framework by conducting a study using two publicly available tools on two datasets originating from development of embedded software systems. Results: Our study shows that even though both datasets contain software artifacts from embedded development, the characteristics of the two datasets differ considerably, and consequently the traceability outcomes. Conclusions: To enable replications and secondary studies, we suggest that datasets should be thoroughly characterized in future studies on traceability recovery, especially when they can not be disclosed. Also, while we conclude that the experimental framework provides useful support, we argue that our proposed context taxonomy is a useful complement. Finally, we discuss how empirical evidence of the feasibility of IR-based traceability recovery can be strengthened in future research.},
  doi       = {10.1049/ic.2012.0014},
  groups    = {meta},
  keywords  = {embedded systems;information retrieval;program diagnostics;software engineering;system recovery;traceability recovery evaluation;information retrieval tool taxonomy;software intensive system development;traceability link;benchmarking;context taxonomy;embedded software system development;software artifact},
}

@InProceedings{borillo1992-linguistic-engineering-to-spacial-SE,
  author    = {Borillo, Mario and Borillo, Andr\'{e}e and Castell, N\'{u}ria and Latour, Dominique and Toussaint, Yannick and Verdejo, M. Felisa},
  booktitle = {Proceedings of the 10th European Conference on Artificial Intelligence},
  title     = {Applying Linguistic Engineering to Spatial Software Engineering: The Traceability Problem},
  year      = {1992},
  address   = {USA},
  pages     = {593–595},
  publisher = {John Wiley \& Sons, Inc.},
  series    = {ECAI ’92},
  isbn      = {0471936081},
  location  = {Vienna, Austria},
  numpages  = {3},
}

@Article{antoniol2002-tracing-code-documentation-links,
  author   = {G. {Antoniol} and G. {Canfora} and G. {Casazza} and A. {De Lucia} and E. {Merlo}},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Recovering traceability links between code and documentation},
  year     = {2002},
  issn     = {1939-3520},
  month    = {Oct},
  number   = {10},
  pages    = {970-983},
  volume   = {28},
  abstract = {Software system documentation is almost always expressed informally in natural language and free text. Examples include requirement specifications, design documents, manual pages, system development journals, error logs, and related maintenance reports. We propose a method based on information retrieval to recover traceability links between source code and free text documents. A premise of our work is that programmers use meaningful names for program items, such as functions, variables, types, classes, and methods. We believe that the application-domain knowledge that programmers process when writing the code is often captured by the mnemonics for identifiers; therefore, the analysis of these mnemonics can help to associate high-level concepts with program concepts and vice-versa. We apply both a probabilistic and a vector space information retrieval model in two case studies to trace C++ source code onto manual pages and Java code to functional requirements. We compare the results of applying the two models, discuss the benefits and limitations, and describe directions for improvements.},
  doi      = {10.1109/TSE.2002.1041053},
  keywords = {system documentation;information retrieval;object-oriented programming;natural languages;probability;software system documentation;information retrieval;object orientation;traceability;program comprehension;traceability link recovery;source code;free text documents;vector space;Documentation;Programming profession;Information retrieval;Natural languages;Context modeling;Information resources;Writing;Java;Inspection;Mathematics},
}

@InProceedings{marcus2003-latent-semantic-indexing-for-traceability-LSI,
  author    = {Andrian {Marcus} and Jonathan I. {Maletic}},
  booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
  title     = {Recovering documentation-to-source-code traceability links using latent semantic indexing},
  year      = {2003},
  month     = {May},
  pages     = {125-135},
  abstract  = {An information retrieval technique, latent semantic indexing, is used to automatically identify traceability links from system documentation to program source code. The results of two experiments to identify links in existing software systems (i.e., the LEDA library, and Albergate) are presented. These results are compared with other similar type experimental results of traceability link identification using different types of information retrieval techniques. The method presented proves to give good results by comparison and additionally it is a low cost, highly flexible method to apply with regards to preprocessing and/or parsing of the source code and documentation.},
  doi       = {10.1109/ICSE.2003.1201194},
  issn      = {0270-5257},
  keywords  = {information retrieval;system documentation;computer aided software engineering;indexing;information retrieval technique;latent semantic indexing;system documentation;program source code;traceability link identification;Indexing;Documentation;Information retrieval;Software systems;Information analysis;Costs;Software engineering;Natural languages;Computer science;Software libraries},
}

@InProceedings{mcmillan2009-combining-text-and-structural-analysis-for-traceability,
  author    = {C. {McMillan} and D. {Poshyvanyk} and M. {Revelle}},
  booktitle = {2009 ICSE Workshop on Traceability in Emerging Forms of Software Engineering},
  title     = {Combining textual and structural analysis of software artifacts for traceability link recovery},
  year      = {2009},
  month     = {May},
  pages     = {41-48},
  abstract  = {Existing methods for recovering traceability links among software documentation artifacts analyze textual similarities among these artifacts. It may be the case, however, that related documentation elements share little terminology or phrasing. This paper presents a technique for indirectly recovering these traceability links in requirements documentation by combining textual with structural information as we conjecture that related requirements share related source code elements. A preliminary case study indicates that our combined approach improves the precision and recall of recovering relevant links among documents as compared to stand-alone methods based solely on analyzing textual similarities.},
  doi       = {10.1109/TEFSE.2009.5069582},
  issn      = {2157-2194},
  keywords  = {information retrieval;software engineering;system documentation;textual analysis;structural analysis;software documentation artifacts;traceability link recovery;requirements documentation;source code elements;Documentation;Large scale integration;Matrix decomposition;Terminology;Information analysis;Computer science;Educational institutions;Information retrieval;Software tools;Indexing},
}

@Article{delucia2012-information-retrieval-for-traceability,
  author    = {De Lucia, Andrea and Marcus, Andrian and Oliveto, Rocco and Poshyvanyk, Denys},
  journal   = {Software and Systems Traceability},
  title     = {Information Retrieval Methods for Automated Traceability Recovery},
  year      = {2012},
  pages     = {71--98},
  abstract  = {The potential benefits of traceability are well known and documented, as well as the impracticability of recovering and maintaining traceability links manually. Indeed, the manual management of traceability information is an error prone and time consuming task. Consequently, despite the advantages that can be gained, explicit traceability is rarely established unless there is a regulatory reason for doing so. Extensive efforts have been brought forth to improve the explicit connection of software artifacts in the software engineering community (both research and commercial). Promising results have been achieved using Information Retrieval (IR) techniques for traceability recovery. IR-based traceability recovery methods propose a list of candidate traceability links based on the similarity between the text contained in the software artifacts. Software artifacts have different structures and the common element among many of them is the textual data, which most often captures the informal semantics of artifacts. For example, source code includes large volume of textual data in the form of comments and identifiers. In consequence, IR-based approaches are very well suited to address the traceability recovery problem. The conjecture is that artifacts with high textual similarity are good candidates to be traced to each other since they share several concepts. In this chapter we overview a general process of using IR-based methods for traceability link recovery and overview some of them in a greater detail: probabilistic, vector space, and Latent Semantic Indexing models. Finally, we discuss common approaches to measuring the performance of IR-based traceability recovery methods and the latest advances in techniques for the analysis of candidate links.},
  address   = {London},
  booktitle = {Software and Systems Traceability},
  doi       = {10.1007/978-1-4471-2239-5_4},
  editor    = {Cleland-Huang, Jane and Gotel, Orlena and Zisman, Andrea},
  file      = {:delucia2012-information-retrieval-for-traceability.pdf:PDF},
  groups    = {AI, meta},
  isbn      = {978-1-4471-2239-5},
  publisher = {Springer London},
  url       = {https://doi.org/10.1007/978-1-4471-2239-5_4},
}

@TechReport{robillard2007-empirical-study-of-assignement-problem,
  author      = {Martin P. Robillard},
  institution = {McGill University},
  title       = {An empirical study of the concept assignment problem},
  year        = {2007},
}

@InProceedings{badreddin2014-req-traceability-model-based-approach,
  author    = {Omar {Badreddin} and Arnon {Sturm} and Timothy C. {Lethbridge}},
  booktitle = {2014 IEEE 4th International Model-Driven Requirements Engineering Workshop (MoDRE)},
  title     = {Requirement traceability: A model-based approach},
  year      = {2014},
  month     = {Aug},
  pages     = {87-91},
  abstract  = {Requirements tractability remains challenging, particularly in the prevalence of code centric approaches. Similarly, within the emerging model centric paradigm, requirements traceability is addressed only to a limited extent. To facilitate such traceability, we call for representing requirements as first class entities in the emerging paradigm of model-oriented programming. This has the objective of enabling software developers, modelers, and business analysts to manipulate requirements entities as textual model and code elements. To illustrate the feasibility of such an approach, we propose a Requirement-Oriented Modeling and Programming Language (ROMPL) that demonstrates how modeling abstractions can be utilized to manage the behavior and relationships of key requirements entities.},
  doi       = {10.1109/MoDRE.2014.6890829},
  groups    = {MDE},
  keywords  = {formal verification;object-oriented programming;requirement traceability;model-oriented programming;code centric approach;textual model;code element;requirement-oriented modeling;programming language;ROMPL;Unified modeling language;Object oriented modeling;Business;Software;Computational modeling;Syntactics;Requirements;Modeling;Action languages;Domain Specific Language;MDA},
}

@InProceedings{panichella2013-using-structural-information-to-improve-IR-traceability,
  author    = {Annibale Panichella and Collin McMillan and Evan Moritz and Davide Palmieri and Rocco Oliveto and Denys Poshyvanyk and Andrea De Lucia},
  booktitle = {2013 17th European Conference on Software Maintenance and Reengineering},
  title     = {When and How Using Structural Information to Improve IR-Based Traceability Recovery},
  year      = {2013},
  month     = {March},
  pages     = {199-208},
  abstract  = {Information Retrieval (IR) has been widely accepted as a method for automated traceability recovery based on the textual similarity among the software artifacts. However, a notorious difficulty for IR-based methods is that artifacts may be related even if they are not textually similar. A growing body of work addresses this challenge by combining IR-based methods with structural information from source code. Unfortunately, the accuracy of such methods is highly dependent on the IR methods. If the IR methods perform poorly, the combined approaches may perform even worse. In this paper, we propose to use the feedback provided by the software engineer when classifying candidate links to regulate the effect of using structural information. Specifically, our approach only considers structural information when the traceability links from the IR methods are verified by the software engineer and classified as correct links. An empirical evaluation conducted on three systems suggests that our approach outperforms both a pure IR-based method and a simple approach for combining textual and structural information.},
  doi       = {10.1109/CSMR.2013.29},
  issn      = {1534-5351},
  keywords  = {information retrieval;pattern classification;program diagnostics;structural information;information retrieval-based traceability recovery;automated traceability recovery;textual similarity;software artifact;source code;candidate link classification;Software;Vectors;Accuracy;Medical services;Context;Indexes;Educational institutions;Traceability Link Recovery;Empirical studies},
}

@InProceedings{bavota2013-role-of-artefact-corpus-in-LSI-traceability,
  author    = {Gabriele Bavota and Andrea De Lucia and Rocco Oliveto and Annibale Panichella and Fabio Ricci and Genoveffa Tortora},
  booktitle = {2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)},
  title     = {The role of artefact corpus in LSI-based traceability recovery},
  year      = {2013},
  month     = {May},
  pages     = {83-89},
  abstract  = {Latent Semantic Indexing (LSI) is an advanced method widely and successfully employed in Information Retrieval (IR). It is an extension of Vector Space Model (VSM) and it is able to overcome VSM in canonical IR scenarios where it is used on very large document repositories. LSI has also been used to semi-automatically generate traceability links between software artefacts. However, in such a scenario LSI is not able to overcome VSM. This contradicting result is probably due to the different characteristics of software artefact repositories as compared to document repositories. In this paper we present a preliminary empirical study to analyze how the size and the vocabulary of the repository-in terms of number of documents and terms (i.e., the vocabulary)-affects the retrieval accuracy. Even if replications are needed to generalize our findings, the study presented in this paper provides some insights that might be used as guidelines for selecting the more adequate methods to be used for traceability recovery depending on the particular application context.},
  doi       = {10.1109/TEFSE.2013.6620160},
  issn      = {2157-2194},
  keywords  = {database indexing;document handling;information retrieval;program diagnostics;artefact corpus;traceability recovery;document repositories;software artefact repositories;semiautomatically traceability link generation;document repositories;canonical IR scenarios;VSM;vector space model;information retrieval;latent semantic indexing;LSI-based traceability recovery;Large scale integration;Accuracy;Indexing;Vocabulary;Vectors;Software;Unified modeling language;Traceability recovery;Latent Semantic Indexing;Vector Space Model;Empirical Studies},
}

@Article{laghouaouta2017-model-composition-tracaebility,
  author   = {Youness Laghouaouta and Adil Anwar and Mahmoud Nassar and Bernard Coulette},
  journal  = {Information and Software Technology},
  title    = {A dedicated approach for model composition traceability},
  year     = {2017},
  issn     = {0950-5849},
  pages    = {142 - 159},
  volume   = {91},
  abstract = {Context: Software systems are often too complex to be expressed by a single model. Recognizing this, the Model Driven Engineering (MDE) proposes multi-modeling approaches to allow developers to describe a system from different perspectives. In this context, model composition has become important since the combination of those partial representations is inevitable. Nevertheless, no approach has been defined for keeping track of the composition effects, and this operation has been overshadowed by model transformations.
Objective
This paper presents a traceability approach dedicated to the composition of models. Two aspects of quality are considered: producing relevant traces; and dealing with scalability.
Method
The composition of softgoal trees has been selected to motivate the need for tracing the composition of models and to illustrate our approach. The base principle is to augment the specification of the composition with the behavior needed to generate the expected composed model accompanied with a trace model. This latter includes traces of the execution details. For that, traceability is considered as a crosscutting concern and encapsulated in an aspect. As part of the proposal, an Eclipse plug-in has been implemented as a tool support. Besides, a comparative experiment has been conducted to assess the traces relevance. We also used the regression method to validate the scalability of the tool support.
Results
Our experiments show that the proposed approach allows generating relevant traces. In addition, the obtained results reveal that tracing a growing number of elements causes an acceptable increase of response time.
Conclusion
This paper presents a traceability approach dedicated to the composition of models and its application to softgoal trees. The experiment results reveal that our proposal considers the composition specificities for producing valuable traceability information while supporting scalability.},
  doi      = {https://doi.org/10.1016/j.infsof.2017.07.002},
  groups   = {MDE},
  keywords = {Model traceability, Model composition, Aspect-oriented modeling, Graph transformations, NFR framework},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584917304494},
}

@Article{sillito2008-questions-during-programming-change,
  author   = {Jonathan Sillito and Gail C. Murphy and Kris De Volder},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Asking and Answering Questions during a Programming Change Task},
  year     = {2008},
  issn     = {1939-3520},
  month    = {July},
  number   = {4},
  pages    = {434-451},
  volume   = {34},
  abstract = {Little is known about the specific kinds of questions programmers ask when evolving a code base and how well existing tools support those questions. To better support the activity of programming, answers are needed to three broad research questions: 1) What does a programmer need to know about a code base when evolving a software system? 2) How does a programmer go about finding that information? 3) How well do existing tools support programmers in answering those questions? We undertook two qualitative studies of programmers performing change tasks to provide answers to these questions. In this paper, we report on an analysis of the data from these two user studies. This paper makes three key contributions. The first contribution is a catalog of 44 types of questions programmers ask during software evolution tasks. The second contribution is a description of the observed behavior around answering those questions. The third contribution is a description of how existing deployed and proposed tools do, and do not, support answering programmers' questions.},
  doi      = {10.1109/TSE.2008.26},
  keywords = {software maintenance;software tools;programming change task;code base;software system;software evolution;Programming profession;Data analysis;Software systems;Software tools;Genetic programming;Lab-on-a-chip;IEEE activities;Computer science;Software psychology;Programming Environments/Construction Tools;Enhancement;Software psychology;Programming Environments/Construction Tools;Enhancement},
}

@InCollection{HoQuang2020-community-infrastructure-for-big-data-research-software-architecture,
  author    = {Truong Ho-Quang and Michel R.V. Chaudron and Regina Hebig and Gregorio Robles},
  booktitle = {Model Management and Analytics for Large Scale Systems},
  publisher = {Elsevier},
  title     = {Challenges and directions for a community infrastructure for Big Data-driven research in software architecture},
  year      = {2020},
  pages     = {13--35},
  doi       = {10.1016/b978-0-12-816649-9.00010-7},
  url       = {https://doi.org/10.1016/b978-0-12-816649-9.00010-7},
}

@InProceedings{musil2017-continuous-architectural-knowledge-integration,
  author    = {Juergen Musil and Fajar J. Ekaputra and Marta Sabou and Tudor Ionescu and Daniel Schall and Angelika Musil and Stefan Biffl},
  booktitle = {2017 IEEE International Conference on Software Architecture (ICSA)},
  title     = {Continuous Architectural Knowledge Integration: Making Heterogeneous Architectural Knowledge Available in Large-Scale Organizations},
  year      = {2017},
  month     = {April},
  pages     = {189-192},
  abstract  = {The timely discovery, sharing and integration of architectural knowledge (AK) have become critical aspects in enabling the software architects to make meaningful conceptual and technical design decisions and trade-offs. In large-scale organizations particular obstacles in making AK available to architects are a heterogeneous pool of internal and external knowledge sources, poor interoperability between AK management tools and limited support of computational AK reasoning. Therefore we introduce the Continuous Architectural Knowledge Integration (CAKI) approach that combines the continuous integration of internal and external AK sources together with enhanced semantic reasoning and personalization capabilities dedicated to large organizations. Preliminary evaluation results show that CAKI potentially reduces AK search effort by concurrently yielding more diverse and relevant results.},
  doi       = {10.1109/ICSA.2017.28},
  keywords  = {data mining;inference mechanisms;knowledge management;open systems;organisational aspects;software architecture;architectural knowledge discovery;architectural knowledge sharing;architectural knowledge integration;conceptual design decisions;technical design decisions;large-scale organization;external knowledge sources;internal knowledge sources;interoperability;continuous architectural knowledge integration;CAKI;external AK sources;internal AK sources;personalization capabilities;semantic reasoning;Tools;Semantics;Cognition;Adaptation models;Software architecture;Organizations;Software;Architectural knowledge management;continuous software architecture;semantic integration},
}

@InCollection{barker2008-scientific-workflow-survey-and-research-directions,
  author    = {Adam Barker and Jano van Hemert},
  booktitle = {Parallel Processing and Applied Mathematics},
  publisher = {Springer Berlin Heidelberg},
  title     = {Scientific Workflow: A Survey and Research Directions},
  year      = {2008},
  pages     = {746--753},
  doi       = {10.1007/978-3-540-68111-3_78},
  url       = {https://doi.org/10.1007/978-3-540-68111-3_78},
}

@InProceedings{dosimont2014-eficient-analysis-methodology-for-huge-application-traces,
  author    = { Damien Dosimont and Generoso Pagano and Guillaume Huard and Vania Marangozova-Martin and Jean-Marc Vincent},
  booktitle = {2014 International Conference on High Performance Computing Simulation (HPCS)},
  title     = {Efficient analysis methodology for huge application traces},
  year      = {2014},
  month     = {July},
  pages     = {951-958},
  abstract  = {The growing complexity of computer system hardware and software makes their behavior analysis a challenging task. In this context, tracing appears to be a promising solution as it provides relevant information about the system execution. However, trace analysis techniques and tools lack in providing the analyst the way to perform an efficient analysis flow because of several issues. First, traces contain a huge volume of data difficult to store, load in memory and work with. Then, the analysis flow is hindered by various result formats, provided by different analysis techniques, often incompatible. Last, analysis frameworks lack an entry point to understand the traced application general behavior. Indeed, traditional visualization techniques suffer from time and space scalability issues due to screen size, and are not able to represent the full trace. In this article, we present how to do an efficient analysis by using the Shneiderman's mantra: “Overview first, zoom and filter, then details on demand”. Our methodology is based on FrameSoC, a trace management infrastructure that provides solutions for trace storage, data access, and analysis flow, managing analysis results and tool. Ocelotl, a visualization tool, takes advantage of FrameSoC and shows a synthetic representation of a trace by using a time aggregation. This visualization solves scalability issues and provides an entry point for the analysis by showing phases and behavior disruptions, with the objective of getting more details by focusing on the interesting trace parts.},
  doi       = {10.1109/HPCSim.2014.6903791},
  keywords  = {data visualisation;program debugging;program diagnostics;huge application traces;computer system hardware;computer system software;trace analysis techniques;visualization techniques;time scalability;space scalability;FrameSoC infrastructure;trace management infrastructure;storage flow;data access flow;analysis flow;Ocelotl visualization tool;time aggregation;Measurement;Complexity theory;Indexing;Data visualization;Arrays;Vectors;Application analysis;trace management;analysis tools;visualization tools;debugging;performance analysis},
}

@InProceedings{borg2013-IR-in-traceability-birds-view,
  author    = {Markus Borg and Per Runeson},
  booktitle = {2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
  title     = {IR in Software Traceability: From a Bird's Eye View},
  year      = {2013},
  month     = {Oct},
  pages     = {243-246},
  abstract  = {Background. Several researchers have proposed creating after-the-fact structure among software artifacts using trace recovery based on Information Retrieval (IR). Due to significant variation points in previous studies, results are not easily aggregated. Aim. We aim at an overview picture of the outcome of previous evaluations. Method. Based on a systematic mapping study, we perform a synthesis of published research. Results. Our synthesis shows that there are no empirical evidence that any IR model outperforms another model consistently. We also display a strong dependency between the Precision and Recall (P-R) values and the input datasets. Finally, our mapping of P-R values on the possible output space highlights the difficulty of recovering accurate trace links using naïve cut-off strategies. Conclusion. Based on our findings, we stress the need for empirical evaluations beyond the basic P-R 'race'.},
  comment   = {All IR technic give comparable P-R results},
  doi       = {10.1109/ESEM.2013.39},
  groups    = {meta},
  issn      = {1949-3789},
  keywords  = {information retrieval;software engineering;software traceability;software artifacts;trace recovery;information retrieval;IR model;precision and recall value;software engineering;Software;Information retrieval;Software engineering;Standards;Accuracy;Large scale integration;Systematics;empirical software engineering;software traceability;information retrieval;secondary study},
}

@Article{anquetil2010-model-driven-tracea-for-SPL,
  author   = {Anquetil, Nicolas and Kulesza, Uirá and Mitschke, Ralf and Moreira, Ana and Royer, Jean-Claude and Rummler, Andreas and Sousa, André},
  journal  = {Software and Systems Modeling},
  title    = {A model-driven traceability framework for software product lines},
  year     = {2010},
  issn     = {1619-1374},
  number   = {4},
  pages    = {427--451},
  volume   = {9},
  abstract = {Software product line (SPL) engineering is a recent approach to software development where a set of software products are derived for a well defined target application domain, from a common set of core assets using analogous means of production (for instance, through Model Driven Engineering). Therefore, such family of products are built from reuse, instead of developed individually from scratch. SPL promise to lower the costs of development, increase the quality of software, give clients more flexibility and reduce time to market. These benefits come with a set of new problems and turn some older problems possibly more complex. One of these problems is traceability management. In the European AMPLE project we are creating a common traceability framework across the various activities of the SPL development. We identified four orthogonal traceability dimensions in SPL development, one of which is an extension of what is often considered as “traceability of variability”. This constitutes one of the two contributions of this paper. The second contribution is the specification of a metamodel for a repository of traceability links in the context of SPL and the implementation of a respective traceability framework. This framework enables fundamental traceability management operations, such as trace import and export, modification, query and visualization. The power of our framework is highlighted with an example scenario.},
  file     = {:anquetil2010-model-driven-tracea-for-SPL.pdf:PDF},
  groups   = {MDE},
  refid    = {Anquetil2010},
  url      = {https://doi.org/10.1007/s10270-009-0120-9},
}

@Article{diaz2015-tracing-variability-from-features-to-product-line-SPL,
  author   = {Díaz, Jessica and Pérez, Jennifer and Garbajosa, Juan},
  journal  = {Requirements Engineering},
  title    = {A model for tracing variability from features to product-line architectures: a case study in smart grids},
  year     = {2015},
  issn     = {1432-010X},
  number   = {3},
  pages    = {323--343},
  volume   = {20},
  abstract = {In current software systems with highly volatile requirements, traceability plays a key role to maintain the consistency between requirements and code. Traceability between artifacts involved in the development of software product line (SPL) is still more critical because it is necessary to guarantee that the selection of variants that realize the different SPL products meet the requirements. Current SPL traceability mechanisms trace from variability in features to variations in the configuration of product-line architecture (PLA) in terms of adding and removing components. However, it is not always possible to materialize the variable features of a SPL through adding or removing components, since sometimes they are materialized inside components, i.e., in part of their functionality: a class, a service, and/or an interface. Additionally, variations that happen inside components may crosscut several components of architecture. These kinds of variations are still challenging and their traceability is not currently well supported. Therefore, it is not possible to guarantee that those SPL products with these kinds of variations meet the requirements. This paper presents a solution for tracing variability from features to PLA by taking these kinds of variations into account. This solution is based on models and traceability between models in order to automate SPL configuration by selecting the variants and realizing the product application. The FPLA modeling framework supports this solution which has been deployed in a software factory. Validation has consisted in putting the solution into practice to develop a product line of power metering management applications for smart grids.},
  groups   = {SPL},
  refid    = {Díaz2015},
  url      = {https://doi.org/10.1007/s00766-014-0203-1},
}

@Article{paige2011-traces-in-moel-driven-engineering,
  author   = {Paige, Richard F. and Drivalos, Nikolaos and Kolovos, Dimitrios S. and Fernandes, Kiran J. and Power, Christopher and Olsen, Goran K. and Zschaler, Steffen},
  journal  = {Software \& Systems Modeling},
  title    = {Rigorous identification and encoding of trace-links in model-driven engineering},
  year     = {2011},
  issn     = {1619-1374},
  number   = {4},
  pages    = {469--487},
  volume   = {10},
  abstract = {Model-driven engineering (MDE) involves the construction and manipulation of many models of different kinds in an engineering process. In principle, models can be used in the product engineering lifecycle in an end-to-end manner for representing requirements, designs and implementations, and assisting in deployment and maintenance. The manipulations applied to models may be manual, but they can also be automated--for example, using model transformations, code generation, and validation. To enhance automated analysis, consistency and coherence of models used in an MDE process, it is useful to identify, establish and maintain trace-links between models. However, the breadth and scope of trace-links that can be used in MDE is substantial, and managing trace-link information can be very complex. In this paper, we contribute to managing the complexity of traceability information in MDE in two ways: firstly, we demonstrate how to identify the different kinds of trace-links that may appear in an end-to-end MDE process; secondly, we describe a rigorous approach to defining semantically rich trace-links between models, where the models themselves may be constructed using diverse modelling languages. The definition of rich trace-links allows us to use tools to maintain and analyse traceability relationships.},
  groups   = {MDE},
  refid    = {Paige2011},
  url      = {https://doi.org/10.1007/s10270-010-0158-8},
}

@InProceedings{drivalos2009-engineering-DSL-for-traceability,
  author    = {Drivalos, Nikolaos and Kolovos, Dimitrios S. and Paige, Richard F. and Fernandes, Kiran J.},
  booktitle = {Software Language Engineering},
  title     = {Engineering a {DSL} for Software Traceability},
  year      = {2009},
  address   = {Berlin, Heidelberg},
  editor    = {Ga{\v{s}}evi{\'{c}}, Dragan and L{\"a}mmel, Ralf and Van Wyk, Eric},
  pages     = {151--167},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The software artefacts at different levels of abstraction and at different stages of the development process are closely inter-related. For developers to stay in control of the development process, traceability information must be maintained. In this paper, we present the engineering of the Traceability Metamodelling Language (TML), a metamodelling language dedicated to defining traceability metamodels. We present the abstract syntax of the language and its semantics, which are defined using a translational approach. Finally, we provide a case study that demonstrates the construction of a traceability metamodel that captures traceability information between two metamodels using TML.},
  file      = {:drivalos2009-engineering-DSL-for-traceability.pdf:PDF},
  groups    = {MDE},
  isbn      = {978-3-642-00434-6},
}

@Article{vale2017-SPL-traceability-a-SMS,
  author   = {Tassio Vale and Eduardo Santana [de Almeida] and Vander Alves and Uirá Kulesza and Nan Niu and Ricardo [de Lima]},
  journal  = {Information and Software Technology},
  title    = {Software product lines traceability: A systematic mapping study},
  year     = {2017},
  issn     = {0950-5849},
  pages    = {1 - 18},
  volume   = {84},
  abstract = {Context: Traceability in Software Product Lines (SPL) is the ability to interrelate software engineering artifacts through required links to answer specific questions related to the families of products and underlying development processes. Despite the existence of studies to map out available evidence on traceability for single systems development, there is a lack of understanding on common strategies, activities, artifacts, and research gaps for SPL traceability. Objective: This paper analyzes 62 studies dating from 2001 to 2015 and discusses seven aspects of SPL traceability: main goals, strategies, application domains, research intensity, research challenges, rigor, and industrial relevance. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas calling for further research. Method: To gather evidence, we defined a mapping study process adapted from existing guidelines. Driven by a set of research questions, this process comprises three major phases: planning, conducting, and documenting the review. Results: This work provides a structured understanding of SPL traceability, indicating areas for further research. The lack of evidence regarding the application of research methods indicates the need for more rigorous SPL traceability studies with better description of context, study design, and limitations. For practitioners, although most identified studies have low industrial relevance, a few of them have high relevance and thus could provide some decision making support for application of SPL traceability in practice. Conclusions: This work concludes that SPL traceability is maturing and pinpoints areas where further investigation should be performed. As future work, we intend to improve the comparison between traceability proposals for SPL and single-system development.},
  doi      = {https://doi.org/10.1016/j.infsof.2016.12.004},
  groups   = {meta},
  keywords = {Systematic mapping study, Software product lines, Software and systems traceability, Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584916304463},
}

@InProceedings{clelandhuang2014-traceability-trends-and-futurte-direction,
  author    = {Cleland-Huang, Jane and Gotel, Orlena C. Z. and Huffman Hayes, Jane and M\"{a}der, Patrick and Zisman, Andrea},
  booktitle = {Future of Software Engineering Proceedings},
  title     = {Software Traceability: Trends and Future Directions},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {55–69},
  publisher = {Association for Computing Machinery},
  series    = {FOSE 2014},
  abstract  = {Software traceability is a sought-after, yet often elusive quality in software-intensive systems. Required in safety-critical systems by many certifying bodies, such as the USA Federal Aviation Authority, software traceability is an essential element of the software development process. In practice, traceability is often conducted in an ad-hoc, after-the-fact manner and, therefore, its benefits are not always fully realized. Over the past decade, researchers have focused on specific areas of the traceability problem, developing more sophisticated tooling, promoting strategic planning, applying information retrieval techniques capable of semi-automating the trace creation and maintenance process, developing new trace query languages and visualization techniques that use trace links, and applying traceability in specific domains such as Model Driven Development, product line systems, and agile project environments. In this paper, we build upon a prior body of work to highlight the state-of-the-art in software traceability, and to present compelling areas of research that need to be addressed.},
  doi       = {10.1145/2593882.2593891},
  file      = {:clelandhuang2014-traceability-trends-and-futurte-direction.pdf:PDF},
  groups    = {meta},
  isbn      = {9781450328654},
  keywords  = {Software Traceability, Roadmap},
  location  = {Hyderabad, India},
  numpages  = {15},
  url       = {https://doi.org/10.1145/2593882.2593891},
}

@InProceedings{gallina2014-model-driven-certification-method-for-process-compliance,
  author    = {Barbara {Gallina}},
  booktitle = {2014 IEEE International Symposium on Software Reliability Engineering Workshops},
  title     = {A Model-Driven Safety Certification Method for Process Compliance},
  year      = {2014},
  month     = {Nov},
  pages     = {204-209},
  abstract  = {A safety case is a contextualized structured argument constituted of process and product-based sub-arguments to show that a system is acceptably safe. The creation of a safety case is an extremely time-consuming and costly activity needed for certification purposes. To reduce time and cost, reuse as well as automatic generation possibilities represent urgent research directions. In this paper, we focus on safety processes mandated by prescriptive standards and we identify process-related structures from which process-based arguments (those aimed at showing that a required development process has been applied according to the standard) can be generated and more easily reused. Then, we propose a model-driven safety certification method to derive those arguments as goal structures given in Goal Structuring Notation from process models given in compliance with Software Process Engineering Meta-model 2.0. The method is illustrated by generating process-based arguments in the context of ISO 26262.},
  doi       = {10.1109/ISSREW.2014.30},
  groups    = {MDE},
  keywords  = {certification;safety-critical software;software engineering;software standards;model-driven safety certification method;process compliance;safety case;contextualized structured argument;process subargument;safety process;process-related structure;process-based argument;goal structures;goal structuring notation;Software Process Engineering Meta-model 2.0;ISO 26262;Hazards;Software;Context;ISO standards;Automotive engineering;Safety processes;safety cases;process-based arguments;safety standards;model driven engineering;Software Process Engineering Meta-model (SPEM) 2.0;Structured Assurance Case Metamodel (SACM);Goal Structuring Notation (GSN)},
}

@InProceedings{asuncion2010-software-traceability-with-topic-modeling,
  author    = {Hazeline U. Asuncion and Arthur U. Asuncion and Richard N. Taylor},
  booktitle = {2010 ACM/IEEE 32nd International Conference on Software Engineering},
  title     = {Software traceability with topic modeling},
  year      = {2010},
  month     = {May},
  pages     = {95-104},
  volume    = {1},
  abstract  = {Software traceability is a fundamentally important task in software engineering. The need for automated traceability increases as projects become more complex and as the number of artifacts increases. We propose an automated technique that combines traceability with a machine learning technique known as topic modeling. Our approach automatically records traceability links during the software development process and learns a probabilistic topic model over artifacts. The learned model allows for the semantic categorization of artifacts and the topical visualization of the software system. To test our approach, we have implemented several tools: an artifact search tool combining keyword-based search and topic modeling, a recording tool that performs prospective traceability, and a visualization tool that allows one to navigate the software architecture and view semantic topics associated with relevant artifacts and architectural components. We apply our approach to several data sets and discuss how topic modeling enhances software traceability, and vice versa.},
  doi       = {10.1145/1806799.1806817},
  groups    = {AI},
  issn      = {1558-1225},
  keywords  = {learning (artificial intelligence);probability;software engineering;software traceability;topic modeling;software engineering;automated traceability;machine learning technique;software development process;probabilistic topic model;semantic categorization;topical visualization;software architecture;Semantics;Software;Machine learning;Visualization;Large scale integration;Probabilistic logic;Resource management;latent dirichlet allocation;software architecture;software traceability;topic model},
}

@InCollection{bouillon2013-survey-on-usage-scenario-requirements-traceability-in-practice,
  author    = {Elke Bouillon and Patrick M\"{a}der and Ilka Philippow},
  booktitle = {Requirements Engineering: Foundation for Software Quality},
  publisher = {Springer Berlin Heidelberg},
  title     = {A Survey on Usage Scenarios for Requirements Traceability in Practice},
  year      = {2013},
  pages     = {158--173},
  abstract  = {[Context and motivation] Requirements traceability is known as an important part of development projects. Studies showed that traceability is applied in practice, but insufficient tool- and method-support hinders its practical use. [Question/problem] We conducted a survey to understand which traceability usage scenarios are most relevant for practitioners. Gaining this information is a required step for providing better traceability support to practitioners. [Principal ideas/results] We identified a list of 29 regularly cited usage scenarios and asked practitioners to assess the frequency of use for each in a typical development project. Our analysis is restricted to those 56 participants that were actively using traceability in order to ensure comparable results. Subjects held various roles in the development and reported about diverse projects. [Contribution] This study provides not only an initial catalog of usage scenarios and their relevance, but also provides insights on practitioner’s traceability practices. In result, we found all scenarios to be used by practitioners. Participants use traceability especially for: finding origin and rationale of requirements, documenting a requirement’s history, and tracking requirement or task implementation state. Furthermore, we highlight topics for ongoing evaluation and better method and tool support in the area of requirements traceability.},
  doi       = {10.1007/978-3-642-37422-7_12},
  groups    = {meta},
  url       = {https://doi.org/10.1007/978-3-642-37422-7_12},
}

@InCollection{clelandHuang2012-trace-queries-safety-requirements-assurance,
  author    = {Jane Cleland-Huang and Mats Heimdahl and Jane Huffman Hayes and Robyn Lutz and Patrick Maeder},
  booktitle = {Requirements Engineering: Foundation for Software Quality},
  publisher = {Springer Berlin Heidelberg},
  title     = {Trace Queries for Safety Requirements in High Assurance Systems},
  year      = {2012},
  pages     = {179--193},
  abstract  = {[Context and motivation] Safety critical software systems pervade almost every facet of our lives. We rely on them for safe air and automative travel, healthcare diagnosis and treatment, power generation and distribution, factory robotics, and advanced assistance systems for special-needs consumers. [Question/Problem] Delivering demonstrably safe systems is difficult, so certification and regulatory agencies routinely require full life-cycle traceability to assist in evaluating them. In practice, however, the traceability links provided by software producers are often incomplete, inaccurate, and ineffective for demonstrating software safety. Also, there has been insufficient integration of formal method artifacts into such traceability. [Principal ideas/results] To address these weaknesses we propose a family of reusable traceability queries that serve as a blueprint for traceability in safety critical systems. In particular we present queries that consider formal artifacts, designed to help demonstrate that: 1) identified hazards are addressed in the safety-related requirements, and 2) the safety-related requirements are realized in the implemented system. We model these traceability queries using the Visual Trace Modeling Language, which has been shown to be more intuitive than the defacto SQL standard. [Contribution] Practitioners building safety critical systems can use these trace queries to make their traceability efforts more complete, accurate and effective. This, in turn, can assist in building safer software systems and in demonstrating their adequate handling of hazards.},
  doi       = {10.1007/978-3-642-28714-5_16},
  url       = {https://doi.org/10.1007/978-3-642-28714-5_16},
}

@InProceedings{dietrich2013-learning-efective-query-transformation-for-enhanced-req-trace-retrieval,
  author    = {Timothy Dietrich and Jane Cleland-Huang and Yonghee Shin},
  booktitle = {2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {Learning effective query transformations for enhanced requirements trace retrieval},
  year      = {2013},
  month     = {Nov},
  pages     = {586-591},
  abstract  = {In automated requirements traceability, significant improvements can be realized through incorporating user feedback into the trace retrieval process. However, existing feedback techniques are designed to improve results for individual queries. In this paper we present a novel technique designed to extend the benefits of user feedback across multiple trace queries. Our approach, named Trace Query Transformation (TQT), utilizes a novel form of Association Rule Mining to learn a set of query transformation rules which are used to improve the efficacy of future trace queries. We evaluate TQT using two different kinds of training sets. The first represents an initial set of queries directly modified by human analysts, while the second represents a set of queries generated by applying a query optimization process based on initial relevance feedback for trace links between a set of source and target documents. Both techniques are evaluated using requirements from theWorldVista Healthcare system, traced against certification requirements for the Commission for Healthcare Information Technology. Results show that the TQT technique returns significant improvements in the quality of generated trace links.},
  doi       = {10.1109/ASE.2013.6693117},
  groups    = {AI},
  keywords  = {data mining;formal verification;health care;learning (artificial intelligence);medical computing;program diagnostics;query processing;relevance feedback;text analysis;effective query transformation learning;requirement trace retrieval enhancement process;automated requirements traceability;user feedback;trace query transformation;association rule mining;training sets;query optimization process;relevance feedback;source documents;target documents;WorldVista Healthcare system;certification requirements;Commission for Healthcare Information Technology;TQT technique;machine learning;text mining;software engineering activities;Training;Association rules;Itemsets;Medical services;Standards;Manuals;Educational institutions;requirements traceability;query replacement;contractual requirements;text mining;machine learning;association rules},
}

@InCollection{li2012-which-visualization-in-this-context,
  author    = {Yang Li and Walid Maalej},
  booktitle = {Requirements Engineering: Foundation for Software Quality},
  publisher = {Springer Berlin Heidelberg},
  title     = {Which Traceability Visualization Is Suitable in This Context? A Comparative Study},
  year      = {2012},
  pages     = {194--210},
  abstract  = {Traceability supports users in describing and tracking the relationships between software artifacts. Techniques such as traceability matrices and graphs visualize these relationships and help users to access and understand them. Researchers agree that different visualization techniques add valuable information in different contexts. However, there is an ambiguity which visualization is suitable for which context. To clarify this we conducted a comparative study of common visualization techniques, including an experiment and interviews with 24 participants.

We found that traceability matrices and graphs are most preferred in management tasks, while hyperlinks are preferred in implementation and testing tasks. Traceability lists seem to be the least attractive technique for most participants. Graphs are preferred to navigate linked artifacts, while matrices are appropriate for overview. Hyperlinks are regarded to fit for fine-grained information. Participants stressed the importance of visualizing semantics of artifacts and links. Our finding also indicates that users are not always able to choose the most suitable visualization.},
  doi       = {10.1007/978-3-642-28714-5_17},
  url       = {https://doi.org/10.1007/978-3-642-28714-5_17},
}

@Article{mader2012-visual-language-for-traceability-queries,
  author    = {Patrick M\"{a}der and Jane Cleland-Huang},
  journal   = {Software {\&} Systems Modeling},
  title     = {A visual language for modeling and executing traceability queries},
  year      = {2012},
  month     = apr,
  number    = {3},
  pages     = {537--553},
  volume    = {12},
  abstract  = {Current software and systems engineering tools provide only basic trace features, and as a result users are often compelled to construct non-trivial traceability queries using generic query languages such as SQL. In this paper, we present an alternative approach which defines traceability strategies for a project using UML class diagrams and then constructs trace queries as constraints upon subsets of the model. The visual trace modeling language (VTML) allows users to model a broad range of trace queries while hiding underlying technical details and data structures. The viability and expressiveness of VTML for use in a real project are demonstrated through modeling a broadly representative set of queries for a web-based health-care system. It is then evaluated through an experiment with human users to assess the readability and writability of VTML queries in comparison to generic SQL queries. We found that users read and constructed traceability queries considerably faster using VTML than using SQL. Furthermore, visually constructed traceability queries were substantially more correct compared to the same queries constructed with SQL.},
  doi       = {10.1007/s10270-012-0237-0},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007/s10270-012-0237-0},
}

@InProceedings{mader2009-motivation-matters-in-traceability-practitioner-survey,
  author    = {Patrick Mader and Orlena Gotel and Ilka Philippow},
  booktitle = {2009 17th IEEE International Requirements Engineering Conference},
  title     = {Motivation Matters in the Traceability Trenches},
  year      = {2009},
  month     = {Aug},
  pages     = {143-148},
  abstract  = {Reports from the field are few and far between when it comes to traceability. As a community, we know little more about the traceability practice in companies today than we did a decade ago. This paper reports on findings from a practitioner survey designed to get a high-level update on traceability practice and problems. What emerges is the importance of the prevailing motivation underlying traceability adoption in an organization and we characterize this in four ways. We use these perspectives to discuss our findings and their implications.},
  doi       = {10.1109/RE.2009.23},
  groups    = {meta},
  issn      = {2332-6441},
  keywords  = {formal specification;formal verification;human factors;software development management;systems analysis;traceability trench adoption;traceability practitioner survey;organizational motivation factor;requirements traceability management;Software systems;Computer science;USA Councils;Data analysis;Programming;Information technology;Documentation;requirements traceability;traceability practice;traceability problem;traceability process},
}

@Article{mader2013-strategic-traceability-for-safety-critical-projects,
  author   = {Patrick Mäder and Paul L. Jones and Yi Zhang and Jane Cleland-Huang},
  journal  = {IEEE Software},
  title    = {Strategic Traceability for Safety-Critical Projects},
  year     = {2013},
  issn     = {1937-4194},
  month    = {May},
  number   = {3},
  pages    = {58-66},
  volume   = {30},
  abstract = {To support any claim that a product is safe for its intended use, manufacturers must establish traceability within that product's development life cycle. Unfortunately, traceability information submitted to regulators and third parties is often weak, casting doubt rather than confidence in a product's integrity. This article evaluates traceability information for 10 submissions prepared by manufacturers for review at the US Food and Drug Administration. The authors observed nine widespread traceability problems that affected regulators' ability to evaluate the product's safety in a timely manner. To address these problems, the authors present a set of guidelines for implementing strategic traceability in a way that supports safety assessments.},
  doi      = {10.1109/MS.2013.60},
  keywords = {Safety;Software development;Software reliability;Software architecture;Product safety;requirements traceability;safety critical;assessment;traceability strategies;software and system safety;documentation},
}

@InProceedings{panis2010-req-traceability-deployment-in-commercial-engineering-organisation,
  author    = {Michael C. {Panis}},
  booktitle = {2010 18th IEEE International Requirements Engineering Conference},
  title     = {Successful Deployment of Requirements Traceability in a Commercial Engineering Organization...Really},
  year      = {2010},
  month     = {Sep.},
  pages     = {303-307},
  abstract  = {Within the world of requirements engineering, it seems a foregone conclusion that traceability is vital to the product development process. Simultaneously, it appears that any implementation of traceability is doomed to failure. This paper examines a commercial engineering company's use of traceability and the reasons why traceability is providing value despite the many challenges it presents. It describes the solution that was deployed and analyzes what has and has not succeeded, factors which should be common to any organization attempting to use requirements traceability.},
  doi       = {10.1109/RE.2010.43},
  issn      = {2332-6441},
  keywords  = {systems analysis;successful deployment;requirements traceability;commercial engineering organization;requirements engineering;foregone conclusion;product development process;Software;Instruments;Book reviews;Accuracy;Hardware;Conferences;Organizations;requirements;traceability},
}

@InProceedings{panichella2013-genetic-programming-for-effective-topic-modeling,
  author    = {Annibale Panichella  and Bogdan Dit  and Rocco Oliveto  and Massimilano Di Penta  and Denys Poshynanyk  and Andrea De Lucia },
  booktitle = {2013 35th International Conference on Software Engineering (ICSE)},
  title     = {How to effectively use topic models for software engineering tasks? An approach based on Genetic Algorithms},
  year      = {2013},
  month     = {May},
  pages     = {522-531},
  abstract  = {Information Retrieval (IR) methods, and in particular topic models, have recently been used to support essential software engineering (SE) tasks, by enabling software textual retrieval and analysis. In all these approaches, topic models have been used on software artifacts in a similar manner as they were used on natural language documents (e.g., using the same settings and parameters) because the underlying assumption was that source code and natural language documents are similar. However, applying topic models on software data using the same settings as for natural language text did not always produce the expected results. Recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text. Our paper builds on this new fundamental finding and proposes a novel solution to adapt, configure and effectively use a topic modeling technique, namely Latent Dirichlet Allocation (LDA), to achieve better (acceptable) performance across various SE tasks. Our paper introduces a novel solution called LDA-GA, which uses Genetic Algorithms (GA) to determine a near-optimal configuration for LDA in the context of three different SE tasks: (1) traceability link recovery, (2) feature location, and (3) software artifact labeling. The results of our empirical studies demonstrate that LDA-GA is able to identify robust LDA configurations, which lead to a higher accuracy on all the datasets for these SE tasks as compared to previously published results, heuristics, and the results of a combinatorial search.},
  doi       = {10.1109/ICSE.2013.6606598},
  issn      = {1558-1225},
  keywords  = {genetic algorithms;information retrieval;natural language processing;software engineering;text analysis;software engineering tasks;genetic algorithms;information retrieval methods;IR methods;SE tasks;software textual retrieval and analysis;natural language documents;source code;software data;natural language text;topic modeling technique;latent Dirichlet allocation;LDA-GA;near-optimal configuration;traceability link recovery;feature location;software artifact labeling;LDA configurations;Software;Software engineering;Natural languages;Genetic algorithms;Accuracy;Context;Labeling;Textual Analysis in Software Engineering;Latent Dirichlet Allocation;Genetic Algoritms},
}

@Article{spanoudakis2004-rule-based-generation-of-req-traceability-relations,
  author   = {George Spanoudakis and Andrea Zisman and Elena Pérez-Miñana and Paul Krause},
  journal  = {Journal of Systems and Software},
  title    = {Rule-based generation of requirements traceability relations},
  year     = {2004},
  issn     = {0164-1212},
  number   = {2},
  pages    = {105 - 127},
  volume   = {72},
  abstract = {The support for traceability between requirement specifications has been recognised as an important task in the development life cycle of software systems. In this paper, we present a rule-based approach to support the automatic generation of traceability relations between documents which specify requirement statements and use cases (expressed in structured forms of natural language), and analysis object models for software systems. The generation of such relations is based on traceability rules of two different types. More specifically, we use requirement-to-object-model rules to trace the requirements and use case specification documents to an analysis object model, and inter-requirements traceability rules to trace requirement and use case specification documents to each other. By deploying such rules, our approach can generate four different types of traceability relations. To implement and demonstrate our approach, we have implemented a traceability prototype system. This system assumes requirement and use case specification documents and analysis object models represented in XML. It also uses traceability rules which are represented in an XML-based rule mark-up language that we have developed for this purpose. This XML-based representation framework makes it easier to deploy our prototype in settings characterised by the use of heterogeneous software engineering and requirements management tools. The developed prototype has been used in a series of experiments that we have conducted to evaluate our approach. The results of these experiments have provided encouraging initial evidence about the plausibility of our approach and are discussed in the paper.},
  doi      = {https://doi.org/10.1016/S0164-1212(03)00242-5},
  groups   = {ws},
  keywords = {Requirement traceability, Natural language processing, Rule-based traceability reasoning},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121203002425},
}

@InProceedings{vonknethen2002-change-oriented-req-traceability-evolution-of-embedded-systems,
  author    = {A. {von Knethen}},
  booktitle = {International Conference on Software Maintenance, 2002. Proceedings.},
  title     = {Change-oriented requirements traceability. Support for evolution of embedded systems},
  year      = {2002},
  month     = {Oct},
  pages     = {482-485},
  abstract  = {Planning of requirements changes is often inaccurate and implementation of changes is time consuming and error prone. One reason for these problems is imprecise and inefficient approaches to analyze the impact of changes. This thesis proposes a precise and efficient impact analysis approach that focuses on functional system requirements changes of embedded control systems. It consists of three parts: (1) a fine-grained conceptual trace model, (2) process descriptions of how to establish traces and how to analyze the impact of changes, and (3) supporting tools. Empirical investigation shows that the approach has a beneficial effect on the effectiveness and efficiency of impact analyses and that it supports a more consistent implementation of changes.},
  doi       = {10.1109/ICSM.2002.1167808},
  issn      = {1063-6773},
  keywords  = {systems analysis;software maintenance;embedded systems;software tools;change-oriented requirements traceability;embedded systems evolution;requirements change planning;time consuming;impact analysis approach;functional system requirements changes;embedded control systems;fine-grained conceptual trace model;process descriptions;software tools;software maintenance;Embedded system;Software engineering;Control systems;Computer science;Computer errors;Software systems;System analysis and design;Costs;Software maintenance},
}

@InCollection{kokaly2017-safety-case-impact-assessment-model-based-automotive,
  author    = {Sahar Kokaly and Rick Salay and Marsha Chechik and Mark Lawford and Tom Maibaum},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {Safety Case Impact Assessment in Automotive Software Systems: An Improved Model-Based Approach},
  year      = {2017},
  pages     = {69--85},
  abstract  = {Like most systems, automotive software systems evolve due to many reasons including adding, removing or modifying features, fixing bugs, or improving system quality. In this context, safety cases, used to demonstrate that a system satisfies predefined safety requirements, often dictated by a standard such as ISO 26262, need to co-evolve. A necessary step is performing an impact assessment to identify how changes in the system affect the safety case. In previous work, we introduced a generic model-based impact assessment approach, that, while sound, was not particularly precise. In this work, we show how exploiting knowledge about system changes, the particular safety case language, and the standard can increase the precision of the impact assessment, reducing any unnecessary revision work required by a safety engineer. We present six precision improvement techniques illustrated on a GSN safety case used with ISO 26262.},
  comment   = {"Traceability" does not appear in title abstract, yet it is very tracea oriented.},
  doi       = {10.1007/978-3-319-66266-4_5},
  groups    = {MDE},
  url       = {https://doi.org/10.1007/978-3-319-66266-4_5},
}

@InProceedings{kuang2012-do-data-dependencies-complement-call-dependencies,
  author    = {Hongyu Kuang and Patrick Mäder and Hao Hu and Achraf Ghabi and LiGuo Huang and Lv Jian and Alexander Egyed},
  booktitle = {2012 28th IEEE International Conference on Software Maintenance (ICSM)},
  title     = {Do data dependencies in source code complement call dependencies for understanding requirements traceability?},
  year      = {2012},
  month     = sep,
  pages     = {181-190},
  abstract  = {It is common practice for requirements traceability research to consider method call dependencies within the source code (e.g., fan-in/fan-out analyses). However, current approaches largely ignore the role of data. The question this paper investigates is whether data dependencies have similar relationships to requirements as do call dependencies. For example, if two methods do not call one another, but do have access to the same data then is this information relevant? We formulated several research questions and validated them on three large software systems, covering about 120 KLOC. Our findings are that data relationships are roughly equally relevant to understanding the relationship to requirements traces than calling dependencies. However, most interestingly, our analyses show that data dependencies complement call dependencies. These findings have strong implications on all forms of code understanding, including trace capture, maintenance, and validation techniques (e.g., information retrieval).},
  doi       = {10.1109/ICSM.2012.6405270},
  issn      = {1063-6773},
  keywords  = {formal specification;formal verification;reverse engineering;data dependency;source code;call dependency;requirements traceability;software system;code understanding;trace capture technique;maintenance technique;validation technique;information retrieval;Java;Servers;Software systems;Conferences;Software maintenance;Video on demand;Motion pictures;requirements traceability;feature location;source code dependencies;program analysis;method call dependencies;method data dependencies},
}

@Article{wallach2019-internationale-governance-robotics-ai,
  author   = {Wendell Wallach and Gary Marchant},
  journal  = {Proceedings of the IEEE},
  title    = {Toward the Agile and Comprehensive International Governance of AI and Robotics [point of view]},
  year     = {2019},
  issn     = {1558-2256},
  month    = {March},
  number   = {3},
  pages    = {505-508},
  volume   = {107},
  abstract = {Rapidly emerging technologies, such as AI and robotics, present a serious challenge to traditional models of government regulation. These technologies are advancing so quickly that in many sectors, traditional regulation cannot keep up, given the cumbersome procedural and bureaucratic procedures and safeguards that modern legislative and rulemaking processes require. Consequently, regulatory systems will predictively fail to put in place appropriately tailored regulatory measures by the time new applications of fast-moving technologies begin to affect society. Perhaps even worse, if a regulatory system does somehow manage to rush into place new regulations for an emerging technology, they will likely be obsolete by the time the ink dries on the enactment. Given this so-called “pacing problem,” traditional regulatory approaches will either produce no regulation or bad regulation [1].},
  comment  = {4 pages on AI in robotics related to soft governance, not a single mention of trac[e|ability].},
  doi      = {10.1109/JPROC.2019.2899422},
  groups   = {AI in SE},
  keywords = {artificial intelligence;government policies;legislation;robots;robotics;government regulation;bureaucratic procedures;regulatory system;regulatory measures;international governance;rulemaking processes;legislative processes;artificial intelligence;Artificial intelligence;Robots;Government;Ethics;Law;Social implications of technology},
}

@Article{vercauteren2020-computerized-intelligence-in-computer-assisted-intervention-medical,
  author   = {Tom Vercauteren and Mathias Unberath and Nicolas Padoy and Nassir Navab},
  journal  = {Proceedings of the IEEE},
  title    = {CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer-Assisted Interventions},
  year     = {2020},
  issn     = {1558-2256},
  month    = {Jan},
  number   = {1},
  pages    = {198-214},
  volume   = {108},
  abstract = {Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human-AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.},
  doi      = {10.1109/JPROC.2019.2946993},
  keywords = {decision making;learning (artificial intelligence);medical computing;surgery;user interfaces;CAI4CAI;data-driven computational approaches;medical images;interventional imaging;interventional suites;interventional systems;intraoperative devices;contextual artificial intelligence for computer-assisted intervention;mixed human-AI actor team;Surgery;Planning;Machine learning;Artificial intelligence;Image segmentation;Biomedical imaging;Artificial intelligence;computer-assisted interventions;context-aware user interface;data fusion;interventional workflow;intraoperative imaging;machine and deep learning;surgical data science;surgical planning;surgical scene understanding},
}

@InProceedings{yu2012-maintainging-invariant-traceability,
  author    = {Yijun Yu and Yu Lin and Zhenjiang Hu and Soichiro Hidaka and Hiroyuki Kato and Lionel Montrieux},
  booktitle = {2012 34th International Conference on Software Engineering (ICSE)},
  title     = {Maintaining invariant traceability through bidirectional transformations},
  year      = {2012},
  month     = {June},
  pages     = {540-550},
  abstract  = {Following the “convention over configuration” paradigm, model-driven development (MDD) generates code to implement the “default” behaviour that has been specified by a template separate from the input model, reducing the decision effort of developers. For flexibility, users of MDD are allowed to customise the model and the generated code in parallel. A synchronisation of changed model or code is maintained by reflecting them on the other end of the code generation, as long as the traceability is unchanged. However, such invariant traceability between corresponding model and code elements can be violated either when (a) users of MDD protect custom changes from the generated code, or when (b) developers of MDD change the template for generating the default behaviour. A mismatch between user and template code is inevitable as they evolve for their own purposes. In this paper, we propose a two-layered invariant traceability framework that reduces the number of mismatches through bidirectional transformations. On top of existing vertical (model↔code) synchronisations between a model and the template code, a horizontal (code↔code) synchronisation between user and template code is supported, aligning the changes in both directions. Our blinkit tool is evaluated using the data set available from the CVS repositories of a MDD project: Eclipse MDT/GMF.},
  doi       = {10.1109/ICSE.2012.6227162},
  issn      = {1558-1225},
  keywords  = {program compilers;program diagnostics;software maintenance;two-layered invariant traceability maintenance framework;bidirectional transformations;convention over configuration paradigm;model-driven development;MDD;code generation;default behaviour generation;code elements;user code;template code;vertical synchronisations;horizontal synchronisation;blinkit tool;CVS repositories;Eclipse MDT-GMF;Synchronization;Java;Computational modeling;Generators;Adaptation models;Educational institutions;Prototypes},
}

@Article{stol2018-ABC-of-SE-Reasearch,
  author     = {Stol, Klaas-Jan and Fitzgerald, Brian},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  title      = {The ABC of Software Engineering Research},
  year       = {2018},
  issn       = {1049-331X},
  month      = sep,
  number     = {3},
  volume     = {27},
  abstract   = {A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research.},
  address    = {New York, NY, USA},
  articleno  = {11},
  doi        = {10.1145/3241743},
  issue_date = {October 2018},
  keywords   = {research strategy, Research methodology},
  numpages   = {51},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3241743},
}

@InProceedings{galvao2007-survey-traceability-in-MDE,
  author    = {Ismenia Galvao and Arda Goknil},
  booktitle = {11th IEEE International Enterprise Distributed Object Computing Conference (EDOC 2007)},
  title     = {Survey of Traceability Approaches in Model-Driven Engineering},
  year      = {2007},
  month     = {Oct},
  pages     = {313-313},
  abstract  = {Models have been used in various engineering fields to help managing complexity and represent information in different abstraction levels, according to specific notations and stakeholder's viewpoints. Model-Driven Engineering (MDE) gives the basic principles for the use of models as primary artefacts throughout the software development phases and presents characteristics that simplify the engineering of software in various domains, such as Enterprise Computing Systems. Hence, for its successful application, MDE processes must consider traceability practices. They help the understanding, capturing, tracking and verification of software artefacts and their relationships and dependencies with other artefacts during the software life-cycle. In this survey, we discuss the state-of-the-art in traceability approaches in MDE and assess them with respect to five general comparison criteria: representation, mapping, scalability, change impact analysis and tool support. As a complementary result, we have identified some open issues that can be better explored by traceability in MDE.},
  doi       = {10.1109/EDOC.2007.42},
  file      = {:galvao2007-survey-traceability-in-MDE.pdf:PDF},
  groups    = {meta},
  issn      = {1541-7719},
  keywords  = {computational complexity;software engineering;traceability approaches;model-driven engineering;complexity management;abstraction levels;software development;software engineering;enterprise computing systems;software life-cycle;impact analysis;Model driven engineering;Programming;Engineering management;Scalability;Software engineering;Distributed computing;Computer science;Application software;Reverse engineering;Software systems},
}

@InProceedings{naslavsky2007-traceability-of-MB-Testing-MT,
  author    = {Naslavsky, Leila and Ziv, Hadar and Richardson, Debra J.},
  booktitle = {Proceedings of the 3rd International Workshop on Advances in Model-Based Testing},
  title     = {Towards Traceability of Model-Based Testing Artifacts},
  year      = {2007},
  address   = {New York, NY, USA},
  pages     = {105–114},
  publisher = {Association for Computing Machinery},
  series    = {A-MOST ’07},
  abstract  = {Practitioners regard software testing as the central means for ensuring that a system behaves as expected. Due to the recent widespread adoption of model-driven development (MDD), code is no longer the single source for selecting test cases. Testing against original expectations can be done with model-based testing that adopts high-level models as the basis for test generation. In addition to test generation, challenges to model-based testing include creation and maintenance of traceability information among test-related artifacts. Traceability is required to support activities such as result evaluation, regression testing and coverage analysis. MDD and model transformation solutions address the traceability problem by creating relationships among transformed artifacts throughout the transformation process. This paper proposes an approach that leverages model transformation traceability techniques to create fine-grained relationships among model-based testing artifacts. Relationships are created during the test generation process. Their fine granularity enables the support for result evaluation, coverage analysis and regression testing.},
  doi       = {10.1145/1291535.1291546},
  isbn      = {9781595938503},
  keywords  = {model-driven development, model-based testing, traceability},
  location  = {London, United Kingdom},
  numpages  = {10},
  url       = {https://doi.org/10.1145/1291535.1291546},
}

@InProceedings{arunthavanathan2016-traceability-with-NLP,
  author    = {A. {Arunthavanathan} and S. {Shanmugathasan} and S. {Ratnavel} and V. {Thiyagarajah} and I. {Perera} and D. {Meedeniya} and D. {Balasubramaniam}},
  booktitle = {2016 Moratuwa Engineering Research Conference (MERCon)},
  title     = {Support for traceability management of software artefacts using Natural Language Processing},
  year      = {2016},
  month     = {April},
  pages     = {18-23},
  abstract  = {One of the major problems in software development process is managing software artefacts. While software evolves, inconsistencies between the artefacts do evolve as well. To resolve the inconsistencies in change management, a tool named “Software Artefacts Traceability Analyzer (SAT-Analyzer)” was introduced as the previous work of this research. Changes in software artefacts in requirement specification, Unified Modelling Language (UML) diagrams and source codes can be tracked with the help of Natural Language Processing (NLP) by creating a structured format of those documents. Therefore, in this research we aim at adding an NLP support as an extension to SAT-Analyzer. Enhancing the traceability links created in the SAT-analyzer tool is another focus due to artefact inconsistencies. This paper includes the research methodology and relevant research carried out in applying NLP for improved traceability management. Tool evaluation with multiple scenarios resulted in average Precision 72.22%, Recall 88.89% and F1 measure of 78.89% suggesting high accuracy for the domain.},
  doi       = {10.1109/MERCon.2016.7480109},
  groups    = {ws},
  keywords  = {natural language processing;program diagnostics;software engineering;Unified Modeling Language;traceability management;natural language processing;software development process;change management;software artefacts traceability analyzer;Unified Modelling Language;UML;NLP;SAT-analyzer;Software;Natural language processing;Unified modeling language;Maintenance engineering;XML;Data mining;Natural Language Processing;Artefacts;Traceability Links;Traceability Visualization;Taxonomy},
}

@InProceedings{drivalos2010-state-based-traceability,
  author    = {Drivalos-Matragkas, Nikolaos and Kolovos, Dimitrios S. and Paige, Richard F. and Fernandes, Kiran J.},
  booktitle = {Proceedings of the 6th ECMFA Traceability Workshop},
  title     = {A State-Based Approach to Traceability Maintenance},
  year      = {2010},
  address   = {New York, NY, USA},
  pages     = {23–30},
  publisher = {Association for Computing Machinery},
  series    = {ECMFA-TW ’10},
  abstract  = {Traceability of software artefacts has been recognized as an important factor for supporting various software development activities. However, establishing traceability requires a substantial investment in effort. Even when an initial set of traceability links has been established, this set is subject to gradual degradation as the associated artefacts are modified, e.g., due to the evolutionary nature of software development. To avoid this, traceability must be constantly maintained and evolved. The manual maintenance of traceability can be time consuming and error-prone. This paper focuses on reducing the manual effort incurred in performing traceability maintenance tasks. This is achieved by introducing a dedicated mechanism in the Traceability Metamodelling Language, which is used for detecting and evolving problematic trace links. A concrete example is used to demonstrate the practicality and usefulness of our approach.},
  doi       = {10.1145/1814392.1814396},
  groups    = {ws},
  isbn      = {9781605589930},
  keywords  = {traceability, model driven engineering, evolution},
  location  = {Paris, France},
  numpages  = {8},
  url       = {https://doi.org/10.1145/1814392.1814396},
}

@Article{ozkaya2020-differences-in-AI-enabled-engineering,
  author    = {Ipek Ozkaya},
  journal   = {IEEE Software},
  title     = {What Is Really Different in Engineering AI-Enabled Systems?},
  year      = {2020},
  issn      = {1937-4194},
  month     = {jul},
  number    = {04},
  pages     = {3-6},
  volume    = {37},
  abstract  = {Advances in machine learning (ML) algorithms and increasing availability of computational power have resulted in huge investments in systems that aspire to exploit artificial intelligence (AI), in particular ML. AIenabled systems, software-reliant systems that include data and components that implement algorithms mimicking learning and problem solving, have inherently different characteristics than software systems alone.<sup>1</sup> However, the development and sustainment of such systems also have many parallels with building, deploying, and sustaining software systems. A common observation is that although software systems are deterministic and you can build and test to a specification, AI-enabled systems, in particular those that include ML components, are generally probabilistic. Systems with ML components can have a high margin of error due to the uncertainty that often follows predictive algorithms. The margin of error can be related to the inability to predict the result in advance or the same result cannot be reproduced. This characteristic makes AI-enabled systems hard to test and verify.<sup>2</sup> Consequently, it is easy to assume that what we know about designing and reasoning about software systems does not immediately apply in AI engineering. AI-enabled systems are software systems. The sneaky part about engineering AI systems is they are "just like" conventional software systems we can design and reason about until they?re not.},
  address   = {Los Alamitos, CA, USA},
  doi       = {10.1109/MS.2020.2993662},
  groups    = {AI in SE},
  publisher = {IEEE Computer Society},
}

@InBook{seibel2012-efficient-traceability-for-MDE,
  author    = {Seibel, Andreas and Hebig, Regina and Giese, Holger},
  editor    = {Cleland-Huang, Jane and Gotel, Orlena and Zisman, Andrea},
  pages     = {215--240},
  publisher = {Springer London},
  title     = {Traceability in Model-Driven Engineering: Efficient and Scalable Traceability Maintenance},
  year      = {2012},
  address   = {London},
  isbn      = {978-1-4471-2239-5},
  abstract  = {Model-Driven Engineering (MDE) employs models and model transformations as first-class citizens throughout the whole software development life cycle. Support for automated traceability is necessary because models in MDE usually have inherent dependencies between each other, which must be visible. Furthermore, software evolves which implies to also maintain traceability. In this chapter, we present an efficient and scalable traceability maintenance approach. It uses formal rules to specify conditions for maintaining traceability links. We also show the constitution of our rules and how we improved them, in comparison to a previous approach. Based on this formalism, we present two maintenance strategies. We show an initial (batch) strategy that is applied in case that no change information is available. The second strategy is incremental and therefore scalable. The incremental strategy is applied when change information is available. We explain our approach and evaluate the efficiency and scalability of our approach by means of the mobile phone product line case study presented in this book.},
  booktitle = {Software and Systems Traceability},
  doi       = {10.1007/978-1-4471-2239-5_10},
  url       = {https://doi.org/10.1007/978-1-4471-2239-5_10},
}

@InProceedings{gotel1994,
  author    = {O. C. Z. {Gotel} and C. W. {Finkelstein}},
  booktitle = {Proceedings of IEEE International Conference on Requirements Engineering},
  title     = {An analysis of the requirements traceability problem},
  year      = {1994},
  month     = {April},
  pages     = {94-101},
  abstract  = {Investigates and discusses the underlying nature of the requirements traceability problem. Our work is based on empirical studies, involving over 100 practitioners, and an evaluation of current support. We introduce the distinction between pre-requirements specification (pre-RS) traceability and post-requirements specification (post-RS) traceability to demonstrate why an all-encompassing solution to the problem is unlikely, and to provide a framework through which to understand its multifaceted nature. We report how the majority of the problems attributed to poor requirements traceability are due to inadequate pre-RS traceability and show the fundamental need for improvements. We present an analysis of the main barriers confronting such improvements in practice, identify relevant areas in which advances have been (or can be) made, and make recommendations for research.<>},
  doi       = {10.1109/ICRE.1994.292398},
  keywords  = {systems analysis;requirements traceability problem analysis;pre-requirements specification traceability;post-requirements specification traceability;requirements engineering practice;requirements traceability tools;Educational institutions;Guidelines;Project management;Research and development},
}

@Article{wohlrab2020-traceability-organization-process-culture,
  author   = {Wohlrab, Rebekka and Knauss, Eric and Steghöfer, Jan-Philipp and Maro, Salome and Anjorin, Anthony and Pelliccione, Patrizio},
  journal  = {Requirements Engineering},
  title    = {Collaborative traceability management: a multiple case study from the perspectives of organization, process, and culture},
  year     = {2020},
  issn     = {1432-010X},
  number   = {1},
  pages    = {21--45},
  volume   = {25},
  abstract = {Traceability is crucial for many activities in software and systems engineering including monitoring the development progress, and proving compliance with standards. In practice, the use and maintenance of trace links are challenging as artifacts undergo constant change, and development takes place in distributed scenarios with multiple collaborating stakeholders. Although traceability management in general has been addressed in previous studies, there is a need for empirical insights into the collaborative aspects of traceability management and how it is situated in existing development contexts. The study reported in this paper aims to close this gap by investigating the relation of collaboration and traceability management, based on an understanding of characteristics of the development effort. In our multiple exploratory case study, we conducted semi-structured interviews with 24 individuals from 15 industrial projects. We explored which challenges arise, how traceability management can support collaboration, how collaboration relates to traceability management approaches, and what characteristics of the development effort influence traceability management and collaboration. We found that practitioners struggle with the following challenges: (1) collaboration across team and tool boundaries, (2) conveying the benefits of traceability, and (3) traceability maintenance. If these challenges are addressed, we found that traceability can facilitate communication and knowledge management in distributed contexts. Moreover, there exist multiple approaches to traceability management with diverse collaboration approaches, i.e., requirements-centered, developer-driven, and mixed approaches. While traceability can be leveraged in software development with both agile and plan-driven paradigms, a certain level of rigor is needed to realize its benefits and overcome challenges. To support practitioners, we provide principles of collaborative traceability management. The main contribution of this paper is empirical evidence of how culture, processes, and organization impact traceability management and collaboration, and principles to support practitioners with collaborative traceability management. We show that collaboration and traceability management have the potential to be mutually beneficial--when investing in one, also the other one is positively affected.},
  refid    = {Wohlrab2020},
  url      = {https://doi.org/10.1007/s00766-018-0306-1},
}

@InCollection{meinicke2017-feature-traceability,
  author    = {Jens Meinicke and Thomas Th\"{u}m and Reimar Schr\"{o}ter and Fabian Benduhn and Thomas Leich and Gunter Saake},
  booktitle = {Mastering Software Variability with {FeatureIDE}},
  publisher = {Springer International Publishing},
  title     = {Feature Traceability for Feature-Oriented Programming},
  year      = {2017},
  pages     = {173--181},
  abstract  = {Feature traceability refers to the ability to locate features in software artifacts. Traceability helps developers to identify relevant artifacts during development and maintenance. Feature-oriented programming already establishes a one-to-one mapping between features and artifacts, whereas conditional compilation comes with a many-to-many mapping. While all artifacts belonging to a feature are contained in a single feature module, the inherent complexity of variable software poses challenges for developers. The interaction of a feature module with other feature modules can easily lead to unwanted feature interactions. Thus, developers need support to understand which other feature modules are also relevant for their intended changes. In addition, debugging feature-oriented programs is typically challenging, as during the composition of feature modules the mapping disappears.},
  doi       = {10.1007/978-3-319-61443-4_15},
  url       = {https://doi.org/10.1007/978-3-319-61443-4_15},
}

@InProceedings{mader2007-tracing-unified-process,
  author    = {Patrick Mader and Ilka Philippow and Matthias Riebisch},
  booktitle = {Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)},
  title     = {A Traceability Link Model for the Unified Process},
  year      = {2007},
  month     = {July},
  pages     = {700-705},
  volume    = {3},
  abstract  = {Traceability links are widely accepted as efficient means to support an evolutionary software development. However, their usage in analysis and design is effort consuming and error prone due to lacking or missing methods and tools for their creation, update and verification. In this paper we analyse and classify Unified Process artefacts to establish a traceability link model for this process. This model defines all required links between the artefacts. Furthermore, it provides a basis for the (semi)-automatic establishment and the verification of links in Unified Process development projects. We also define a first set of rules as step towards an efficient management of the links. In the ongoing project the rule set is extended to establish a whole framework of methods and rules.},
  doi       = {10.1109/SNPD.2007.342},
  keywords  = {formal verification;software process improvement;traceability link model;evolutionary software development;link verification;unified process development projects;Design methodology;Programming;Software engineering;Artificial intelligence;Distributed computing;Error correction;Software systems;Standards development;Design engineering;Performance analysis},
}

@Article{ieeeglossary-se,
  author   = { {Institute of Electrical and Electronics Engineers (IEEE)} },
  journal  = {IEEE Std 610.12-1990},
  title    = {IEEE Standard Glossary of Software Engineering Terminology},
  year     = {1990},
  month    = {Dec},
  pages    = {1-84},
  abstract = {This IEEE Standards product is part of the family on Software Engineering. This standard identifies terms currently in use in the field of Software Engineering. Standard definitions for those terms are established.},
  doi      = {10.1109/IEEESTD.1990.101064},
  keywords = {glossaries;software engineering;standards;IEEE Std 610.12-1990;standard glossary;software engineering terminology;Terminology;Software engineering;Standards;glossary;terminology;dictionary;Software engineering;Definitions},
}

@Article{ieeeglossary-req,
  author   = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  journal  = {IEEE Std 830-1984},
  title    = {IEEE Guide for Software Requirements Specifications},
  year     = {1984},
  month    = {Feb},
  pages    = {1-26},
  abstract = {The content and qualities of a good software requirements specification (SRS) are described and several sample SRS outlines are presented. This recommended practice is aimed at specifying requirements of software to be developed but also can be applied to assist in the selection of in-house and commercial software products.},
  doi      = {10.1109/IEEESTD.1984.119205},
  keywords = {software engineering;systems analysis;software requirements specification;SRS;formal requirements specifications languages;input/output specifications;SRS prototype outline;Software engineering;System analysis and design;software;requirements;specifications},
}

@Book{swebok2014,
  editor    = {Bourque, Pierre and Fairley, Richard E.},
  publisher = {IEEE Computer Society},
  title     = {{SWEBOK}: Guide to the Software Engineering Body of Knowledge},
  year      = {2014},
  address   = {Los Alamitos, CA},
  edition   = {Version 3.0},
  isbn      = {978-0-7695-5166-1},
  abstract  = {The purpose of the Guide to the Software Engineering Body of Knowledge is to provide a validated classification of the bounds of the software engineering discipline and topical access that will support this discipline. The Body of Knowledge is subdivided into a set of software engineering Knowledge Areas (KA) that differentiate among the various important concepts, allowing readers to find their way quickly to subjects of interest. Upon finding a subject, readers are referred to key papers or book chapters. SWEBOK Guide V3.0 builds upon SWEBOK 2004 to refresh and add new reviewed content. Emphases on engineering practice lead the Guide toward a strong relationship with the normative literature. The normative literature is validated by consensus formed among practitioners and is concentrated in standards and related documents. The two major standards bodies for software engineering (IEEE Computer Society Software and Systems Engineering Standards Committee and ISO/IEC JTC1/SC7) are represented in the project. The Guide is oriented toward a variety of audiences, all over the world. It aims to serve public and private organizations in need of a consistent view of software engineering for defining education and training requirements, classifying jobs, developing performance evaluation policies or specifying software development tasks. It also addresses practicing, or managing, software engineers and the officials responsible for making public policy regarding licensing and professional guidelines. In addition, professional societies and educators defining the certification rules, accreditation policies for university curricula, and guidelines for professional practice will benefit from the SWEBOK Guide, as well as the students learning the software engineering profession and educators and trainers engaged in defining curricula and course content. It is hoped that readers will find this book useful in guiding them toward the knowledge and resources they need in their lifelong career development as software engineering professionals.},
  file      = {IEEE Digital Library:2014/SWEBOK2014.pdf:PDF},
  keywords  = {01841 103 ieee book software engineering guide},
  url       = {http://www.swebok.org/},
}

@InCollection{HendersonSellers2008,
  author    = {Brian Henderson-Sellers and Cesar Gonzalez-Perez},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer Berlin Heidelberg},
  title     = {Standardizing Methodology Metamodelling and Notation: An {ISO} Exemplar},
  year      = {2008},
  pages     = {1--12},
  doi       = {10.1007/978-3-540-78942-0_1},
  url       = {https://doi.org/10.1007/978-3-540-78942-0_1},
}

@InCollection{henderson2008-study-of-iso24744-2007,
  author    = {Brian Henderson-Sellers and Cesar Gonzalez-Perez},
  booktitle = {Lecture Notes in Business Information Processing},
  publisher = {Springer Berlin Heidelberg},
  title     = {Standardizing Methodology Metamodelling and Notation: An {ISO} Exemplar},
  year      = {2008},
  pages     = {1--12},
  doi       = {10.1007/978-3-540-78942-0_1},
  url       = {https://doi.org/10.1007/978-3-540-78942-0_1},
}

@Misc{sotovalero2020tracebased,
  author        = {César Soto-Valero and Thomas Durieux and Nicolas Harrand and Benoit Baudry},
  title         = {Trace-based Debloat for Java Bytecode},
  year          = {2020},
  abstract      = {Software bloat is code that is packaged in an application but is actually not used and not necessary to run the application. The presence of bloat is an issue for software security, for performance, and for maintenance. In recent years, several works have proposed techniques to detect and remove software bloat. In this paper, we introduce a novel technique to debloat Java bytecode through dynamic analysis, which we call trace-based debloat. We have developed JDBL, a tool that automates the collection of accurate execution traces and the debloating process. Given a Java project and a workload, JDBL generates a debloated version of the project that is syntactically correct and preserves the original behavior, modulo the workload. We evaluate the feasibility and the effectiveness of trace-based debloat with 395 open-source Java libraries for a total 10M+ lines of code. We demonstrate that our approach significantly reduces the size of these libraries while preserving the functionalities needed by their clients.},
  archiveprefix = {arXiv},
  eprint        = {2008.08401},
  primaryclass  = {cs.SE},
}

@InCollection{ziegenhagen2020-expanding-tracea-with-dynamic-tracing-data,
  author    = {Dennis Ziegenhagen and Andreas Speck and Elke Pulvermueller},
  booktitle = {Communications in Computer and Information Science},
  publisher = {Springer International Publishing},
  title     = {Expanding Tracing Capabilities Using Dynamic Tracing Data},
  year      = {2020},
  pages     = {319--340},
  abstract  = {Software traceability enables gaining insight into artifact relationships and dependencies throughout software development. This information can be used to support project maintenance and to reduce costs, e.g. by estimating the impact of artifact changes. Many traceability applications require manual effort for creating and managing the necessary data. Current approaches aim at reducing this effort by automating various involved tasks. To support this, we propose an enrichment of tracing data by capturing interactions that influence the artifacts’ life-cycle, which we refer to as dynamic tracing data. Its purpose is to expand capabilities of traceability applications and to enable assistance in development tasks. In this paper, we present our research methodology and current results, most importantly a flexible and modular framework for capturing and using dynamic tracing data, as well as an example scenario to demonstrate a possible implementation and usage of the framework.},
  doi       = {10.1007/978-3-030-40223-5_16},
  url       = {https://doi.org/10.1007/978-3-030-40223-5_16},
}

@InCollection{speck2020-inteligent-solution-industrial-prerequisite,
  author    = {Andreas Speck and Melanie Windrich and Elke Pulverm\"{u}ller and Dennis Ziegenhagen and Timo Wilgen},
  booktitle = {Human Centred Intelligent Systems},
  publisher = {Springer Singapore},
  title     = {An Industrial Production Scenario as Prerequisite for Applying Intelligent Solutions},
  year      = {2020},
  month     = may,
  pages     = {161--172},
  abstract  = {Besides many initiatives for making manufacturing and production more intelligent, the processes in the industrial productions are still mostly traditional. The paper presents a scenario providing prerequisites for applying intelligent solutions. The lean universal control concept presented in the paper supports almost any kind of automation devices such as robot arms, PLC, and numerical control machine tools. Due to its open architecture, the universal control system serves as a base of intelligent solutions supporting the production. A graphical workflow notation for modeling the control programs is combined with automated checking which helps the human developers identifying and preventing rule violations already in the design process of the control application programs. In a further step, the control programs can first be tested with simulated devices. The visualization blends simulated and real devices controlled by the control application programs. This fosters the human users to monitor and validate the behavior of the devices. It supports a save commissioning of real devices.},
  doi       = {10.1007/978-981-15-5784-2_13},
  url       = {https://doi.org/10.1007/978-981-15-5784-2_13},
}

@Conference{ziegenhagen2019-developer-tool-interaction,
  author       = {Dennis Ziegenhagen. and Andreas Speck. and Elke Pulvermüller.},
  booktitle    = {Proceedings of the 14th International Conference on Evaluation of Novel Approaches to Software Engineering - Volume 1: ENASE,},
  title        = {Using Developer-tool-Interactions to Expand Tracing Capabilities},
  year         = {2019},
  organization = {INSTICC},
  pages        = {518-525},
  publisher    = {SciTePress},
  abstract     = {Expanding current software traceability methodologies offers opportunities to significantly support development activities. State-of-the-art traceability frameworks use tracing data at specific points in time. This data includes information about development artefacts and their relations, which may be used for analysis, visualisation and similar purposes. In between those points in time, developers create, modify or delete requirements, diagrams, source code and other relevant artefacts. We propose to capture such artefact interactions in order to enrich the tracing data. By applying existing approaches in the field of developer-tool interaction analysis to the enriched data, we aim at supporting the developer’s work. In this paper, we present the overall approach, along with our development of a modular framework which may be used to capture the desired data from various tools, manage it and finally enable the execution of developer-interaction analyses.},
  doi          = {10.5220/0007762905180525},
  isbn         = {978-989-758-375-9},
}

@Misc{sculley2018-winners-curse-progress-empirical-rigor,
  author   = {D. Sculley and Jasper Snoek and Alex Wiltschko and Ali Rahimi},
  title    = {Winner's Curse? On Pace, Progress, and Empirical Rigor},
  year     = {2018},
  abstract = {The field of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result.  In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole.  This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
  url      = {https://openreview.net/forum?id=rJWF0Fywf},
}

@TechReport{SPICE-ISO15504,
  author      = {{International Organization for Standardization}},
  institution = {{International Organization for Standardization}},
  title       = {{ISO/IEC 15504:2004 Information technology -- Process assessment -- Part 4: Guidance on use for process improvement and process capability determination}},
  year        = {2003},
  biburl      = {https://www.bibsonomy.org/bibtex/290ef972af947676a3977866f74fe45ea/caduta},
}

@InProceedings{ko2008-whyline-debugging,
  author    = {Ko, Andrew J. and Myers, Brad A.},
  booktitle = {Proceedings of the 30th International Conference on Software Engineering},
  title     = {Debugging Reinvented: Asking and Answering Why and Why Not Questions about Program Behavior},
  year      = {2008},
  address   = {New York, NY, USA},
  pages     = {301–310},
  publisher = {Association for Computing Machinery},
  series    = {ICSE '08},
  abstract  = {When software developers want to understand the reason for a program's behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of why did and why didn't questions derived from the program's code and execution. The tool then finds one or more possible explanations for the output in question, using a combination of static and dynamic slicing, precise call graphs, and new algorithms for determining potential sources of values and explanations for why a line of code was not reached. Evaluations of the tool on one task showed that novice programmers with the Whyline were twice as fast as expert programmers without it. The tool has the potential to simplify debugging in many software development contexts.},
  doi       = {10.1145/1368088.1368130},
  isbn      = {9781605580791},
  keywords  = {whyline},
  location  = {Leipzig, Germany},
  numpages  = {10},
  url       = {https://doi.org/10.1145/1368088.1368130},
}

@InProceedings{lorenzoli2008-automatic-generation-of-software-behavioral-model,
  author    = {Lorenzoli, Davide and Mariani, Leonardo and Pezz\`{e}, Mauro},
  booktitle = {Proceedings of the 30th International Conference on Software Engineering},
  title     = {Automatic Generation of Software Behavioral Models},
  year      = {2008},
  address   = {New York, NY, USA},
  pages     = {501–510},
  publisher = {Association for Computing Machinery},
  series    = {ICSE '08},
  abstract  = {Dynamic analysis of software systems produces behavioral models that are useful for analysis, verification and testing.The main techniques for extracting models of functional behavior generate either models of constraints on data, usually in the form of Boolean expressions, or models of interactions between components, usually in the form of finite state machines. Both data and interaction models are useful for analyzing and verifying different aspects of software behavior, but none of them captures the complex interplay between data values and components interactions. Thus related analysis and testing techniques can miss important information.In this paper, we focus on the generation of models of relations between data values and component interactions, and we present GK-tail, a technique to automatically generate extended finite state machines (EFSMs) from interaction traces. EFSMs model the interplay between data values and component interactions by annotating FSM edges with conditions on data values. We show that EFSMs include details that are not captured by either Boolean expressions or (classic) FSM alone, and allow for more accurate analysis and verification than separate models, even if considered jointly.},
  doi       = {10.1145/1368088.1368157},
  isbn      = {9781605580791},
  keywords  = {model synthesis, dynamic analysis, gk-tail},
  location  = {Leipzig, Germany},
  numpages  = {10},
  url       = {https://doi.org/10.1145/1368088.1368157},
}

@InProceedings{sculley2015-hidden-tefchnical-debt-in-machine-learning,
  author    = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
  title     = {Hidden Technical Debt in Machine Learning Systems},
  year      = {2015},
  address   = {Cambridge, MA, USA},
  pages     = {2503–2511},
  publisher = {MIT Press},
  series    = {NIPS'15},
  abstract  = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
  location  = {Montreal, Canada},
  numpages  = {9},
}

@Misc{iso26262,
  author    = {ISO},
  title     = {{Road vehicles -- Functional safety}},
  year      = {{2011}},
  key       = {{ISO 26262}},
  keywords  = {26262 iso},
  number    = {{ISO 26262}},
  publisher = {{ISO, Geneva, Switzerland}},
  type      = {{Norm}},
}

@Article{moy2013-DO-178C-testing,
  author      = {Moy, Yannick and Ledinot, Emmanuel and Delseny, Hervé and Wiels, Virginie and Monate, Benjamin},
  journal     = {IEEE Software},
  title       = {Testing or Formal Verification: DO-178C Alternatives and Industrial Experience},
  year        = {2013},
  issn        = {0740-7459},
  number      = {3},
  pages       = {50-57},
  volume      = {30},
  address     = {Los Alamitos, CA, USA},
  description = {Testing or Formal Verification: DO-178C Alternatives and Industrial Experience},
  doi         = {http://doi.ieeecomputersociety.org/10.1109/MS.2013.43},
  keywords    = {DO-178C Formal Testing Verification},
  publisher   = {IEEE Computer Society},
}

@Misc{hllermeier2019-aleatoric-and-epistemic-uncertiainty-in-ml,
  author        = {Eyke Hüllermeier and Willem Waegeman},
  title         = {Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1910.09457},
  primaryclass  = {cs.LG},
}

@Article{watts2017-incoherency-problem,
  author   = {Watts, Duncan J.},
  journal  = {Nature Human Behaviour},
  title    = {Should social science be more solution-oriented?},
  year     = {2017},
  issn     = {2397-3374},
  number   = {1},
  pages    = {0015},
  volume   = {1},
  abstract = {Over the past 100 years, social science has generated a tremendous number of theories on the topics of individual and collective human behaviour. However, it has been much less successful at reconciling the innumerable inconsistencies and contradictions among these competing explanations, a situation that has not been resolved by recent advances in ‘computational social science’. In this Perspective, I argue that this ‘incoherency problem’ has been perpetuated by an historical emphasis in social science on the advancement of theories over the solution of practical problems. I argue that one way for social science to make progress is to adopt a more solution-oriented approach, starting first with a practical problem and then asking what theories (and methods) must be brought to bear to solve it. Finally, I conclude with a few suggestions regarding the sort of problems on which progress might be made and how we might organize ourselves to solve them.},
  refid    = {Watts2017},
  url      = {https://doi.org/10.1038/s41562-016-0015},
}

@Article{kitchenham2008,
  author   = {Barbara Kitchenham and O. {Pearl Brereton} and David Budgen and Mark Turner and John Bailey and Stephen Linkman},
  journal  = {Information and Software Technology},
  title    = {Systematic literature reviews in software engineering – A systematic literature review},
  year     = {2009},
  issn     = {0950-5849},
  note     = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
  number   = {1},
  pages    = {7 - 15},
  volume   = {51},
  abstract = {Background
In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference.
Aims
This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence.
Method
We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings.
Results
Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4.
Conclusions
Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners.},
  doi      = {https://doi.org/10.1016/j.infsof.2008.09.009},
  keywords = {Systematic literature review, Evidence-based software engineering, Tertiary study, Systematic review quality, Cost estimation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584908001390},
}

@InProceedings{feldmann2019-mde-intermodel-inconsistencies,
  author    = {Stefan Feldmann and Konstantin Kernschmidt and Manuel Wimmer and Birgit Vogel{-}Heuser},
  title     = {Managing inter-model inconsistencies in model-based systems engineering: Application in automated production systems engineering},
  year      = {2019},
  pages     = {105--134},
  volume    = {153},
  abstract  = {To cope with the challenge of managing the complexity of automated production systems, model-based approaches are applied increasingly. However, due to the multitude of different disciplines involved in automated production systems engineering, e.g., mechanical, electrical, and software engineering, several modeling languages are used within a project to describe the system from different perspectives. To ensure that the resulting system models are not contradictory, the necessity to continuously diagnose and handle inconsistencies within and in between models arises. This article proposes a comprehensive approach that allows stakeholders to specify, diagnose, and handle inconsistencies in model-based systems engineering. In particular, to explicitly capture the dependencies and consistency rules that must hold between the disparate engineering models, a dedicated graphical modeling language is proposed. By means of this language, stakeholders can specify, diagnose, and handle inconsistencies in the accompanying inconsistency management framework. The approach is implemented based on the Eclipse Modeling Framework (EMF) and evaluated based on a demonstrator project as well as a small user experiment. First findings indicate that the approach is expressive enough to capture typical dependencies and consistency rules in the automated production system domain and that it requires less effort compared to manually developing inter-model inconsistency management solutions.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/jss/FeldmannKWV19.bib},
  doi       = {10.1016/j.jss.2019.03.060},
  journal   = {J. Syst. Softw.},
  timestamp = {Mon, 24 Feb 2020 15:58:06 +0100},
  url       = {https://doi.org/10.1016/j.jss.2019.03.060},
}

@Article{beam2020-ai-reproduciblity-in-health,
  author   = {Beam, Andrew L. and Manrai, Arjun K. and Ghassemi, Marzyeh},
  journal  = {JAMA},
  title    = {{Challenges to the Reproducibility of Machine Learning Models in Health Care}},
  year     = {2020},
  issn     = {0098-7484},
  month    = {01},
  number   = {4},
  pages    = {305-306},
  volume   = {323},
  abstract = {{Reproducibility has been an important and intensely debated topic in science and medicine for the past few decades. As the scientific enterprise has grown in scope and complexity, concerns regarding how well new findings can be reproduced and validated across different scientific teams and study populations have emerged. In some instances, the failure to replicate numerous previous studies has added to the growing concern that science and biomedicine may be in the midst of a “reproducibility crisis.” Against this backdrop, high-capacity machine learning models are beginning to demonstrate early successes in clinical applications, and some have received approval from the US Food and Drug Administration. This new class of clinical prediction tools presents unique challenges and obstacles to reproducibility, which must be carefully considered to ensure that these techniques are valid and deployed safely and effectively.}},
  doi      = {10.1001/jama.2019.20866},
  eprint   = {https://jamanetwork.com/journals/jama/articlepdf/2758612/jama\_beam\_2020\_vp\_190172.pdf},
  url      = {https://doi.org/10.1001/jama.2019.20866},
}

@Article{avital2018-realistic-evaluation-of-ai,
  author        = {Avital Oliver and Augustus Odena and Colin Raffel and Ekin D. Cubuk and Ian J. Goodfellow},
  journal       = {CoRR},
  title         = {Realistic Evaluation of Deep Semi-Supervised Learning Algorithms},
  year          = {2018},
  volume        = {abs/1804.09170},
  abstract      = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1804-09170.bib},
  eprint        = {1804.09170},
  timestamp     = {Mon, 13 Aug 2018 16:46:38 +0200},
  url           = {http://arxiv.org/abs/1804.09170},
}

@Proceedings{request2018,
  title     = {ReQuEST '18: Proceedings of the 1st on Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning},
  year      = {2018},
  address   = {New York, NY, USA},
  isbn      = {9781450359238},
  publisher = {Association for Computing Machinery},
  abstract  = {Artificial Intelligence (AI), Machine Learning (ML) and other emerging workloads demand efficient computer systems from the cloud to the edge. Systems designers, however, face numerous challenges from tackling the ever-growing space of design and optimization choices (including algorithms, models, software frameworks, libraries, hardware platforms, optimization techniques) to balancing off multiple objectives (including accuracy, speed, throughput, power, size, price). Furthermore, the lack of a common experimental framework and methodology makes it even more challenging to keep up with and build upon the latest research advances.The Reproducibly Quality-Efficient Systems Tournaments () initiative is a community effort to develop a rigorous methodology, open platform and  for co-designing the efficient and reliable software/hardware stack for emerging workloads. ReQuEST invites a multidisciplinary community to collaborate on benchmarking and optimizing workloads across diverse platforms, models, data sets, libraries and tools, while gradually adopting best practice. The community effectively creates a "marketplace" for trading Pareto-efficient implementations (code and data) as portable, customizable and reusable   and . We envision that such a community-driven and decentralized marketplace will help accelerate adoption and technology transfer of novel AI/ML techniques similar to the open-source movement.Please see the front matter for the 1st ReQuEST tournament on Co-designing Pareto-efficient Deep Learning Inference at ASPLOS'18 to learn more about the shared workflows and validated results, as well as about our next steps for the ReQuEST initiative.},
  location  = {Williamsburg, VA, USA},
}

@InProceedings{burgueno2018-uncertainty-confidence-in-mdd,
  author    = {Burgue\~{n}o, Loli and Bertoa, Manuel F. and Moreno, Nathalie and Vallecillo, Antonio},
  booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
  title     = {Expressing Confidence in Models and in Model Transformation Elements},
  year      = {2018},
  address   = {New York, NY, USA},
  pages     = {57–66},
  publisher = {Association for Computing Machinery},
  series    = {MODELS '18},
  abstract  = {The expression and management of uncertainty, both in the information and in the operations that manipulate it, is a critical issue in those systems that work with physical environments. Measurement uncertainty can be due to several factors, such as unreliable data sources, tolerance in the measurements, or the inability to determine if a certain event has actually happened or not. In particular, this contribution focuses on the expression of one kind of uncertainty, namely the confidence on the model elements, i.e., the degree of belief that we have on their occurrence, and on how such an uncertainty can be managed and propagated through model transformations, whose rules can also be subject to uncertainty.},
  doi       = {10.1145/3239372.3239394},
  isbn      = {9781450349499},
  keywords  = {models, confidence, model transformations, Uncertainty},
  location  = {Copenhagen, Denmark},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3239372.3239394},
}

@Article{cleland2014-dont-fire-the-architects,
  author   = {Jane {Cleland-Huang}},
  journal  = {IEEE Software},
  title    = {Don't Fire the Architect! Where Were the Requirements?},
  year     = {2014},
  issn     = {1937-4194},
  month    = {Mar},
  number   = {2},
  pages    = {27-29},
  volume   = {31},
  abstract = {The Healthcare.gov debacle of 2013 leads many to wonder if a better understanding of the project's requirements could have lessened the impact of the failed launch. The Web extra at http://youtu.be/qyQldlPz1ws is an audio podcast of author Jane Cleland-Huang reading her Requirements column, in which she discusses the Healthcare.gov debacle of 2013 and how it led many to wonder if a better understanding of the projec's requirements could have lessened the impact of the failed launch.},
  doi      = {10.1109/MS.2014.34},
  keywords = {medical information systems;social aspects of automation;Web sites;healthcare.gov debacle;project requirements;failed launch;Web extra;audio podcast;Software engineering;Medical services;Insurance;Performance evaluation;Computer architecture;requirements;architecture;Healthcare.gov},
}

@InProceedings{gotel1995-contribution-structures-req-eng,
  author    = {O. {Gotel} and A. {Finkelstein}},
  booktitle = {Proceedings of 1995 IEEE International Symposium on Requirements Engineering (RE'95)},
  title     = {Contribution structures [Requirements artifacts]},
  year      = {1995},
  month     = {March},
  pages     = {100-107},
  abstract  = {The invisibility of the individuals and groups that gave rise to requirements artifacts has been identified as a primary reason for the persistence of requirements traceability problems. The paper presents an approach based on modelling the dynamic contribution structures underlying requirements artifacts, which addresses this issue. It shows how these structures can be defined, using information about the agents who have contributed to artifact production, in conjunction with details of the numerous traceability relations that hold within and between artifacts themselves. It further outlines how the approach can be implemented, demonstrates the potential it provides for "personnel-based" requirements traceability, and discusses issues pertinent to its uptake.},
  doi       = {10.1109/ISRE.1995.512550},
  keywords  = {systems analysis;personnel;social sciences;software development management;human resource management;management of change;requirements artifacts;requirements traceability problems;dynamic contribution structures;agents;artifact production;traceability relations;personnel-based requirements traceability;Production;Software maintenance;Humans;Educational institutions;Software development management;Software systems;Computer science;Information analysis;Personnel},
}

@Article{ingolfo2013-regulatory-compliance-of-soft-req,
  author     = {Ingolfo, Silvia and Siena, Alberto and Mylopoulos, John and Susi, Angelo and Perini, Anna},
  journal    = {Data Knowl. Eng.},
  title      = {Arguing Regulatory Compliance of Software Requirements},
  year       = {2013},
  issn       = {0169-023X},
  month      = sep,
  pages      = {279–296},
  volume     = {87},
  abstract   = {A software system complies with a regulation if its operation is consistent with the regulation under all circumstances. The importance of regulatory compliance for software systems has been growing, as regulations are increasingly impacting both the functional and non-functional requirements of legacy and new systems. HIPAA and SOX are recent examples of laws with broad impact on software systems, as attested by the billions of dollars spent in the US alone on compliance. In this paper we propose a framework for establishing regulatory compliance for a given set of software requirements. The framework assumes as inputs models of the requirements (expressed in i*) and the regulations (expressed in Nomos). In addition, we adopt and integrate with i* and Nomos a modeling technique for capturing arguments and establishing their acceptability. Given these, the framework proposes a systematic process for revising the requirements, and arguing through a discussion among stakeholders that the revisions make the requirements compliant. A pilot industrial case study involving fragments of the Italian regulation on privacy for Electronic Health Records provides preliminary evidence of the framework's adequacy and indicates directions for further improvements.},
  address    = {NLD},
  doi        = {10.1016/j.datak.2012.12.004},
  issue_date = {September, 2013},
  keywords   = {Argumentation, Requirement engineering, Regulatory compliance, Empirical evaluation},
  numpages   = {18},
  publisher  = {Elsevier Science Publishers B. V.},
  url        = {https://doi.org/10.1016/j.datak.2012.12.004},
}

@InProceedings{elamin2018-repositories-as-graph-databases,
  author    = {R. {Elamin} and R. {Osman}},
  booktitle = {2018 IEEE International Conference on Software Quality, Reliability and Security (QRS)},
  title     = {Implementing Traceability Repositories as Graph Databases for Software Quality Improvement},
  year      = {2018},
  pages     = {269-276},
  doi       = {10.1109/QRS.2018.00040},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Analysis\;0\;1\;0xcce6ffff\;\;\;;
1 StaticGroup:MDE\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:Conceptualisation\;0\;0\;0x8a8a8aff\;\;Slides\;;
1 StaticGroup:AI\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:AI in SE\;0\;1\;0xe6e6ccff\;\;Shows the lack of interest in traceability\;;
1 StaticGroup:SPL\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:ws\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:meta\;0\;1\;0x00ffffff\;\;\;;
}
